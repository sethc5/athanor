{
  "domain": "cy3_machine_learning",
  "query": "",
  "n_gaps_considered": 20,
  "hypotheses": [
    {
      "gap_concept_a": "Policy generalization",
      "gap_concept_b": "Policy learning",
      "source_question": "Can a reinforcement learning policy trained on a subset of Calabi-Yau threefold polytopes (e.g., low h¹¹ or small volume regime) generalize to predict line bundle cohomology h⁰ ≥ 3 across the full KS database without retraining, and what polytope features control this generalization gap?",
      "statement": "We hypothesize that a reinforcement learning policy trained on low-Hodge-number Calabi-Yau threefolds (h¹¹ ∈ [13, 40]) learns a generalizable value function for predicting line bundle global sections h⁰ ≥ 3, and that generalization failure to high-h¹¹ regimes (h¹¹ ∈ [90, 128]) is causally explained by monotonic increase in normalized lattice point density beyond a critical threshold (approximately 0.15), rather than by distributional shift in Mori cone rank or SR ideal cardinality.",
      "mechanism": "During RL training on low-h¹¹ polytopes, the policy learns a state-value function Q(s, a) that encodes a heuristic: high normalized lattice density rho_norm correlates with rich line bundle cohomology in the training regime (h¹¹ ≤ 40). However, in high-h¹¹ polytopes, rho_norm undergoes a phase transition—it saturates or decouples from h⁰ predictability due to combinatorial explosion in the lattice point set, causing Q-values to become miscalibrated. The policy's action selection becomes unreliable specifically when rho_norm exceeds 0.15, because the training data contained no examples at that density, creating a distribution extrapolation failure.",
      "prediction": "A PPO policy trained on h¹¹ in [13, 40] (n=5000 polytopes) will achieve greater than 85 percent accuracy predicting h⁰ greater than or equal to 3 on a held-out test set from the same h¹¹ range. When tested on h¹¹ in [90, 128] without retraining, accuracy will drop to less than 60 percent. A domain-adapted version fine-tuned on 200 examples from h¹¹ in [90, 128] with rho_norm as an explicit input feature will recover greater than 80 percent accuracy on the high-h¹¹ set.",
      "falsifiable": true,
      "falsification_criteria": "If accuracy on held-out high-h¹¹ polytopes (h¹¹ in [90, 128]) remains above 75 percent without any adaptation or rho_norm feature engineering, the hypothesis is false. Alternatively, if domain adaptation with rho_norm as an explicit feature fails to recover above 78 percent accuracy on high-h¹¹ polytopes, the causal mechanism is refuted.",
      "minimum_effect_size": "Generalization accuracy gap (train h¹¹ in [13, 40] vs. test h¹¹ in [90, 128]) at least 25 percentage points (baseline 85 percent to degraded 60 percent). Domain adaptation recovery at least 20 percentage points (60 percent to 80 percent). Ablation effect of rho_norm feature at least 12 percentage points attributed variance.",
      "novelty": 5,
      "rigor": 5,
      "impact": 5,
      "replication_risk": "low",
      "experiment": {
        "approach": "Train a PPO agent on a curated subset of reflexive polytopes stratified by h¹¹, evaluate its policy on progressively distant h¹¹ ranges, and perform causal decomposition via feature ablation and domain adaptation to isolate normalized lattice density as the generalization bottleneck.",
        "steps": [
          "Download and parse the Kreuzer-Skarke database; extract reflexive polytope vertex matrices, compute Gorenstein index, face lattice depth, Mori cone generators, SR ideal cardinality, and normalized lattice point density rho_norm for all polytopes.",
          "Stratify polytopes by h¹¹ into bins: [13-40] (train/val), [50-80] (intermediate test), [90-128] (far test). For each polytope, sample 20 random line bundles and compute exact h⁰(L) via CyTools cohomology computation.",
          "Construct RL environment: state = [vertex matrix embedding (PCA-reduced to 20 dimensions), rho_norm, Gorenstein index, face lattice depth, SR ideal cardinality, Mori rank]; action = {accept polytope for expensive cohomology, reject}; reward = +1 if predicted h⁰ greater than or equal to 3 matches ground truth, -1 otherwise.",
          "Train PPO agent (PyTorch, batch size 64, learning rate 3e-4, entropy coefficient 0.01) on stratified train set h¹¹ in [13, 40] (n=5000) for 10000 episodes until validation AUC plateaus at target greater than 0.85.",
          "Evaluate trained policy on: (A) held-out test set h¹¹ in [13, 40] (n=500), (B) intermediate test h¹¹ in [50, 80] (n=500), (C) far test h¹¹ in [90, 128] (n=500). Record accuracy, precision, recall, AUC.",
          "Perform stratified ablation: retrain PPO agent three times, each time omitting one feature: (i) without rho_norm, (ii) without Mori rank, (iii) without SR cardinality. Measure accuracy on far test set for each ablated variant.",
          "Train domain-adaptation module: fine-tune the original policy on a small sample from h¹¹ in [90, 128] (n=200 polytopes) for 500 additional episodes, with rho_norm explicitly normalized to [0, 1]. Evaluate on held-out far test (n=500).",
          "Repeat domain adaptation with feature engineering variants: (alpha) add polynomial features rho_norm squared and rho_norm times Gorenstein; (beta) add bias correction term learned from train vs. test distribution of rho_norm.",
          "Analyze generalization failure landscape: scatter-plot policy accuracy versus rho_norm; fit logistic regression to threshold where accuracy drops below 75 percent; test whether rho_norm greater than 0.15 is a significant breakpoint.",
          "Validation on unscanned polytopes: apply best-performing domain-adapted policy to screen 1000 unscanned polytopes with h¹¹ in [90, 128]; select top-200 by policy score for exact cohomology computation; measure fraction with h⁰ greater than or equal to 3."
        ],
        "tools": [
          "Kreuzer-Skarke database (reflexive polytope archive)",
          "CyTools (Calabi-Yau geometry toolbox; cohomology computation)",
          "PyTorch (RL training: PPO implementation from stable-baselines3)",
          "Macaulay2 (exact h⁰(L) verification)",
          "scikit-learn (feature ablation, logistic regression, change-point detection)",
          "pandas and matplotlib (data stratification, visualization)",
          "Ray Tune (hyperparameter sweep for RL learning rate)"
        ],
        "computational": true,
        "estimated_effort": "8 to 10 weeks wall-clock compute time. Cohomology precomputation: 1 to 2 weeks. RL training per agent: 2 to 3 days; 5 to 6 agents total equals 2 to 3 weeks serial or 4 to 5 days parallel. Analysis and validation: 1 to 2 weeks.",
        "data_requirements": "Kreuzer-Skarke database (approximately 500000 reflexive polytopes). Precomputed h⁰(L) for sampled line bundles on stratified subset (approximately 200000 polytopes times 20 bundles). Topological invariants (Hodge numbers, c₂, Gorenstein index, Mori generators). Total storage: 50 to 100 GB.",
        "expected_positive": "Baseline policy achieves greater than 85 percent accuracy on held-out low-h¹¹ test set and greater than 80 percent on intermediate h¹¹ in [50, 80], but drops to 55 to 62 percent on far h¹¹ in [90, 128]. Domain adaptation recovers at least 80 percent on far test. Ablation study shows: removing rho_norm yields accuracy approximately 65 percent on far test (delta at least 15 pp); removing Mori rank yields approximately 76 percent (delta at most 4 pp); removing SR cardinality yields approximately 78 percent (delta at most 7 pp). LOESS analysis confirms rho_norm threshold approximately 0.15 as a breakpoint.",
        "expected_negative": "Far-test accuracy remains above 75 percent without adaptation, indicating true h¹¹-invariant generalization and falsifying the hypothesis. Alternatively, domain adaptation with rho_norm fails to improve accuracy above 75 percent, refuting the causal mechanism. Or ablation shows Mori rank or SR cardinality equally or more predictive than rho_norm (delta at least equal to delta_rho).",
        "null_hypothesis": "H_0: There exists no significant difference in policy accuracy between held-out low-h¹¹ test sets and far-h¹¹ test sets (H_0: delta accuracy at most 5 pp). The distribution of (polytope features to h⁰ greater than or equal to 3 label) is invariant across h¹¹ ranges.",
        "statistical_test": "Two-sided proportion z-test (H_0: p_low_h11 equals p_far_h11 vs. H_1: p_low_h11 not equal to p_far_h11), comparing accuracy on n=500 low-h¹¹ test versus n=500 far-h¹¹ test. Reject H_0 if absolute z greater than 1.96 (alpha equals 0.05). For ablation, one-way ANOVA on feature importance (delta accuracy per ablated feature), followed by post-hoc Tukey HSD test to determine whether rho_norm ablation effect significantly exceeds Mori rank or SR cardinality ablation (alpha equals 0.05). For change-point detection in rho_norm threshold: binary segmentation or PELT algorithm at alpha equals 0.05.",
        "minimum_detectable_effect": "Primary: accuracy gap (low-h¹¹ vs. far-h¹¹) at least 20 pp (e.g., 85 percent to 65 percent), detectable with n=500 per group at alpha equals 0.05, power equals 0.90. Feature ablation: rho_norm ablation delta accuracy at least 12 pp with 80 percent power (paired comparison, n=500 shared polytopes). Domain adaptation recovery: at least 18 pp improvement on far test (60 percent to 78 percent) with alpha equals 0.05. Rho_norm threshold detection: minimum detectable shift in breakpoint at least 0.05 using binary segmentation at alpha equals 0.05.",
        "statistical_power_notes": "Primary comparison: assume baseline accuracy approximately 85 percent on low-h¹¹, degraded to approximately 60 percent on far-h¹¹ (effect size approximately 25 pp). With n equals 500 per group, two-sided z-test at alpha equals 0.05 achieves greater than 99 percent power. Ablation study: paired comparison (same polytopes tested with full vs. ablated policy), n equals 500, assuming rho_norm ablation effect approximately 15 pp with sigma approximately 20 pp; paired t-test achieves approximately 90 percent power at alpha equals 0.05. Domain adaptation: n equals 500 far-test polytopes, effect size 18 pp (60 percent to 78 percent), one-sided z-test, alpha equals 0.05 yields greater than 95 percent power. For change-point detection: binary segmentation requires approximately 400 polytopes stratified across rho_norm range [0, 0.25] to detect a 0.05-unit shift in threshold with alpha equals 0.05.",
        "limitations": [
          "Cohomology computation is expensive; precomputation will be limited to approximately 200000 to 300000 polytopes; some high-h¹¹ strata may have sparse sampling, biasing estimates.",
          "RL training is stochastic; results depend on random seed and hyperparameter initialization. Mitigation: run 5 replicates per variant and report mean plus or minus std.",
          "Normalized lattice density rho_norm is continuous with no pre-existing theory of critical threshold; the 0.15 choice is exploratory. Change-point analysis may not detect clean breakpoint if transition is gradual.",
          "Line bundle sampling (20 random bundles per polytope) may not uniformly cover charge lattice. Mitigation: use stratified sampling of line bundle rank and Chern class.",
          "Domain adaptation is fine-tuned on only 200 examples from high-h¹¹ regime; this may overfit. Mitigation: perform cross-validation within the adaptation set.",
          "Policy evaluation is binary (h⁰ at least 3 vs. less than 3); finer-grained prediction requires regression and may be harder.",
          "Unscanned polytope validation assumes 10000 plus unscanned examples with precomputed invariants; not all may be available.",
          "Generalization failure could be driven by unmeasured polytope properties not captured by rho_norm, Gorenstein index, or Mori rank."
        ],
        "requires_followup": "None required. This is a fully computational study. If the domain-adapted policy achieves greater than 80 percent accuracy on far-test high-h¹¹ polytopes, the next step would be wet-lab validation: use the policy to select 50 to 100 high-confidence h⁰ greater than or equal to 3 candidates from unscanned polytopes and perform exact cohomology computation (via CyTools or Macaulay2) to confirm the hit rate. This would validate the pre-screener on truly novel data and enable deployment in large-scale CY3 landscape scans (100 times speedup of cohomology computation claimed in the research gap)."
      },
      "keywords": [
        "reinforcement learning generalization",
        "Calabi-Yau polytope prediction",
        "domain adaptation CY3",
        "line bundle cohomology RL",
        "Kreuzer-Skarke database screening"
      ],
      "gap_similarity": 0.6846974492073059,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 5.0,
      "final_score": 5.0
    },
    {
      "gap_concept_a": "Reinforcement Learning",
      "gap_concept_b": "Policy learning",
      "source_question": "Can policy learning agents trained via reinforcement learning sequentially construct line bundles on Calabi-Yau threefolds and predict high-dimensional cohomology h⁰ ≥ 3 from polytope combinatorics alone, outperforming supervised regressors on unseen KS polytopes?",
      "statement": "We hypothesize that a reinforcement learning agent trained via proximal policy optimization (PPO) to learn sequential line bundle construction policies on Calabi-Yau threefolds will generalize to unseen polytopes by exploiting normalized polytope combinatorics as state representation, and will predict h⁰ ≥ 3 bundles with ≥75% F1-score and ≥100× speedup versus brute-force enumeration, outperforming supervised XGBoost and graph neural network baselines on out-of-distribution KS polytopes.",
      "mechanism": "The policy learning agent learns to map compact polytope-derived invariants (normalized lattice point density, Gorenstein index, face lattice depth, intersection numbers) to geometrically valid line bundle charge vectors via reward-shaped credit assignment. This learned policy compresses expensive toric divisor cohomology computation into a single forward pass, generalizing across polytopes because the reward structure (geometric validity + cohomology targets + moduli stabilization) is invariant to polytope identity. The causal chain is: normalized polytope combinatorics → policy network state embedding → action (bundle charge proposal) → predicted h⁰ via learned surrogate cohomology model, bypassing the expensive toric divisor lattice computation entirely.",
      "prediction": "A PPO-trained policy agent will achieve ≥75% F1-score (with ≥70% recall, ≥80% precision) predicting h⁰ ≥ 3 line bundle existence on 2,000 held-out scanned KS polytopes (heldout set disjoint from 10,000 training set), with median inference speedup of ≥100× versus brute-force enumeration (measured as wall-clock time per polytope including policy rollout). This policy will outperform XGBoost baseline by ≥10 percentage points F1-score and GNN baseline by ≥8 percentage points F1-score on the same held-out set.",
      "falsifiable": true,
      "falsification_criteria": "If the trained policy achieves <65% F1-score on the held-out 2,000 scanned polytopes, or if F1-score on unseen (unscanned) polytopes drops below 60% when tested against surrogate cohomology labels, or if wall-clock speedup versus XGBoost is <50×, the hypothesis is refuted. Additionally, if XGBoost baseline achieves >80% F1-score (matching or exceeding the policy), policy learning adds no causal value over cheaper supervised methods.",
      "minimum_effect_size": "F1-score ≥75% on held-out scanned polytopes; >10 percentage point absolute improvement in F1 over XGBoost baseline; ≥100× wall-clock speedup versus brute-force enumeration; ≥70% recall to ensure high h⁰ ≥ 3 bundles are not missed in pre-screening.",
      "novelty": 4,
      "rigor": 5,
      "impact": 5,
      "replication_risk": "low",
      "experiment": {
        "approach": "Design a Markov Decision Process where agent state encodes normalized polytope invariants, actions propose line bundle charge vectors, and rewards combine geometric validity (kernel/cokernel finiteness), cohomology targets (h⁰ ≥ 3), and moduli stabilization (Swiss cheese parameter bounds). Train a PPO policy on 10,000 scanned KS polytopes with ground-truth cohomology labels using CyTools; evaluate generalization on a held-out 2,000 scanned polytopes and a surrogate-labeled set of unscanned polytopes. Measure F1-score, speedup, and compare against XGBoost and GNN baselines.",
        "steps": [
          "Step 1: Construct polytope feature tensors for 10,000 scanned KS polytopes: compute normalized invariants (lattice point density per volume^{2/3}, Gorenstein index, face lattice depth, Mori cone cardinality, second Chern class c₂) and stack into state vectors; normalize to mean 0, std 1 per feature.",
          "Step 2: Define MDP: state = (polytope_embedding ∈ R^{32}, bundle_charge_history, step_counter ∈ [0, T_max=5]); action = proposed line bundle charge vector ∈ Z^{rank(Pic(CY3))} (discrete, bounded to [−20, +20] per coordinate); terminal condition = T_max steps or agent commits to bundle selection.",
          "Step 3: Implement reward function: r(s,a) = w₁·[h⁰(L)≥3] + w₂·[|h¹(L)|+|h²(L)|+|h³(L)| minimal] + w₃·[Swiss cheese feasible] + w₄·[geometric validity] − w₅·[step count/T_max]. Tune weights {w₁,...,w₅} via Bayesian hyperparameter search on 500-polytope validation set (separate from train/test).",
          "Step 4: Train PPO agent using Stable Baselines3 library: policy = 2-layer MLP (256 hidden units, ReLU), value function = parallel MLP; learning rate = 3e−4, batch size = 256, n_steps = 2048, entropy coeff = 0.01; train for 500,000 environment steps (~50 epochs over training set). Use curriculum learning: phase 1 (epochs 1−10): sample polytopes uniformly; phase 2 (epochs 11−50): oversample polytopes with χ = −6 and h¹¹ ∈ [13, 50] (rare cases).",
          "Step 5: Pre-compute ground-truth cohomology labels for 10,000 training polytopes and 2,000 held-out scanned polytopes via CyTools (h⁰, h¹, h², h³ for all Pic generators and line bundles of small degree). Record exact wall-clock time per polytope (toric divisor computation).",
          "Step 6: Train XGBoost and graph neural network (GNN) baselines on same 10,000 polytopes. XGBoost: 200 trees, max_depth=6, learning_rate=0.05, predicting binary label h⁰≥3 for each line bundle type. GNN: 3-layer graph convolution on polytope dual graph (vertices=divisors, edges=intersections), output logits for h⁰≥3.",
          "Step 7: Evaluate all three models on held-out 2,000 scanned polytopes: compute F1-score, precision, recall, AUC-ROC for h⁰≥3 prediction. Measure wall-clock inference time per polytope (policy rollout vs. XGBoost inference vs. GNN forward pass).",
          "Step 8: Generate surrogate cohomology labels for ~50,000 unscanned KS polytopes at h¹¹=18 via trained XGBoost (cheapest surrogate). Evaluate policy agent on this set against surrogate labels; compare F1-score drop versus held-out performance (quantify domain shift).",
          "Step 9: Ablation study: train three ablated policy variants—(A) reward only h⁰≥3 (drop geometric validity and moduli terms), (B) reward only geometric validity (drop h⁰ target), (C) reward only moduli (drop both)—and measure F1-score degradation relative to full reward. This isolates which reward components drive generalization.",
          "Step 10: Measure speedup: benchmark wall-clock time for policy (state embedding + network forward pass per polytope) vs. brute-force enumeration (full CyTools toric divisor computation). Report median and percentiles over test set."
        ],
        "tools": [
          "CyTools (open-source; includes Calabi-Yau geometry, cohomology computation, KS database interface)",
          "Stable Baselines3 (PPO implementation, PyTorch backend)",
          "XGBoost (scikit-learn API)",
          "PyTorch Geometric (graph neural network layers)",
          "CVXPY (moduli stabilization feasibility check)",
          "Kreuzer-Skarke database (104,891 reflexive polytopes)",
          "GPU cluster (1× V100 or A100 for PPO training; 4 days estimated)"
        ],
        "computational": true,
        "estimated_effort": "4–6 weeks compute time (2 weeks infrastructure setup + data preprocessing; 2 weeks policy training + baseline training in parallel; 1 week evaluation + ablation + writeup). Bottleneck: CyTools cohomology computation for ground truth (~1 week on 4 cores for 10,000 polytopes), which can run in parallel with development.",
        "data_requirements": "Kreuzer-Skarke database (reflexive polytopes, free); CyTools-computed cohomology labels for 12,000 KS polytopes (h⁰, h¹, h², h³ for line bundles); polytope features (vertices, face lattice) for 104,891 KS polytopes. All public or derivable from public databases. Estimated storage: <50 GB.",
        "expected_positive": "Policy achieves F1-score ≥75% on held-out scanned polytopes (≥70% recall, ≥80% precision); outperforms XGBoost by ≥10 percentage points and GNN by ≥8 points; speedup ≥100× versus brute-force; F1-score on surrogate-labeled unscanned polytopes remains ≥65%; ablation shows full reward achieves ≥5 point F1 improvement over single-term rewards.",
        "expected_negative": "Policy F1-score <65% on held-out scanned polytopes; baseline XGBoost achieves >80% F1 (matching policy); policy speedup <50×; F1 drops >15 points on unscanned polytopes (domain shift); ablation shows reward terms are redundant (all single-term variants match full reward within noise).",
        "null_hypothesis": "H₀: The learned PPO policy does not generalize better than supervised regressors (XGBoost, GNN) to unseen polytopes. Specifically, the policy F1-score on held-out scanned polytopes is ≤ max(XGBoost F1, GNN F1), or the policy speedup is <20× versus brute-force enumeration, or the policy F1-score drops >20 percentage points on unscanned polytopes.",
        "statistical_test": "Two-sided Welch t-test comparing F1-scores: policy vs. XGBoost baseline on 2,000 held-out polytopes, alpha=0.05, desired power=0.90. Mann-Whitney U test (non-parametric) for wall-clock speedup, alpha=0.05. Bonferroni correction for multiple comparisons (3 models × 2 metrics = 6 tests; alpha_adjusted=0.05/6≈0.0083).",
        "minimum_detectable_effect": "F1-score difference ≥0.08 (8 percentage points) between policy and XGBoost baseline. Assuming binomial variance in F1 (typical SD ~0.04 per model on 2,000 samples), this requires n=2,000 with power=0.90, one-sided. Speedup: ≥100× median (observed via percentile statistics, not formal test, but reported with 95% CI via bootstrap).",
        "statistical_power_notes": "For F1-score comparison (alpha=0.05, desired power=0.90, assuming effect size Cohen's d=0.2 for a 8-point F1 difference): n=2,000 held-out polytopes provides >99% power. For speedup, measure wall-clock time for 2,000 polytopes per method; report median, IQR, and 95% CI via percentile bootstrap (10,000 resamples). Ablation study: paired t-test on F1 scores of full reward vs. each ablated variant, n=2,000, alpha=0.05, power=0.80.",
        "limitations": [
          "Ground-truth cohomology labels for only 12,000 polytopes (10% of KS database); remaining 92,000 polytopes rely on XGBoost surrogate labels, which introduces label noise and may underestimate policy generalization.",
          "Policy trained only on KS database; generalization to non-reflexive polytopes or other string compactifications (F-theory, heterotic) is unexplored.",
          "Reward function design (weights w₁–w₅) is semi-arbitrary; different weight configurations may substantially alter policy behavior and are not comprehensively scanned.",
          "No guarantee that a learned policy policy is interpretable; credit assignment (which polytope features drive bundle selection) remains a black box.",
          "Surrogate cohomology model (XGBoost) itself may be biased toward certain polytope geometries, causing policy to overfit to that bias.",
          "Domain shift: policy may exploit spurious correlations between normalized invariants and h⁰ that break on truly unseen (not-yet-computed) polytopes; only surrogate evaluation is feasible."
        ],
        "requires_followup": "After computational validation, wet-lab analog is to exhaustively compute cohomology for 50,000 unscanned KS polytopes (h¹¹=18) using CyTools on a large cluster (6–12 months wall time) and re-evaluate policy F1-score on true labels. This would confirm that policy generalization is causal (learned polytope→bundle mapping) rather than artifact of XGBoost surrogate bias. Additionally, run policy agent on a held-out subset (2,000 polytopes) of newly computed polytopes to isolate out-of-distribution performance."
      },
      "keywords": [
        "reinforcement learning policy optimization",
        "Calabi-Yau line bundle cohomology prediction",
        "polytope combinatorics generalization",
        "toric geometry machine learning",
        "sequential bundle construction",
        "moduli space pre-screening"
      ],
      "gap_similarity": 0.6494361162185669,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.65,
      "final_score": 4.65
    },
    {
      "gap_concept_a": "Generalization",
      "gap_concept_b": "generalization performance",
      "source_question": "Does the generalization capacity of ML models trained on small CY3 polytope datasets (N < 5000) systematically depend on the latent dimensionality of the toric geometry feature space, and can we predict out-of-distribution performance on the remaining 195k+ KS polytopes from intrinsic model properties rather than held-out validation alone?",
      "statement": "We hypothesize that the effective dimensionality of a polytope's toric geometry feature space—measured as the intrinsic dimension of the normalized lattice point density, face lattice depth, and Gorenstein index—causally constrains the generalization error of ML models trained on CY3 cohomology prediction, and that this constraint can be predicted from model-agnostic geometric complexity metrics before empirical validation.",
      "mechanism": "Higher intrinsic dimensionality of polytope feature space increases the sample complexity required to achieve a given generalization error because the VC dimension proxy (effective degrees of freedom in toric invariants) grows with geometric complexity. Models trained on low-dimensional polytope subsets encounter feature combinations in the unlabeled 195k+ KS polytopes that fall outside their training support, causing systematic prediction drift. This drift is quantifiable via a PAC-Bayes bound parameterized by the Rademacher complexity of the target cohomology class (h⁰, h¹¹) and the geometric radius (KL divergence between training and test polytope distributions in toric space).",
      "prediction": "For polytopes in the test set that lie beyond 3 standard deviations from the training set's centroid in the normalized (lattice density, f-vector magnitude, Gorenstein index) space, the mean absolute prediction error (MAE) in h⁰ will increase by at least 25% compared to polytopes within the training set's convex hull, with this excess error proportion varying sub-linearly (R² > 0.60) with the intrinsic dimension estimate of the training feature space.",
      "falsifiable": true,
      "falsification_criteria": "If out-of-distribution generalization error does not correlate significantly (Spearman ρ > 0.40, p < 0.05) with geometric distance in toric space, or if the error increase is ≤ 10% even for polytopes > 3σ away, the hypothesis is refuted. Additionally, if the variance explained by geometric complexity metrics (lattice density, face lattice depth, Gorenstein index) in a multiple regression model of generalization error is < 15%, the mechanistic claim is falsified.",
      "minimum_effect_size": "Spearman rank correlation ≥ 0.40 between polytope distance in toric feature space and per-sample absolute error; R² ≥ 0.60 in regression of generalization error gap (out-of-distribution vs. in-distribution MAE) on intrinsic dimension estimate; ≥25% relative increase in MAE for OOD polytopes; variance explained by toric geometry metrics in generalization error > 15%.",
      "novelty": 4,
      "rigor": 5,
      "impact": 5,
      "replication_risk": "low",
      "experiment": {
        "approach": "Conduct a systematic empirical study on the KS database coupling intrinsic dimension estimation of polytope feature space with generalization error analysis across multiple ML architectures. Train ensemble models (GNN, Transformer, XGBoost) on stratified subsets of ~50k labeled KS polytopes (h¹¹, h⁰, Hodge numbers), measure prediction error stratified by geometric distance from training set, and fit a unified PAC-Bayes generalization bound model parameterized by toric complexity metrics to predict out-of-distribution performance on the unlabeled 195k+ polytopes.",
        "steps": [
          "Extract and normalize polytope features for all ~50k scanned KS polytopes: (a) normalized lattice point density ρ_norm = |lattice points| / Volume^(1/3), (b) f-vector magnitude ||f|| = sum of face counts, (c) Gorenstein index g_idx, (d) dual polytope reflexivity proxy r_eff.",
          "Estimate intrinsic dimensionality of the ~50k polytope feature space using three methods: (i) PCA retained variance (threshold 95%), (ii) local intrinsic dimension (LID) estimator via k-NN (k=30), (iii) persistent homology dimension of Vietoris-Rips complex sampled at ε = max distance / 10. Compute consensus intrinsic dimension d_eff as median of three estimates.",
          "Create 10 random train/test splits stratified by Hodge pair (h¹¹, h²¹) to ensure balanced χ and generation distribution: training set size N_train ∈ {500, 1000, 2500, 5000, 10000}, test set = remaining ~40k polytopes.",
          "For each split, train ensemble of 5 models per architecture (GNN on toric divisor graph + Chern class features, Transformer on polytope vertex sequences, XGBoost on toric invariants) using 5-fold cross-validation on training set. Hyperparameter optimization via Bayesian search (10 iterations per architecture per split) to maximize validation R² on h⁰ and h¹¹ prediction.",
          "For each test sample, compute geometric distance d_toric to training set centroid in normalized feature space [ρ_norm, ||f||, g_idx] using Euclidean metric. Stratify test predictions by distance quantiles: d < 1σ (in-distribution), 1σ ≤ d < 2σ (borderline), 2σ ≤ d < 3σ (out-of-distribution adjacent), d ≥ 3σ (far OOD).",
          "For each model and each distance stratum, compute: (a) mean absolute error (MAE) in h⁰ and h¹¹, (b) Kolmogorov-Smirnov distance between predicted and true cohomology class distributions, (c) calibration error (Brier score) for binary 'h⁰ ≥ 3' task (string-theory relevant).",
          "Fit a generalized linear model (GLM) of form: error_i ~ α + β₁·d_toric_i + β₂·d_eff + β₃·(d_toric·d_eff)_i + β₄·N_train + β₅·architecture_class + ε_i, where d_eff is the intrinsic dimension computed in step 2. Extract β coefficients and 95% CI.",
          "Derive a PAC-Bayes generalization bound for each model: GenBound = √[(log(2√n/δ) + KL(empirical || prior)) / (2n)] + (1/n)·Σ|error_i|, where KL term incorporates toric feature space geometry via a learned prior centered on training feature distribution centroid.",
          "Project bound predictions onto unlabeled 195k+ polytopes: estimate d_toric for each using training set centroid, apply fitted GLM and PAC-Bayes bound, rank polytopes by predicted generalization error.",
          "Validate predictions on a held-out subset of the labeled 50k (random 10%, ~5k polytopes) not used in any train/test split. Compute Spearman rank correlation ρ between predicted and actual error, R² of bound fit, calibration plot (predicted vs. realized error)."
        ],
        "tools": [
          "Kreuzer-Skarke (KS) database (~50k labeled polytopes with h¹¹, h⁰, c₂, κ_{abc})",
          "CyTools (toric divisor graph, Chern class, Gorenstein index computation)",
          "PyTorch Geometric (GNN on polytope graphs)",
          "Hugging Face Transformers (attention-based polytope encoding)",
          "XGBoost (toric invariant regression baseline)",
          "scikit-learn (PCA, intrinsic dimension estimation)",
          "scikit-tda (persistent homology for dimension estimation)",
          "scipy.stats (Kolmogorov-Smirnov, Spearman correlation tests)",
          "statsmodels (GLM, PAC-Bayes bound fitting)",
          "Public computational resources (GPU cluster for GNN/Transformer training, ~200–500 GPU-hours total)"
        ],
        "computational": true,
        "estimated_effort": "8–12 weeks: 2 weeks data extraction & feature engineering, 4 weeks ensemble training across 10 splits × 3 architectures × 4 training set sizes, 2 weeks error analysis and geometric distance stratification, 2 weeks bound fitting and validation, 1 week final reporting and figure generation. Parallelizable across 20+ GPU cores.",
        "data_requirements": "Labeled KS polytopes (h⁰, h¹¹, h²¹, c₂, κ_{abc}) available from companion CyTools scan project; toric divisor graph and Chern class features computable from reflexive polytope files (Kreuzer-Skarke database .dat format); mirror symmetry pairs for cross-validation. No proprietary data required; all public.",
        "expected_positive": "Spearman ρ ≥ 0.40 (p < 0.001) between polytope distance in toric feature space and per-sample absolute error across architectures. GLM β₁ (distance coefficient) > 0 with 95% CI excluding zero. R² ≥ 0.60 in predicting generalization error gap (OOD vs. in-distribution MAE) from intrinsic dimension estimate and training set size. For polytopes ≥ 3σ from training centroid, MAE in h⁰ increases ≥ 25% relative to in-distribution median. Variance explained by toric geometry metrics (ρ_norm, ||f||, g_idx) in multi-regression of error > 15%. Calibration plot (predicted vs. realized error on held-out 5k) shows no significant bias (zero-mean residuals, R² > 0.50).",
        "expected_negative": "Spearman ρ < 0.30 or p > 0.05 between distance and error. GLM β₁ indistinguishable from zero. R² < 0.30 in error gap prediction. OOD error increase < 10%. Variance explained by toric metrics < 5%. Calibration plot exhibits >20% prediction bias or R² < 0.30. These results would indicate that model generalization depends primarily on architecture and hyperparameters, not polytope geometry.",
        "null_hypothesis": "H₀: The generalization error of ML models trained on CY3 cohomology prediction is independent of the geometric distance of test samples from the training polytope set in toric feature space, and intrinsic dimension of polytope feature space does not causally influence the sample complexity required to achieve a target error rate.",
        "statistical_test": "Primary: Spearman rank correlation test between geometric distance (d_toric, stratified into 4 bins) and per-sample absolute error, one-tailed H₀: ρ ≤ 0, α = 0.05, n ≈ 40k test samples across 10 splits. Secondary: GLM F-test on β₁ (distance coefficient) with Type I error α = 0.05. Tertiary: Permutation test on learned PAC-Bayes bound: shuffle true labels within distance strata 1000×, refit bound, compute null distribution of R², empirical p-value = (# permutations with R² ≥ observed) / 1000. Two-sided, α = 0.05.",
        "minimum_detectable_effect": "Spearman ρ ≥ 0.40 (detectable with n ≈ 40k and power 0.90 at α = 0.05); GLM β₁ ≥ 0.002 (in units of MAE per normalized distance unit), detectable with F-test power ≥ 0.80; R² ≥ 0.60 in error gap GLM; ≥25% relative MAE increase for OOD polytopes (effect size Cohen's d ≥ 0.35 assuming SD of MAE ≈ 0.08). For calibration (validation on held-out 5k), bias-corrected R² ≥ 0.50 detectable with root mean squared error of bound ≤ 0.10.",
        "statistical_power_notes": "Primary correlation test: n ≈ 40k test polytopes per split × 10 splits = 400k paired (distance, error) observations; power to detect ρ = 0.40 at α = 0.05 (two-sided) is > 0.99. GLM: ~4000 in-distribution + 36k OOD samples per split, power to detect β₁ = 0.002 (effect size ≈ 0.25 SD) is 0.85+ at α = 0.05. Bound validation (5k held-out): power to detect R² = 0.60 vs. R² = 0.30 null is ≈0.88 with α = 0.05. No adjustments for multiple comparisons needed (10 splits are replications, not independent tests; pool results).",
        "limitations": [
          "Feature space intrinsic dimension estimate depends on sampling density and method choice (PCA, LID, persistent homology); consensus approach mitigates but does not eliminate sensitivity. Validate d_eff robustness via bootstrap resampling.",
          "Generalization error is task-specific (h⁰ vs. h¹¹ vs. h²¹); hypothesis tested on h⁰ and h¹¹ only. h²¹ may exhibit different geometric sensitivity due to Hodge diamond constraints.",
          "KS database is biased toward lower h¹¹ (majority h¹¹ ≤ 30); rare high-h¹¹ polytopes (> 100) may not be well-represented, limiting OOD generalization claims to 'adjacent' OOD regime (d ~ 2–3σ).",
          "PAC-Bayes bound is often loose in practice; framework provides direction of causality (geometry → sample complexity) but may not give tight predictions for absolute error rates. Empirical GLM fit is more practical for screening.",
          "Assumes euclidean metric in normalized feature space; alternative metrics (Wasserstein distance on polytope vertex distributions, graph edit distance on toric fan) not explored; may alter sensitivity.",
          "Model architectures (GNN, Transformer, XGBoost) have different inductive biases; some may be more robust to geometric drift. Results pool across architectures; architecture-specific sensitivity requires subgroup analysis (increases multiple testing burden).",
          "Validation on 5k held-out polytopes from labeled 50k does not directly test generalization to unlabeled 195k+; extrapolation beyond d = 3σ is speculative. Full validation would require labeling new polytopes."
        ],
        "requires_followup": "To fully validate predictive generalization theory on the unlabeled 195k+ KS polytopes: (1) [Computational follow-up, ~2 weeks] Apply fitted PAC-Bayes bound and GLM to score all 195k+ polytopes by predicted generalization error; identify top 1000 candidates with highest predicted error and lowest prior h⁰ estimates. (2) [Wet-lab/algebraic computation follow-up, ~8–12 weeks] Commission computation of true cohomology (h⁰, h¹¹) for randomly selected subset of 200–500 polytopes from the predicted high-error pool via CyTools + algebraic geometry computation (requires specialist resources or established collaboration with Kreuzer-Skarke maintainers). (3) Measure realized error on these newly labeled polytopes; compare against bound predictions to test calibration and falsify hypothesis if needed. (4) If validated, deploy fitted model as a fast pre-screener in the full KS pipeline to prioritize cohomology computation."
      },
      "keywords": [
        "CY3 generalization bounds",
        "toric geometry feature intrinsic dimensionality",
        "PAC-Bayes cohomology prediction",
        "Rademacher complexity polytopes",
        "out-of-distribution robustness algebraic geometry"
      ],
      "gap_similarity": 0.7024638652801514,
      "gap_distance": 14,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.65,
      "final_score": 4.65
    },
    {
      "gap_concept_a": "Model robustness",
      "gap_concept_b": "model limitations",
      "source_question": "Do uncharacterized model limitations in CY3 datasets (e.g., polytope sampling bias, incomplete GLSM coverage, missing line bundle orbits) systematically reduce robustness of ML predictors across the KS database, and can principled limitation-aware training reverse this degradation?",
      "statement": "We hypothesize that uncharacterized polytope sampling bias and incomplete GLSM coverage in KS database training sets directly cause systematic robustness degradation of ML line-bundle predictors, and that this degradation can be quantitatively reversed through limitation-aware regularization that enforces Serre duality consistency and polytope family rebalancing.",
      "mechanism": "Training data bias (sparse high-h¹¹ sampling, incomplete dual-polytope structures, restricted GLSM matrices) causes ML models to learn spurious correlations in the majority polytope family while developing poor generalization to rare subsets. This is causal because: (1) models trained on unbiased polytope subsets show measurably higher calibration on held-out rare families; (2) adding a Serre duality loss term (h⁰ + h³ = h⁰, h¹ + h² = h¹ for dual bundles) explicitly constrains the function class to respect known physical symmetries, forcing the model away from spurious patterns; (3) rebalancing training polytopes by Gorenstein index and h¹¹ density recovers out-of-distribution accuracy without additional data.",
      "prediction": "Models trained with limitation-aware regularization (Serre duality loss weight λ=0.5, polytope rebalancing to uniform h¹¹ deciles) will show ≥25% reduction in RMSE on high-h¹¹ held-out test sets (h¹¹ > 80) compared to baseline XGBoost/GNN trained on the standard KS split, while maintaining <5% accuracy loss on the in-distribution test set.",
      "falsifiable": true,
      "falsification_criteria": "If RMSE on held-out high-h¹¹ polytopes (h¹¹ > 80) shows <10% improvement (or worsens) after limitation-aware regularization, compared to baseline; OR if in-distribution test accuracy drops >10% (suggesting regularization is too aggressive); OR if Serre duality loss term contributes <2% variance explained in the final model, indicating the physical constraint is redundant.",
      "minimum_effect_size": "≥25% relative RMSE reduction on h¹¹ > 80 held-out set (e.g., baseline RMSE = 1.2 → regularized RMSE ≤ 0.9 for h⁰ prediction); calibration error (Platt scaling ECE) improvement of ≥0.05; AUC for 'h⁰≥3 existence' classification improved by ≥5 percentage points on rare polytope families.",
      "novelty": 4,
      "rigor": 5,
      "impact": 5,
      "replication_risk": "low",
      "experiment": {
        "approach": "Conduct a controlled computational study: (1) audit KS database polytope and GLSM coverage to define sampling bias; (2) train baseline and limitation-aware models on stratified subsets; (3) measure robustness degradation as a function of h¹¹ and Gorenstein index distance; (4) apply Serre duality loss and polytope rebalancing to reverse it.",
        "steps": [
          "Query CyTools KS database (104 three-generation polytopes + full sample ≤195k) and measure h¹¹ histogram, Gorenstein index distribution, dual polytope coverage (% with complete dual structure), and GLSM charge matrix density (% non-zero entries) as function of polytope properties.",
          "Define 'limitation profile': for each polytope, compute a limitation score as (1 - coverage of dual polytope) + (1 - GLSM density) + (1 - h¹¹ quantile rank in training set). Stratify held-out test set into two: 'low limitation' (≤25th percentile) and 'high limitation' (≥75th percentile).",
          "Curate training set as two splits: (A) 'biased': random sample of 30k polytopes weighted by GLSM completeness (simulates current database bias), (B) 'rebalanced': same 30k but stratified uniformly across h¹¹ deciles and Gorenstein indices.",
          "Train four models on each split using XGBoost (baseline feature set: vertex matrix properties, face lattice depth, Mori cone generators) and a Graph Neural Network (toric divisor graph input): (i) baseline (L2 regularization only), (ii) + Serre duality loss (weight λ=0.5), (iii) + polytope rebalancing, (iv) + both.",
          "For each model × split combination, measure: RMSE on h⁰(L) prediction across all line bundles; AUC for binary 'h⁰ ≥ 3' classification; calibration error (Platt scaling ECE, ≤0.1 is good); separate metrics on low-limitation and high-limitation test subsets.",
          "Compute Δ RMSE = RMSE(high limitation) − RMSE(low limitation) and plot as function of h¹¹ bin. Verify hypothesis: baseline (biased) shows largest Δ RMSE; limitation-aware (both) shows smallest.",
          "Measure variance explained by Serre duality loss term alone (ablate λ → 0) to verify physical consistency is not redundant.",
          "Construct Pareto frontier: for each model configuration, plot (computational cost per polytope: feature computation time + model inference) vs. (AUC on h⁰ ≥ 3 prediction on high-limitation set). Identify sweet spot: ≥25% RMSE improvement with <20% computational overhead.",
          "Generate failure-mode report: identify polytope families (specific h¹¹ ranges, Gorenstein indices) where even the best limitation-aware model under-performs, marking them as 'requires direct computation'."
        ],
        "tools": [
          "CyTools database API (polytope feature extraction, GLSM matrix queries, dual polytope structures)",
          "XGBoost (v1.7+) with custom Serre duality loss term",
          "PyTorch Geometric + custom GNN for toric divisor graphs",
          "scikit-learn (stratified splits, Platt scaling, AUC/calibration metrics)",
          "Pandas + NumPy (data audit, stratification, variance decomposition)",
          "Matplotlib/Plotly (RMSE degradation curves, Pareto frontiers)",
          "Kreuzer-Skarke database (complete polytope list, ~195k reflexive polytopes)"
        ],
        "computational": true,
        "estimated_effort": "3–4 weeks: 1 week data audit and curation, 1.5 weeks model training (parallelized across GPU cluster for GNNs), 1 week evaluation and Pareto frontier construction, 0.5 week failure-mode analysis and reporting.",
        "data_requirements": "CyTools KS database (polytope vertices, GLSM matrices, computed line bundle cohomology for ≥50k polytopes as ground truth), dual polytope structures (computed or fetched from Kreuzer-Skarke). Minimum: 30k training polytopes with >80% GLSM coverage, 10k held-out test polytopes stratified by h¹¹ and Gorenstein index.",
        "expected_positive": "Limitation-aware regularized models show ≥25% RMSE reduction on high-h¹¹ (h¹¹ > 80) test polytopes; in-distribution test accuracy loss <5%; calibration error (ECE) improves by ≥0.05; Serre duality loss term explains ≥2% of variance in final model; Pareto frontier identifies a configuration with >90% of best-case RMSE and <1.2× baseline computational cost.",
        "expected_negative": "Limitation-aware models show <10% RMSE improvement on high-h¹¹ test set, or in-distribution accuracy drops >10%; Serre duality loss contributes <1% variance; or RMSE degradation (Δ RMSE) is not significantly different between biased and rebalanced training sets, suggesting sampling bias is not the causal driver.",
        "null_hypothesis": "H₀: Data limitations (polytope sampling bias, incomplete GLSM) have no causal effect on model robustness across the KS database. Equivalently: RMSE degradation from low-limitation to high-limitation test sets is independent of training data bias and cannot be systematically reduced through Serre duality regularization or polytope rebalancing.",
        "statistical_test": "Two-sided t-test (paired samples) comparing RMSE on high-limitation test polytopes (h¹¹ > 80) between baseline and limitation-aware models, α=0.05. Secondary: Spearman correlation test between 'limitation score' and RMSE residuals (ρ > 0.3 expected under hypothesis); Mann–Whitney U test for AUC differences between biased and rebalanced splits (one-sided, α=0.05).",
        "minimum_detectable_effect": "Cohen's d ≥ 0.6 for RMSE difference (25% improvement = ~0.3 units on typical h⁰ scale of 0–100; effect size ~d=0.6 with n≥500 test samples per condition, 80% power). For calibration (ECE): Δ ECE ≥ 0.05 (difference of ~5 percentage points in miscalibration). For AUC: Δ AUC ≥ 0.05 (e.g., 0.82 → 0.87) on h⁰ ≥ 3 classification, detectable with ~1000 test polytopes at 80% power.",
        "statistical_power_notes": "Planned test set sizes: 5,000 polytopes stratified across h¹¹ deciles (low-limitation: 2,500, high-limitation: 2,500). With n=2,500 per group, α=0.05 two-sided t-test, a true RMSE difference of Δ=0.3 (25% of baseline 1.2) yields d≈0.6, power ≈95%. For calibration ECE: n=5,000 total allows detection of Δ ECE=0.05 with 90% power. For GNN training convergence: run each model for 200 epochs (validation loss plateaus by ~150) with early stopping (no improvement for 20 epochs).",
        "limitations": [
          "Ground truth cohomology only available for ~50k polytopes; remaining 145k+ require inference, creating a 'ground truth sparsity' problem. Mitigation: evaluate on the curated 50k subset first; validate generalization via cross-validation within ground-truth set.",
          "Serre duality loss is deterministic constraint (valid for all line bundles) but may over-regularize if model variance is not the primary source of error. Mitigation: ablate loss weight (λ ∈ {0, 0.1, 0.5, 1.0}) and report Pareto curve.",
          "Polytope rebalancing may discard rare polytope families during training, reducing diversity. Mitigation: use stratified rebalancing (keep ≥5% per h¹¹ bin) and compare diversity metrics (e.g., effective dimensionality of training set).",
          "GLSM matrices incomplete for many polytopes, but incompleteness itself is informative (may indicate non-generic structure). Mitigation: treat missing GLSM entries as separate feature category and model uncertainty explicitly via quantile regression.",
          "Generalization to KS polytopes not in training set (next 145k) requires assumption that limitation-aware regularization works similarly across unscanned region. Mitigation: restrict final deployment claims to 'certified safe' polytope families (those for which high-limitation test performance is ≥85% of in-distribution); mark others as 'requires direct computation'."
        ],
        "requires_followup": "Wet-lab proxy: For 10–20 representative high-limitation polytopes predicted by the best-fit model to have h⁰(L) ≥ 3, run direct toric cohomology computation (via Macaulay2 or Oscar) to verify predictions. If ≥90% are correct, the model is deployment-ready for a 100× speedup pipeline; if <80%, rerun experiment with higher Serre duality loss weight (λ > 1) or request additional ground-truth data from CyTools. This 2–3 week validation confirms computational predictions correspond to mathematical reality."
      },
      "keywords": [
        "Calabi-Yau line bundle prediction",
        "model robustness out-of-distribution",
        "data limitation quantification",
        "Serre duality regularization",
        "polytope sampling bias",
        "domain-aware ML calibration"
      ],
      "gap_similarity": 0.6243652105331421,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.65,
      "final_score": 4.65
    },
    {
      "gap_concept_a": "Asymptotic distribution",
      "gap_concept_b": "Convergence rate",
      "source_question": "Can convergence rate bounds for graph neural network embeddings on polytope data be derived from first principles using asymptotic distribution theory, and do tighter convergence guarantees improve practical ML screening speed for high-Hodge-number CY3 detection?",
      "statement": "We hypothesize that convergence rates of graph neural network embeddings on reflexive polytope data are determined by a polynomial dependence on polytope Gorenstein index (g) and face lattice depth (d), specifically O(n^{-1/2} * g^{-1} * log(d)^{-1}), where n is the training set size; and that this theoretical bound enables surrogate models trained on polytope subsets to predict h0 >= 3 occurrence with <5% additional error compared to full-database models.",
      "mechanism": "Reflexive polytopes in the KS database form a structured combinatorial space where the Gorenstein index quantifies arithmetic complexity and face lattice depth controls the intrinsic dimensionality of the polytope's combinatorial type. GNN embeddings on such data satisfy uniform concentration inequalities derived from covering-number bounds on polytope configuration spaces; polytopes with higher Gorenstein index have sparser lattice point sets, reducing effective model capacity required and tightening concentration. The convergence rate O(n^{-1/2} * g^{-1} * log(d)^{-1}) emerges from adapting empirical process theory (Rademacher complexity) to the measure space of reflexive polytopes, where the VC dimension is controlled by face lattice combinatorics.",
      "prediction": "When training GNNs on random subsets of KS polytopes at fixed h11 in {13, 18, 25}: (1) embedding Wasserstein distance from asymptotic limit decreases as O(n^{-0.45±0.10}), consistent with n^{-1/2} up to logarithmic factors; (2) polytopes with Gorenstein index g >= 4 show >=40% faster convergence (lower constant in O notation) than g=1 polytopes at fixed n; (3) a GNN trained on 15% of the h11=18 subset (~7,500 polytopes) using theory-guided early stopping will predict h0>=3 with <5% relative increase in false-negative rate compared to full-database model on held-out test of 5,000 polytopes.",
      "falsifiable": true,
      "falsification_criteria": "If embedding Wasserstein distance decays slower than n^{-0.35} (exponent alpha < 0.35, 95% CI), or if Gorenstein index shows <15% relative speedup per unit increase in g, or if the 15%-subset GNN shows >=8% relative increase in false-negative rate for h0>=3 prediction, the hypothesis is refuted.",
      "minimum_effect_size": "Convergence exponent alpha in [0.40, 0.55] (n^{-alpha} decay); Gorenstein index speedup >=30% per unit g; h0>=3 prediction error increase <=5% relative when training on 15% subset versus full database.",
      "novelty": 5,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Conduct a three-phase computational study: (Phase 1) develop a measure-theoretic framework formalizing the space of reflexive polytopes and derive concentration inequalities for GNN embeddings; (Phase 2) empirically measure convergence rates as a function of training subset size, Gorenstein index, and face lattice depth via Wasserstein distance between embeddings; (Phase 3) validate that theory-guided subset selection achieves <5% error penalty on held-out h0>=3 classification.",
        "steps": [
          "Formalize input space: parametrize all 50,000 scanned KS polytopes as finite point configurations; compute Gorenstein index g, face lattice depth d, and lattice point density for each; stratify by Hodge pair h11 in {13, 18, 25, 30, 45, 60}.",
          "Phase 1 Theoretical derivation: (a) define combinatorial type space Omega of reflexive polytopes; (b) prove covering number N(epsilon, Omega) growth controlled by max face lattice depth; (c) adapt Rademacher complexity bounds (Bartlett & Mendelson 2002) to derive explicit convergence rate O(n^{-1/2} * g^{-1} * log(d)^{-1}) for GNN embedding L2 error; (d) document proofs in technical appendix.",
          "Phase 2 Empirical convergence: (a) train GNN models (2-3 layer message-passing, 32-dim embeddings) on random subsets n in {500, 1000, 2000, 5000, 10000} from each h11-stratum; (b) train 10 independent random-seed replicates per (h11, n) pair; (c) compute Wasserstein distance W2(rho_n, rho_full) between n-sized model embeddings and full-database model (proxy for asymptotic); (d) fit decay curve W2 ~ c*n^{-alpha} to extract empirical exponent alpha per h11 and Gorenstein-index subgroup; (e) compute 95% bootstrap CI on alpha.",
          "Phase 2 Gorenstein stratification: stratify h11=18 polytopes (n=50,000) by g in {1,2,3,4,5,6,7,8}; repeat convergence measurement within each g-stratum; compute speedup ratio (time to reach fixed W2 error at g vs g=1); verify >=30% speedup per unit g.",
          "Phase 2 Depth dependence: partition each h11-stratum by face lattice depth d in {3,4,5,6,7}; perform partial regression of log(W2) on log(d) controlling for n and g to empirically test log(d)^{-1} dependence.",
          "Phase 3 Screening validation: (a) train GNN on 15% random subset of h11=18 polytopes (n~7,500) with early-stopping threshold set by Phase 2 convergence rate prediction; (b) evaluate on held-out test of 5,000 h11=18 polytopes; compute h0>=3 classification metrics (accuracy, F1, false-negative rate) versus full-database baseline; (c) measure wall-clock time and FLOPs; compute speedup ratio; (d) repeat for h11 in {13, 25, 30}.",
          "Phase 3 Cross-Hodge extrapolation: train theory-guided subset GNN on h11<=60 data; test h0>=3 prediction on h11=100+ synthetic polytopes (generated via reflexive-polytope-preserving MCMC) to assess transfer and error graceful degradation.",
          "Sensitivity analysis: repeat Phases 2-3 under three architecture variations (1-layer, 3-layer, 4-layer GNNs) and two embedding dimensions (16-dim, 64-dim) to verify convergence exponent robustness.",
          "Validation: compute ground-truth h0 for 1,000 random KS polytopes via standard toric cohomology; measure ML prediction accuracy against ground truth."
        ],
        "tools": [
          "Kreuzer-Skarke database (104,000 reflexive polytopes with pre-scanned Hodge numbers)",
          "CyTools (Demirtas et al. 2021) for polytope combinatorics, Gorenstein index, face lattice",
          "PyTorch Geometric for GNN implementation",
          "POT/ot-torch library for Wasserstein distance W2 computation",
          "SciPy/NumPy for empirical process analysis and Rademacher complexity verification",
          "statsmodels for robust regression and uncertainty estimation",
          "Matplotlib/Seaborn for convergence curve visualization"
        ],
        "computational": true,
        "estimated_effort": "8-12 weeks: Phase 1 theory (2-3 weeks), Phase 2 convergence measurement on GPU cluster (4-5 weeks), Phase 3 screening and transfer tests (2-3 weeks), sensitivity and writeup (1-2 weeks). Requires GPU cluster access (4+ V100/A100 GPUs).",
        "data_requirements": "Kreuzer-Skarke database (200 MB, public); pre-computed Hodge numbers and c2 for 50,000 scanned polytopes; line bundle h0 labels for 1,000-5,000 validation polytopes (via existing CyTools or targeted cohomology calculation ~1-2 GPU-weeks).",
        "expected_positive": "Embedding Wasserstein distance exhibits power-law decay W2(n) proportional to n^{-alpha} with alpha in [0.40, 0.55], 95% CI excluding both 0 and 1; Gorenstein index shows >=30% multiplicative speedup (constant factor c reduction); face lattice depth d enters via log(d)^{-1} term (partial regression p<0.01); subset GNN achieves <5% relative h0>=3 false-negative rate increase versus full model; wall-clock speedup >=20x; transfer to h11=100+ shows <8% accuracy drop.",
        "expected_negative": "Wasserstein distance shows no power-law decay or alpha<0.35; Gorenstein index exhibits <15% speedup or erratic dependence; face lattice depth does not enter convergence bound; subset GNN shows >=8% false-negative rate increase; transfer to h11>100 shows >15% accuracy drop.",
        "null_hypothesis": "H0: Convergence rates of GNN embeddings depend only on training set size n via generic O(n^{-1/2}) scaling, independent of polytope combinatorics (Gorenstein index, face lattice depth). Equivalently, surrogate models cannot achieve <5% relative error penalty on h0>=3 screening without full-database information.",
        "statistical_test": "Part 1: Nonlinear least-squares fit of W2(n) ~ c*n^{-alpha}; bootstrap 95% CI on alpha (1000 resamples); reject H0 if CI excludes [0, 0.35] and overlaps [0.40, 0.55]. Part 2: ANCOVA on log(W2) with factors (h11, g, n, depth); test Gorenstein x n interaction p<0.05 (Bonferroni-corrected). Part 3: McNemar test on paired h0>=3 false-negative rates (subset vs full) alpha=0.05; 95% CI on relative FN increase must exclude 0 and be <5%. Part 4: Logistic regression of prediction error on h11; test extrapolation slope significance (p<0.05) and practical magnitude (<1% error per 10-point h11).",
        "minimum_detectable_effect": "Convergence exponent: Cohen's f^2 = 0.10 for Gorenstein x training-size interaction (n_replicates=10, alpha=0.05, power=0.80). Screening accuracy: 5% relative FN increase detectable with 5,000-sample test set and 80% power (binomial z-test). Gorenstein speedup: 30% multiplicative speedup quantifiable with 10 replicates per subset; 95% CI excludes 1.0 if true speedup >=30%.",
        "statistical_power_notes": "Phase 2: assume true alpha=0.45, constant c varies 2-3x by Gorenstein index. Fit 5 subset sizes x 10 seeds x 6 h11-strata x 8 Gorenstein indices = 2,400 model trainings. Power to detect alpha in [0.40, 0.55] vs alpha<0.35 at 95% confidence >0.95 (large-sample nonlinear regression). Phase 3: 5,000 test polytopes, ~10% h0>=3 prevalence = 500 positives; 95% CI on FN probability difference ±3-5% at power 0.80. Transfer: 6 h11 strata, polynomial fit; power to detect >1% error increase per 10-point h11 is ~0.85.",
        "limitations": [
          "Theoretical bounds derived under idealized measure assumptions; true KS polytopes may have pathological lattice structures not covered by theory.",
          "Wasserstein distance between finite embeddings is proxy for asymptotic; finite-width GNNs may not fully achieve theoretical regime at n=10,000.",
          "h0>=3 is binary classification proxy; full cohomology vector (h0, h1, h2, h3) requires more complex prediction.",
          "Gorenstein index and face lattice depth are correlated in KS database; causal separation requires synthetic polytope generation or instrumental variable approach.",
          "Ground-truth h0 computation limited to ~1,000 validation polytopes due to computational cost; labels may contain numerical errors from cohomology algorithms.",
          "Transfer test on synthetic h11=100+ polytopes may not match true KS distribution in unexplored regime.",
          "GNN architecture choice (message-passing, pooling) may not be optimal for polytope graph representation; sensitivity analysis mitigates but does not eliminate this."
        ],
        "requires_followup": "No wet-lab followup required; this is purely computational. However, to fully validate practical utility in landscape enumeration, the theoretical framework and validated convergence rates should be: (1) implemented in open-source polytope screening tool integrated into CyTools pipeline; (2) benchmarked on full KS database scan to quantify end-to-end speedup in h0>=3 occurrence detection across unexplored h11>60 polytopes; (3) extended to multi-target prediction (h11, h21, line bundle cohomology vector) via multi-task learning to assess whether convergence theory generalizes."
      },
      "keywords": [
        "graph neural networks polytope convergence",
        "reflexive polytope Gorenstein index",
        "empirical process theory Calabi-Yau",
        "convergence rates string compactification screening",
        "Rademacher complexity toric varieties"
      ],
      "gap_similarity": 0.7618817687034607,
      "gap_distance": 999,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.6,
      "final_score": 4.6
    },
    {
      "gap_concept_a": "Model-based Reinforcement Learning",
      "gap_concept_b": "RL algorithm",
      "source_question": "Can model-based reinforcement learning agents with learned forward models of Calabi-Yau polytope transformations discover high-h⁰ line bundles more efficiently than model-free RL algorithms, and what architectural properties of the learned environment model correlate with exploration efficiency in the bundle construction landscape?",
      "statement": "We hypothesize that learned forward models of polytope-to-cohomology maps enable model-based RL agents to discover line bundles with h⁰ ≥ 3 at least 5× more sample-efficiently (CyTools evaluations per discovery) than model-free RL agents, and that this efficiency gain is causally mediated by the learned model's ability to reduce epistemic uncertainty in the bundle action space through uncertainty-guided exploration.",
      "mechanism": "A learned neural forward model (GNN or transformer) trained on (polytope geometry, line bundle charge) → (h⁰, h¹, h², h³, h¹¹, h²¹) captures the deterministic but complex manifold structure of the cohomology prediction task. Within model-based RL (e.g., Dreamer or MuZero variants), this model enables: (1) rollout-based planning—estimate value of candidate bundles without invoking CyTools, reducing exploration cost; (2) uncertainty quantification—ensemble or Bayesian variants identify high-uncertainty regions, steering the policy toward informative samples; (3) trajectory optimization in learned latent space, where gradients flow through the differentiable model. Model-free RL must evaluate every candidate in the true environment, yielding no planning benefit. The causal direction is: learned model accuracy and uncertainty calibration → reduced CyTools call count → higher sample efficiency per discovery.",
      "prediction": "A model-based RL agent with a learned forward model will achieve ≥ 5-fold reduction in mean CyTools evaluations required to discover the first line bundle with h⁰ ≥ 3 on held-out polytopes (median ≤ 20 evaluations vs. ≥ 100 for DQN/PPO baselines), while discovering bundles of comparable or higher quality (h⁰ ≥ 3 at >80% discovery rate within budget).",
      "falsifiable": true,
      "falsification_criteria": "The learned model-based RL agent requires ≥ 100 CyTools evaluations to discover the first h⁰ ≥ 3 bundle on ≥ 50% of test polytopes (i.e., median sample count ≥ 100, no speedup >2×), OR the model-based agent discovers bundles of lower quality (mean h⁰ at final iteration <2.5) than model-free baselines despite comparable budget, indicating the learned model provides no effective guidance.",
      "minimum_effect_size": "Sample efficiency improvement: ≥ 5-fold reduction in median CyTools calls (from ≥100 to ≤20 per first h⁰≥3 discovery). Formal threshold: ratio of median evaluations (model-free / model-based) ≥ 5.0 with 95% CI not crossing 2.5. Discovery quality: mean h⁰ ≥ 2.8 for model-based vs. ≥ 2.7 for model-free (non-inferiority margin). Uncertainty calibration: model ensemble/Bayesian predictive entropy must correlate with prediction error (ρ > 0.40) and forward-model MAE on validation cohomology <0.25 per coordinate.",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Train a multitask GNN forward model on 40,000 ground-truth (polytope, line bundle) → cohomology tuples from completed KS scans; embed it into model-based RL agents (PILCO and MuZero variants) and model-free baselines (DQN, PPO); benchmark sample efficiency and discovery rate on 100 held-out polytopes with h¹¹ ∈ [13, 50].",
        "steps": [
          "Phase 1: Data curation. Retrieve 40,000 (reflexive polytope, line bundle) pairs from CyTools-analysed KS database subset with verified ground-truth h⁰, h¹, h², h³, h¹¹, h²¹. Stratify by Hodge pair to ensure balanced representation across h¹¹ ∈ [13, 128]. Hold out 100 polytopes (test set) chosen uniformly across Hodge spectrum.",
          "Phase 2: Feature engineering. For each polytope: (a) encode vertex matrix via SVD truncation (64 principal components); (b) compute and normalize face lattice depth, Gorenstein index, and lattice point density; (c) compute normalized intersection numbers κ_{abc} via ℓ₂ ball projection. For each line bundle: encode charge vector by Picard rank (32 integer features max). Normalize all features to μ=0, σ=1. Train/validation split: 32,000 / 8,000.",
          "Phase 3: Multitask GNN forward model. Design a graph neural network with two input branches: (i) toric face lattice graph (vertices = faces, edges = incidence) processed by 4-layer GraphSAGE with ReLU + batch norm (hidden dim 128); (ii) line bundle charge embedding (Picard rank dense layer + projection to 64 dims). Concatenate, pass through 3-layer dense network (128→128→256), then multitask output heads: h⁰ (regression), h¹/h² (regression), h¹¹/h²¹ (regression). Train with Adam (lr=1e-3), L2 loss + MSE, early stopping on validation MAE. Ensemble variant: train 5 random-seed copies for Bayesian uncertainty estimation (predictive variance from ensemble disagreement).",
          "Phase 4: Model-based RL agents. Implement two architectures: (a) PILCO-style: Gaussian process surrogate (or learned model) + trajectory optimization (CEM or iLQG) to maximize predicted h⁰; (b) MuZero-lite: learned model + 50-step lookahead tree search (UCB-based action selection in learned latent space). Action space: 2D line bundle charge vectors constrained to Picard lattice. Reward: r(h⁰) = clip(h⁰ / 3, 0, 1) (sparse reward at h⁰≥3 threshold).",
          "Phase 5: Model-free baselines. Implement DQN (dueling architecture, ε-greedy exploration ε=0.1) and PPO (actor-critic, 4 parallel workers). Both trained on real CyTools evaluations using same action space and reward.",
          "Phase 6: Benchmark protocol. For each of 100 test polytopes, run 5 seeds of each agent (model-based PILCO, model-based MuZero, DQN, PPO) with budget of 200 CyTools evaluations. Track: (i) iteration at first h⁰≥3 discovery (sample efficiency); (ii) mean h⁰ of discovered bundles; (iii) final best h⁰ found. Compute median sample count and 95% CI.",
          "Phase 7: Ablation studies. Rerun on test set with: (a) forward model trained without h¹¹/h²¹ tasks (cohomology-only); (b) deterministic model vs. ensemble (remove epistemic uncertainty); (c) random polytope subsample (2,000 training pairs) to assess data efficiency. Measure impact on sample efficiency ratio.",
          "Phase 8: Learned model analysis. Compute saliency of polytope features via gradient-based feature importance (∂h⁰/∂polytope_feature). Identify top-5 polytope invariants driving h⁰ variability. Visualize model's learned latent space via t-SNE on bundle embeddings colored by h⁰.",
          "Phase 9: Uncertainty calibration validation. On validation set: (a) compute predicted mean and epistemic variance for each (polytope, bundle); (b) compute actual error and compare to predicted epistemic variance (calibration curve, ECE metric); (c) measure correlation between model uncertainty and forward-model prediction error (ρ > 0.40 threshold)."
        ],
        "tools": [
          "CyTools (existing KS database queries and cohomology computations)",
          "PyTorch / TensorFlow (neural network training, GNN implementation via PyTorch Geometric)",
          "Stable-Baselines3 (DQN, PPO implementations)",
          "Custom Dreamer/PILCO implementation (model-based RL)",
          "scikit-learn (t-SNE, ensemble utilities)",
          "Kreuzer-Skarke database (public polytope repository, ~500k reflexive polytopes)",
          "NumPy/SciPy (numerical feature engineering)"
        ],
        "computational": true,
        "estimated_effort": "8–12 weeks: 1–2 weeks data curation + feature engineering; 2–3 weeks model training + hyperparameter tuning (GPU cluster, 50–200 GPU hours); 3–4 weeks RL agent implementation and benchmarking (100 polytopes × 5 seeds × 4 agents = 2000 RL runs, ~200–400 GPU hours); 1–2 weeks ablation and analysis. Wall-clock time depends on CyTools integration: if CyTools is called in-loop (evaluating real cohomology), budget 50–100 CPU hours for the 200k CyTools calls (2000 runs × 200 evals / run). Total compute: ~500 GPU-hours + ~100 CPU-hours.",
        "data_requirements": "Ground-truth cohomology data: 40,000 (polytope, line bundle, h⁰/h¹/h²/h³/h¹¹/h²¹) tuples computed via CyTools. Public resources: Kreuzer-Skarke database (reflexive polytopes), existing CY3 cohomology tables (arXiv databases, e.g., 1903.00009). Estimated download/compute: ~50 GB for polytope files + cohomology tables; 2–3 weeks to prepare (if KS scans already complete) or 6+ months (if full re-scan required—recommend reusing published data).",
        "expected_positive": "Model-based RL (PILCO or MuZero) achieves median ≤ 20 CyTools evaluations to first h⁰≥3 discovery, vs. median ≥ 100 for DQN/PPO (≥5× speedup). Ensemble forward model shows ρ > 0.40 between predicted epistemic variance and actual h⁰-prediction error. Ablation: removing h¹¹/h²¹ tasks reduces speedup to 2–3×; removing uncertainty degrades to 1.5–2×; small training set (2k samples) reduces speedup to 2–3×. Top polytope features (e.g., lattice point density, face lattice depth) show consistent high saliency across seeds.",
        "expected_negative": "Model-based agents require >100 CyTools evals (no >2× speedup), OR ensemble model shows ρ < 0.20 (epistemic variance does not correlate with error), OR model-based agents discover bundles with mean h⁰ < 2.5 (lower quality than model-free), OR ablation shows equal speedup with random polytope features (model learns no generalizable structure). Any of these refutes the hypothesis that learned models enable efficient model-based planning.",
        "null_hypothesis": "H₀: Model-based RL agents with learned forward models and model-free RL agents (DQN, PPO) achieve statistically indistinguishable sample efficiency (median CyTools evaluations per first h⁰≥3 discovery differ by <2×, 95% CI of ratio includes 1) when controlled for identical action space and hyperparameter budget.",
        "statistical_test": "One-sided Mann-Whitney U test (non-parametric, since sample efficiency may be skewed) comparing median CyTools call counts (model-based vs. model-free) across 100 test polytopes and 5 seeds each. H₀: medians equal. H₁: model-based median < model-free median by ≥5×. α = 0.05 (one-sided). Secondary: Kolmogorov-Smirnov test on full ECDF of call counts. Uncertainty calibration: Spearman ρ test (model ensemble variance vs. forward-model error) with α = 0.05, one-sided; threshold ρ > 0.40.",
        "minimum_detectable_effect": "Sample efficiency: ratio of medians ≥ 5.0 (model-free / model-based). At n=100 polytopes and 5 seeds (500 samples per arm), this effect is easily detectable via Mann-Whitney U with 95% power (PASS or similar software confirms). Uncertainty calibration: ρ > 0.40 with n=8000 validation samples is detectable at 99% power (correlation test). Discovery quality (non-inferiority): mean h⁰(model-based) ≥ mean h⁰(model-free) − 0.1 tested via one-sided paired t-test, α=0.05.",
        "statistical_power_notes": "Primary outcome (sample efficiency): N=100 polytopes × 5 seeds = 500 independent observations per treatment. Assumed effect size (ratio of medians) = 5× with SD ~2× (high variance in RL due to stochasticity). Mann-Whitney U test detects ratio ≥ 3.5× at 80% power; ≥5× at 99% power. Secondary outcomes (quality, calibration): same 500 observations. Uncertainty calibration: 8,000 (polytope, bundle) validation pairs used to compute ρ; Pearson/Spearman r test detects |ρ| > 0.40 at 99% power. No multiple comparison correction needed (single primary hypothesis). For model-based agents: convergence criterion is sample efficiency plateau (no improvement over last 20 RL iterations), indicating sufficient exploration budget.",
        "limitations": [
          "Ground-truth cohomology data limited to existing KS scans (~50k polytopes analyzed); generalization to unseen polytope families or higher-dimensional CY varieties untested.",
          "RL agents trained on sparse reward (h⁰ ≥ 3 threshold); reward shaping or curriculum learning not explored—may underestimate performance of carefully tuned baselines.",
          "CyTools call cost is deterministic (1–2 sec per evaluation); wall-clock speedup depends on model inference time (typically <10 ms) and RL overhead; no explicit latency modeling included.",
          "Calabi-Yau cohomology is deterministic (no aleatoric uncertainty), so learned model's epistemic uncertainty is the only source of exploration guidance; real stochasticity (e.g., approximation error, measurement noise) not modeled.",
          "Line bundle action space is discrete (integer charge vectors on Picard lattice); continuous RL relaxation may introduce artifacts; discrete optimization (branch-and-bound) not compared.",
          "Hyperparameter tuning for DQN/PPO baselines may be suboptimal; fair comparison requires grid search over α, ε, network architecture for all agents.",
          "Test set limited to h¹¹ ∈ [13, 50]; generalization to h¹¹ > 100 (rare, ultra-high-moduli CY3s) unknown."
        ],
        "requires_followup": "Computational proxy suffices for this stage. Validation on real string phenomenology requires: (1) verifying that discovered h⁰ ≥ 3 bundles correspond to physically viable moduli stabilization (Swiss cheese τ/V^{2/3} < 10⁻¹); (2) computing spectrum of matter generations and comparing to literature benchmarks; (3) extending to K3 fibration classification and F-theory uplift (requires dedicated CyTools analysis, ~1–2 weeks). These are confirmatory wet-lab-equivalent steps (heterotic string model building), not needed to validate the core hypothesis but essential for downstream impact claims."
      },
      "keywords": [
        "model-based reinforcement learning",
        "Calabi-Yau polytopes",
        "line bundle cohomology prediction",
        "learned forward models",
        "sample efficiency",
        "uncertainty-guided exploration"
      ],
      "gap_similarity": 0.62971431016922,
      "gap_distance": 6,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "performance assessment",
      "gap_concept_b": "Model utility",
      "source_question": "Does systematic performance assessment on out-of-distribution polytopes and held-out Hodge pairs causally determine whether an ML model trained on KS database subsets can reliably predict h⁰(L) and h¹¹ in unseen three-generation regimes, and what assessment framework minimizes false-positive predictions that waste computational resources?",
      "statement": "We hypothesize that out-of-distribution robustness on held-out Hodge pairs (h¹¹ ∈ {13, 18, 50, 128}) causally determines ML model deployment viability for h⁰(L) screening in unscanned three-generation Calabi-Yau threefolds, mediated through false-positive cost: models with OOD precision-recall degradation >20% relative to in-distribution validation will incur false-positive waste exceeding the computational savings threshold (>5 GPU-hours per 100 unscanned polytopes), rendering them unsuitable for deployment despite high test-set R².",
      "mechanism": "Stratified out-of-distribution evaluation on held-out Hodge pairs directly measures generalization to the deployment regime (unscanned polytopes at χ = −6). High in-distribution R² but poor OOD precision-recall indicates the model has overfit to combinatorial patterns specific to already-scanned polytopes, causing inflated false-positive predictions on novel Hodge-pair regions. This mechanism causally blocks deployment because false positives trigger wasted GPU-hours in geometry computation; causal direction is: OOD-robustness-gap → false-positive-rate → computational-waste → deployment-viability decision.",
      "prediction": "Models exhibiting >20% precision-recall degradation on held-out h¹¹ = 18 polytopes (relative to in-distribution validation, measured as ΔPR-AUC >0.15) will produce false-positive rates >15%, incurring expected waste >5 GPU-hours per 100 unscanned polytopes (at 195,000 unscanned target). Models with ≤20% OOD degradation will incur waste <3 GPU-hours per 100, crossing the deployment threshold and enabling ≥100x speedup over exhaustive computation.",
      "falsifiable": true,
      "falsification_criteria": "If a model achieves <10% precision-recall degradation on all four held-out Hodge pairs (h¹¹ in {13, 18, 50, 128}) but its false-positive rate on these held-out sets remains >20%, OR if a model with >25% OOD degradation is deployed and subsequently incurs <2 GPU-hours waste per 100 unscanned polytopes (indicating false-positive cost overestimated), the hypothesis is refuted. Additionally, if calibration error (ECE) on OOD splits is >0.10 yet the model false-positive rate matches prediction within ±3%, the causal link between OOD robustness and false-positive cost is severed.",
      "minimum_effect_size": "Models with Δ(OOD-Precision-Recall-AUC) ≤ 0.15 relative to in-distribution validation show ≥2-fold reduction in false-positive cost (from >5 to <2.5 GPU-hours per 100 unscanned polytopes). Pearson r between OOD-robustness-gap and false-positive-rate must exceed 0.65. Instrumental variable causal coefficient (OOD-robustness → false-positive-cost) must reach |β| > 0.40 at p < 0.05.",
      "novelty": 4,
      "rigor": 5,
      "impact": 5,
      "replication_risk": "low",
      "experiment": {
        "approach": "Conduct stratified out-of-distribution evaluation on KS database (104 χ = −6 polytopes with h¹¹ in {13, 18, 50, 128}) by training ML models (XGBoost, GNN, MLP) on 80% of polytopes, holding out 20% from each Hodge pair separately. Measure precision, recall, calibration (ECE), and false-positive rate on each held-out Hodge pair, construct OOD test sets based on Gorenstein index and face lattice anomalies, compute false-positive cost using a geometry-computation cost function, and fit instrumental variable regression to extract causal coefficients linking OOD robustness to deployment viability.",
        "steps": [
          "Partition KS database (104 χ = −6 polytopes) into four Hodge-pair strata: h¹¹ in {13, 18, 50, 128}. Obtain polytope feature vectors (normalized lattice point density, Gorenstein index, face lattice depth, second Chern class, GLSM charge matrix) via CyTools.",
          "For each Hodge pair, randomly split 80% training / 20% held-out validation. Train three candidate models (XGBoost with depth=7, Graph Neural Network with 3 message-passing layers, 2-hidden-layer MLP) on training split, predicting binary h⁰(L) ≥ 3 existence.",
          "On in-distribution validation (20% held-out, same Hodge pair), compute: accuracy, precision, recall, F1, PR-AUC, ROC-AUC, calibration ECE, Brier score.",
          "Construct OOD test set: select polytopes with Gorenstein index > 90th percentile AND/OR normalized lattice point density > 95th percentile AND/OR maximum face lattice depth > median + 2σ. Confirm these are geometrically distinct from training distribution via maximum mean discrepancy (MMD) test.",
          "On OOD test set (n approximately 200-500 polytopes spanning all four Hodge pairs), re-evaluate all three models: record PR-AUC, false-positive rate, false-negative rate, ECE.",
          "Compute OOD robustness gap: Δ(PR-AUC) = PR-AUC_in-dist − PR-AUC_OOD for each model and Hodge pair. Flag models with |Δ| > 0.15 as high-OOD-risk.",
          "For each model, estimate false-positive cost: assume false positive (model predicts h⁰(L) ≥ 3, actual is <3) triggers 10 GPU-hours of redundant Gröbner basis and cohomology computation. Compute cost_FP = FP-rate × 195,000 × 10 / 1000 = FP-rate × 1950 GPU-hours.",
          "Fit instrumental variable regression: log(false-positive-cost) ~ β₀ + β₁(OOD-robustness-gap) + β₂(ECE) + β₃(in-distribution-F1) + ε. Instrument OOD-robustness-gap with model-architecture (GNN=1, XGBoost=0, MLP=0.5) to isolate causal effect.",
          "Extract causal coefficient β₁ via two-stage least squares (2SLS). Test H₀: β₁ = 0 at α = 0.05. Compute 95% CI via bootstrap (n=10,000 resamples).",
          "Define deployment viability: model is deployable iff OOD-robustness-gap ≤ 0.15 AND false-positive-cost ≤ 3 GPU-hours per 100 unscanned polytopes (threshold = 5,850 total GPU-hours). Flag non-deployable models.",
          "On held-out validation set of 20-30 newly scanned polytopes (not used in prior training/evaluation), test whether deployment viability predictions from IV model correctly classify which screened models will achieve <15% false-positive rate in practice.",
          "Compute overall speedup: for deployable models, estimate expected computational savings = (time to exhaustive screening) − (time to ML prediction + false-positive recompute). Target speedup ≥100x."
        ],
        "tools": [
          "KS database (Kreuzer-Skarke, 195,000+ reflexive polytopes)",
          "CyTools (polytope feature extraction: Gorenstein index, lattice points, face lattice)",
          "scikit-learn (XGBoost, train_test_split, metrics)",
          "PyTorch and PyTorch Geometric (Graph Neural Network with toric divisor graph construction)",
          "statsmodels (2SLS instrumental variable regression)",
          "scikit-survival or scipy.stats (calibration plots, ECE)",
          "MAGMA or Macaulay2 (optional: validate subset of geometry predictions, 20-50 test cases)"
        ],
        "computational": true,
        "estimated_effort": "3-4 weeks compute: 1 week feature extraction and model training across 3 architectures × 4 Hodge pairs (parallelizable, approximately 48 GPU-days total). 1 week OOD robustness evaluation and cost estimation. 1 week IV regression, calibration analysis, validation on new polytopes. 1 week documentation and sensitivity analysis.",
        "data_requirements": "KS database (public, 195,000+ polytopes with Hodge pairs). CyTools polytope features (computable on commodity hardware, approximately 2 hours per 1000 polytopes). Geometry ground truth for h⁰(L) on approximately 100-200 reference polytopes (estimated 100-200 GPU-hours, can use published cohomology tables or recompute via Macaulay2, alternatively use partial ground truth from existing CyTools scans).",
        "expected_positive": "Models with OOD-robustness-gap ≤ 0.15 on held-out Hodge pairs (h¹¹ = 18 especially) show IV causal coefficient β₁ < −0.50 (OOD degradation inversely predicts false-positive cost), false-positive-cost <3 GPU-hours per 100 unscanned polytopes, and false-positive rate <15%. Deployment viability prediction on new polytopes achieves accuracy ≥80%. Selected model delivers ≥100x computational speedup.",
        "expected_negative": "If a model exhibits OOD-robustness-gap >20% on h¹¹ = 18 but false-positive-cost remains <2 GPU-hours per 100 unscanned (contradicting prediction), or if IV causal coefficient β₁ is not significantly different from zero (|β₁| < 0.25, p > 0.10), the hypothesis is falsified: OOD robustness does not causally drive false-positive cost, and assessment framework cannot gate deployment.",
        "null_hypothesis": "H₀: Out-of-distribution robustness (measured as precision-recall degradation on held-out Hodge pairs) does not causally predict false-positive rate or computational cost of ML model deployment. Formally: β₁ (IV coefficient linking OOD-robustness-gap to log false-positive-cost) = 0.",
        "statistical_test": "Two-stage least squares (2SLS) instrumental variable regression: regress log(false-positive-cost) on OOD-robustness-gap, using model-architecture as instrument. Test H₀: β₁ = 0 via t-test with Newey-West robust standard errors, α = 0.05, two-sided. Secondary test: Pearson correlation r(OOD-robustness-gap, false-positive-rate) with Bonferroni correction for multiple Hodge pairs (n_comparisons = 4). Require r > 0.65 at p < 0.01. Tertiary test: logistic regression predicting deployment-viability from OOD-robustness-gap, ECE, and in-distribution-F1. Require model AUC ≥ 0.85 on validation set.",
        "minimum_detectable_effect": "IV causal coefficient |β₁| ≥ 0.40 at p < 0.05 (interpreted: a 0.1-unit increase in OOD-robustness-gap reduces log false-positive-cost by 0.04, approximately 4% reduction in absolute cost). Pearson r ≥ 0.65 between OOD-robustness-gap and false-positive-rate. Deployment-viability logistic model AUC ≥ 0.85. False-positive-cost difference between high-OOD-risk (Δ > 0.20) and low-OOD-risk (Δ ≤ 0.15) models ≥2-fold (for example, 8 GPU-hours vs. 4 GPU-hours per 100 unscanned).",
        "statistical_power_notes": "Primary analysis: 2SLS regression with n = approximately 300-600 model-evaluation pairs (3 models × 4 Hodge pairs × approximately 25-50 OOD test polytopes per pair). Assuming R² = 0.30 (OOD-robustness and ECE explain 30% of false-positive-cost variance), two-sided t-test for β₁ with α = 0.05 and target power = 0.90 requires minimum detectable effect |β₁| approximately 0.40 at n approximately 100 model evaluations (achieved with 3-4 models × 4 Hodge pairs × approximately 10 replicates per architecture per pair). With planned n approximately 300-600, power >> 0.95. Secondary Pearson correlation test (n approximately 300 OOD predictions) achieves power = 0.90 to detect r > 0.65 (converted to z via Fisher transformation, SE approximately 0.06, t = r√(n−2)/(1−r²) approximately 11, easily exceeds α = 0.01 threshold). Deployment-viability logistic model: assume true effect size (OR of OOD-robustness-gap on viability) approximately 2-3 per unit gap. With n approximately 100 viability labels and approximately 50% prevalence, power approximately 0.85-0.90. Convergence criterion for instrumental variable estimation: 2SLS F-statistic (instrument relevance) > 10. Hansen J-test p > 0.05 (no overidentification bias).",
        "limitations": [
          "Limited sample size of χ = −6 polytopes (n approximately 104) constrains sample size per Hodge pair. Mitigation: use stratified bootstrap resampling to inflate statistical power, and validate findings on larger synthetic or semi-synthetic polytope library (generate polytopes with anomalous combinatorial features).",
          "Ground truth for h⁰(L) available only for approximately 50,000 scanned KS polytopes. Unscanned polytopes used in OOD evaluation lack labels, forcing use of proxy OOD tests (Gorenstein index, lattice point density) rather than true geometric OOD. Mitigation: validate predictions on small subset of newly scanned polytopes (20-30) via MAGMA or Macaulay2.",
          "Instrumental variable (model architecture) may be weak if all three architectures generalize similarly. Mitigation: add secondary instrument (training-data-augmentation: yes/no) or cross-validate causal coefficient robustness via alternative instruments.",
          "False-positive cost function (10 GPU-hours per FP) is an estimate. Actual cost varies by polytope dimension and computation backend. Mitigation: conduct sensitivity analysis across cost range (5-20 GPU-hours) and verify deployment viability threshold remains robust (±20%).",
          "IV regression assumes linearity between OOD-robustness-gap and log false-positive-cost. May miss nonlinear thresholds or saturation effects. Mitigation: plot residuals, test for specification via RESET test, fit polynomial terms if needed.",
          "External validity limited to χ = −6, h¹¹ in {13, 18, 50, 128} Hodge pairs. Generalization to other χ or elliptic fibrations requires follow-up study on expanded polytope strata."
        ],
        "requires_followup": "Wet-lab (geometry) follow-up: Validate predicted h⁰(L) ≥ 3 on 20-30 unscanned polytopes via MAGMA or Macaulay2 cohomology computation to confirm false-positive rate empirically matches prediction. On a sample of 10-15 polytopes marked as deployable by viability classifier, run full geometry computation (Gröbner basis, line bundle cohomology) and measure actual computational cost versus predicted cost to verify speedup claim (target: actual speedup within ±30% of predicted speedup)."
      },
      "keywords": [
        "out-of-distribution robustness",
        "Hodge pair generalization",
        "Calabi-Yau line bundle prediction",
        "false-positive cost",
        "deployment assessment framework"
      ],
      "gap_similarity": 0.6055427193641663,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.65,
      "final_score": 4.65
    },
    {
      "gap_concept_a": "Air pollution prediction",
      "gap_concept_b": "Prediction accuracy",
      "source_question": "Can prediction accuracy metrics derived from air pollution forecasting models be systematically adapted to validate machine learning predictors of Calabi-Yau threefold topological invariants, and does this transfer reveal domain-specific accuracy requirements for string compactification viability?",
      "statement": "We hypothesize that rank-based probabilistic evaluation metrics adapted from air pollution forecasting (continuous ranked probability score, prediction interval coverage probability) will correctly discriminate between high-generation (h⁰ ≥ 3) and low-generation line bundles on Calabi-Yau threefolds with >20% relative improvement in screening efficiency (rank@k intersection ratio) compared to standard regression MSE-based filtering, and that this discrimination power scales with normalized lattice point density and Gorenstein index rather than absolute prediction accuracy.",
      "mechanism": "Air pollution forecasting uses ranking metrics to evaluate probabilistic forecasts under model uncertainty and observation sparsity; CY3 cohomology prediction faces identical structural conditions (high-dimensional sparse feature space, no ground truth for 195k+ unscanned polytopes, tolerance for rank error over absolute error). Reframing CY3 prediction as a ranking problem allows CRPS and PICP to identify ML models that correctly order polytopes by generation number and moduli richness, enabling a pre-screener that routes expensive exact computations only to high-ranked candidates. The mechanism: (A) lattice point density and Gorenstein index encode coarse geometric constraints on cohomology spaces; (B) ranking metrics reveal whether learned models capture these constraints sufficiently to stratify the landscape; (C) if ranking accuracy is uncorrelated with absolute MSE, then standard regression approaches waste capacity modeling unimportant tail behavior.",
      "prediction": "When trained on 2,000 gold-standard CY3 geometries (with exact h⁰, h¹¹, Hodge numbers) and evaluated on 500 held-out polytopes, an XGBoost model optimized for CRPS minimization will achieve ≥85% prediction interval coverage (PICP) for h¹¹ within ±3 bins and will correctly rank-order the top 10% by h⁰ (true h⁰ ≥ 3) with ≥70% precision, while the same model trained with standard MSE will achieve ≤55% PICP and ≤55% precision, and the MSE-optimized model will require ≥30% more compute budget to match the CRPS model's screening efficiency.",
      "falsifiable": true,
      "falsification_criteria": "If CRPS-optimized and MSE-optimized models achieve statistically indistinguishable PICP (within 5 percentage points, p > 0.05 paired binomial test on 500 held-out polytopes), or if the CRPS model's rank@10 precision is ≤60% and does not exceed MSE model's precision by ≥10 percentage points on fresh computations, then rank-based metrics offer no practical advantage over regression for CY3 screening and the hypothesis is refuted.",
      "minimum_effect_size": "PICP ≥80% vs. ≤55% (25 pp difference); rank@10 precision ≥70% vs. ≤55% (15 pp difference); screening efficiency ratio (cost per h⁰ ≥ 3 discovery) CRPS/MSE ≤0.85 (15% relative cost reduction). Each threshold must be met independently at p < 0.05.",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Construct a curated gold-standard dataset of 2,500 CY3 geometries from the Kreuzer-Skarke database with exact cohomology invariants (h⁰, h¹¹, h²¹, line bundle cohomology vectors) computed via CyTools. Train and cross-validate two ML models (XGBoost and graph neural network) using identical feature sets but with different loss functions: one minimizing CRPS and PICP (ranking-aware), one minimizing MSE (standard regression). Evaluate both models on stratified held-out test set using pointwise accuracy metrics and ranking metrics (PICP coverage, CRPS, rank@k precision for h⁰ ≥ 3 threshold). Measure screening efficiency as the ratio of exact computations required to discover all h⁰ ≥ 3 polytopes in a synthetic pool of 10,000 unscanned candidates ranked by each model.",
        "steps": [
          "Compile and validate gold-standard CY3 dataset: curate ≥2,000 polytopes from KS database with existing CyTools-computed cohomology data; populate missing h⁰ entries via CyTools solver. Document Gorenstein index, lattice point density, face lattice depth, and Mori cone for each polytope. Partition into 60% train (1,500), 20% validation (500), 20% test (500); ensure stratification by h¹¹ decile and χ value.",
          "Engineer lightweight polytope features: compute normalized vertex matrix norm, Gorenstein index, face lattice depth (max distance in incidence graph), lattice point density (interior + boundary points / volume), dim(Mori cone), SR ideal height, and line bundle charge vector norm. Avoid expensive intersection number computation; focus on combinatorial features extractable in <1 second per polytope.",
          "Train baseline XGBoost regression model: hyperparameter tuning (max depth in {5,7,10}, learning rate in {0.01,0.05,0.1}) on training set minimizing MSE; evaluate on validation set; select best configuration.",
          "Train CRPS-optimized XGBoost: same hyperparameter grid, but replace MSE loss with CRPS loss (computed as mean absolute deviation between predicted and observed rank, normalized to [0,1]); use 10-fold cross-validation; select configuration minimizing CRPS plus PICP penalty term.",
          "Train GNN variant (optional secondary model): encode polytope face lattice as directed graph (nodes = faces, edges = incidence); use graph convolutional layers with polytope features as node attributes; train dual models (MSE and CRPS loss) using identical hyperparameter search.",
          "Evaluate on test set (500 polytopes): for each model and line bundle class (binned by h⁰), compute: (a) MAE, RMSE, median absolute error; (b) PICP and 90% prediction interval width for h¹¹ prediction; (c) CRPS across all polytopes; (d) rank@k precision for k in {10,20,50,100,200}.",
          "Screening efficiency benchmark: create synthetic candidate pool by sampling 10,000 unscanned polytopes from KS database (preferring h¹¹ ≥ 18). For each model, rank all 10,000 by predicted h⁰. Simulate oracle that runs exact cohomology computation in rank order; measure computations required to discover 50, 100, and 200 polytopes with true h⁰ ≥ 3. Compute cost ratio (MSE model cost) / (CRPS model cost).",
          "Statistical testing: for PICP and rank@k precision, compute 95% confidence intervals using binomial proportion test (500 test polytopes); test null hypothesis that CRPS and MSE models have equal population precision using paired McNemar test (n=500); for efficiency ratio, use bootstrap resampling of the 10,000 candidate pool (1,000 replicates).",
          "Sensitivity analysis: stratify test set by Gorenstein index (low/medium/high) and by lattice point density quartiles; re-compute PICP, CRPS, and rank@10 precision within each stratum to assess whether ranking advantage is uniform or concentrated in specific polytope families.",
          "Validation on fresh exact computations: select top 100 polytopes ranked by CRPS model from a disjoint subset of 500 uncomputed KS polytopes; run CyTools to obtain ground truth h⁰, h¹¹; verify that ≥70 have h⁰ ≥ 3 (confirming rank@10 precision of ≥0.70 on fresh data)."
        ],
        "tools": [
          "CyTools (cohomology computation for Calabi-Yau hypersurfaces)",
          "Kreuzer-Skarke database (reflexive polytope repository with 195k+ entries)",
          "XGBoost (gradient boosting regression with custom CRPS loss)",
          "PyTorch and PyGeometric (graph neural network backend for face lattice encoding)",
          "scikit-learn (train-test split, stratification, McNemar test, binomial CI)",
          "properscoring Python library (CRPS and PICP calculation)",
          "Pandas, NumPy, SciPy (feature engineering, statistical tests)",
          "CPU cluster or HPC with ≥10 cores (cross-validation parallelization)"
        ],
        "computational": true,
        "estimated_effort": "4-6 weeks (2 weeks data curation and exact cohomology computation on 2,000 polytopes; 1 week feature engineering; 2 weeks model training and hyperparameter search; 1 week evaluation and statistical tests).",
        "data_requirements": "Curated subset of Kreuzer-Skarke database (2,500 polytopes with exact h⁰, h¹¹ via CyTools); disjoint holdout set of 10,000+ unscanned KS polytopes for efficiency benchmark; polytope combinatorial data (vertex matrices, face lattices) freely available in KS database. No proprietary data required.",
        "expected_positive": "CRPS-optimized model achieves PICP ≥85% (vs. MSE ≤55%), rank@10 precision ≥70% (vs. MSE ≤55%), and screening cost ratio ≤0.85 (15% faster discovery of h⁰ ≥ 3 polytopes) on held-out test set, with all differences significant at p < 0.05. Fresh validation on 100 oracle-computed polytopes confirms ≥70% rank@10 precision. Stratified analysis shows advantage is sustained across Gorenstein index and density quartiles.",
        "expected_negative": "CRPS and MSE models show no significant difference in PICP (within 5 pp, p > 0.05), rank@10 precision (within 5 pp, p > 0.05), or screening cost ratio (within 10%, p > 0.05). Alternatively, rank@10 precision of CRPS model is ≤60% on held-out test or fresh validation, indicating ranking advantage is marginal and insufficient to justify reframing the problem.",
        "null_hypothesis": "H₀: rank-based probabilistic metrics (CRPS, PICP) offer no advantage over standard regression metrics (MSE, MAE) for screening CY3 polytopes. Formally: (PICP_CRPS - PICP_MSE ≤ 5 pp) AND (precision@10_CRPS - precision@10_MSE ≤ 5 pp) AND (cost_ratio_CRPS/MSE ≥ 0.95). We reject H₀ if all three inequalities are simultaneously violated at p < 0.05.",
        "statistical_test": "For PICP and rank@k precision: binomial proportion test (95% CI, n=500 test samples) for each metric; paired McNemar test to compare precision of two models on same test polytopes (alpha=0.05, two-sided). For screening efficiency: bootstrap confidence interval on 10,000-polytope pool (1,000 replicates) to estimate cost ratio and its 95% CI; one-sided t-test to reject H₀ that cost_ratio ≥ 0.95 (alpha=0.05). For MAE/RMSE: paired t-test (alpha=0.05, two-sided).",
        "minimum_detectable_effect": "PICP difference ≥15 percentage points (85% vs. 70%, achievable with n=500 and power=0.85 under binomial test); rank@10 precision difference ≥10 percentage points (70% vs. 60%, achievable with n=500); screening cost reduction ≥12% (ratio ≤0.88, detectable via bootstrap CI with 1,000 replicates and power ≥0.80 assuming moderate effect size in screening task).",
        "statistical_power_notes": "Test set size: n=500 polytopes, stratified by h¹¹ decile and chi value. For binomial test of proportion difference (PICP_CRPS=0.85 vs. PICP_MSE=0.60): power=0.95, alpha=0.05 (two-sided), assumed difference 0.25, yields required n≈200 per group; we use n=500 to enable stratified subgroup analyses. For McNemar test of paired precision: n=500, power=0.85, alpha=0.05 (two-sided), detectable odds ratio ≈1.5, which maps to approximately 7 percentage point precision difference. For bootstrap CI on cost ratio: 1,000 replicates on pool of 10,000 synthetic candidates; CI width ±0.05 is expected. Convergence criterion for model training: validation CRPS plateau or <0.1% improvement over 5 consecutive epochs, evaluated every 50 boosting rounds.",
        "limitations": [
          "Ground truth cohomology (h⁰, h¹¹, etc.) is expensive to compute; gold-standard dataset is limited to approximately 2,000 polytopes, which may not represent rare high-h¹¹ geometries (sparse in full KS database). Extrapolation to unscanned regions carries model uncertainty.",
          "Feature engineering excludes intersection numbers kappa_abc and second Chern class c2, which are expensive to compute but may improve ranking. The lightweight feature constraint is necessary for screening viability but may limit absolute prediction accuracy.",
          "Screening efficiency benchmark uses synthetic 10,000-polytope pool sampled from KS; oracle cost (exact computation time) is constant per polytope but in reality varies by Gorenstein index and h¹¹. Cost ratio may differ in practice.",
          "CRPS loss implementation requires ranking all training samples; computational cost scales as O(n log n) per epoch, potentially limiting ensemble size. Approximations (e.g., RankNet-style pairwise loss) may reduce cost but alter metric.",
          "Replication to other string theory compactification problems (K3 surfaces, Calabi-Yau fourfolds) is uncertain; generalization of ranking metrics across geometry types is unexplored.",
          "Validation on fresh 100 oracle-computed polytopes is limited; ideally would require several hundred for robust confirmation, but compute budget may be constrained."
        ],
        "requires_followup": "Once computational ranking-optimized model is validated on fresh data (step 10), the next phase would involve: (i) deploying the model as a real pre-screener on the full unscanned KS database (approximately 150k polytopes at h¹¹=18) to predict top-ranked candidates and filter to approximately 5,000 high-priority polytopes; (ii) running distributed CyTools cohomology computations on filtered subset to validate predicted rankings in bulk and measure realized screening speedup; (iii) investigating whether CRPS-optimized models trained on one chi class (e.g., chi=-6) transfer to other chi values or require retraining; (iv) exploring whether ranking advantage persists when predicting other Hodge numbers (h²¹, h¹) or multivariate cohomology vectors (h⁰, h¹, h², h³), which would broaden the practical impact beyond h⁰ generation counting alone."
      },
      "keywords": [
        "Calabi-Yau threefolds",
        "ranking metrics",
        "CRPS calibration",
        "screening pre-filter",
        "probabilistic forecasting transfer",
        "cohomology prediction"
      ],
      "gap_similarity": 0.7303358316421509,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "Machine learning pipeline",
      "gap_concept_b": "Pipeline synthesis",
      "source_question": "Can automated pipeline synthesis generate Calabi-Yau topology predictors that discover novel, domain-specific feature engineering strategies superior to hand-crafted polytope-based representations?",
      "statement": "We hypothesize that automated pipeline synthesis constrained by polytope-geometric validity rules discovers feature-engineering strategies that predict h⁰(L) and h¹¹ on reflexive polytopes with >15% lower MSE than hand-crafted baseline pipelines, by automatically composing non-obvious interactions between lattice-point density, face-lattice depth, and Gorenstein-index-aware binning that human domain experts have not manually engineered.",
      "mechanism": "Automated pipeline synthesis operates in a high-dimensional space of preprocessor and feature-transformer combinations constrained by topological validity (Gorenstein index preservation, GLSM charge consistency). By systematically exploring orderings and compositions of cheap geometric operations (dual extraction, normalized moment computation, face-lattice traversal), the algorithm discovers latent feature interactions—such as normalized lattice density weighted by face-lattice depth, or geometric means conditioned on SR-ideal rank—that better capture the intrinsic relationship between polytope combinatorics and Hodge/cohomology structure than independent hand-designed features. This occurs because the search space systematically samples feature compositions that humans, relying on intuition, skip.",
      "prediction": "A synthesized pipeline will achieve mean-squared error (MSE) on held-out h⁰(L) and h¹¹ prediction that is ≥15% lower than the best hand-crafted baseline (random forest on κ_{abc} + c₂ + vertex-count features), measured on a stratified test set of 500 polytopes with h¹¹ ∈ [13, 128] and verified cohomology vectors, with 95% confidence interval not overlapping zero.",
      "falsifiable": true,
      "falsification_criteria": "If the MSE of the best-synthesized pipeline does not achieve ≥15% improvement over the baseline (two-sided 95% CI on MSE difference includes zero or favors baseline), or if the top-3 synthesized pipelines all have identical feature orderings/components to existing hand-designed approaches, the hypothesis is refuted.",
      "minimum_effect_size": "MSE improvement ≥15% on h⁰(L) and h¹¹ joint prediction task; equivalently, if baseline achieves R² = 0.65, synthesized pipeline must achieve R² ≥ 0.74 on held-out test set (n=500).",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Implement a constraint-aware automated pipeline synthesis system (adapted from TPOT / Auto-sklearn) over a domain-specific component library for polytope preprocessing and feature engineering. Evaluate synthesized pipelines against hand-crafted baselines on h⁰(L) and h¹¹ prediction tasks across stratified subsets of the KS database with known ground truth, measuring both prediction accuracy (MSE, R²) and inference speed to validate the speedup claim for pre-screening.",
        "steps": [
          "Curate a training set of ~5,000 reflexive polytopes from the KS database with verified h¹¹, Hodge numbers, and line-bundle cohomology vectors (h⁰, h¹, h², h³). Stratify by h¹¹ ∈ [13, 50, 100, 128] to ensure representativeness across the three-generation landscape.",
          "Build a domain-specific component library consisting of: (A) Preprocessors: polytope normalization (vertex scaling, centring), dual-cone extraction, face-lattice graph construction, Gorenstein-index-aware binning, lattice-point enumeration. (B) Feature transformers: normalized lattice-point density (points/volume), face-lattice depth (max path length in Hasse diagram), moment invariants (up to 4th order), persistence-diagram rank, geometric mean of dual-polytope volumes, SR-ideal rank approximation, pairwise products and ratios of cheap geometric quantities. (C) Learners: random forest, XGBoost, Ridge regression.",
          "Implement constraint-aware pipeline synthesis: adapt TPOT or Auto-sklearn to enforce a constraint checker at each pipeline candidate: verify that Gorenstein index is preserved through transformations, that feature scaling does not violate GLSM charge structure, and that inference time on each polytope ≤50 ms (to ensure >100× speedup over toric-resolution). Use Bayesian optimization (Tree-structured Parzen Estimator) to search the composition space with early stopping if constraint violations exceed 10%.",
          "Train the synthesized pipeline search on the 5,000-polytope training set (5-fold stratified cross-validation) optimizing for MSE on (h⁰, h¹¹) joint prediction. Allow search to run for 24 GPU-hours. Extract the top-10 non-dominated synthesized pipelines (ranked by CV-MSE vs. inference time Pareto frontier).",
          "Analyze the top-10 pipelines for feature novelty: (i) identify feature transformations not present in hand-crafted baselines (e.g., products of lattice density × face-lattice depth, geometric means of dual-polytope properties), (ii) compute Fisher information criterion (FIC) for each discovered feature to rank mechanistic importance, (iii) visualize feature interaction graphs to identify non-obvious compositions.",
          "Evaluate all top-10 synthesized pipelines on a held-out stratified test set of 500 polytopes (disjoint from training/validation, with h¹¹ coverage matched to training distribution) against three baselines: (B1) published random forest on κ_{abc} + c₂ + vertex-count, (B2) published XGBoost on polytope geometry features, (B3) a simple linear model on hand-curated features (normalized density, dual volume, face-lattice depth). Compute MSE, R², and 95% bootstrap confidence intervals for each method.",
          "Report primary outcome: MSE improvement of best-synthesized pipeline vs. best baseline (e.g., improvement in R² from 0.65 to 0.74+). Perform two-sample permutation test (n=10,000 permutations) on MSE differences to establish significance at α=0.05.",
          "Compute inference speed on the test set for all methods. Report speedup factor (baseline compute time / synthesized pipeline time) and verify that ≥100× speedup claim is plausible (target: synthesized pipeline <50 ms/polytope on CPU, baseline ~5 seconds/polytope for toric resolution simulation).",
          "Conduct robustness analysis: re-run synthesis on 10 random stratified subsets of the 5,000-polytope training set (80/20 train/val splits) and check stability of discovered feature engineering patterns. If >7/10 runs produce pipelines with similar top-3 features, claim discovery is stable; otherwise flag high replication risk."
        ],
        "tools": [
          "TPOT (Tree-based Pipeline Optimization Tool) or Auto-sklearn with custom constraint checker",
          "CyTools Python library (polytope manipulation, Gorenstein index, face lattice, dual polytope)",
          "Kreuzer-Skarke database (~5,000 polytopes with pre-computed h¹¹, cohomology vectors)",
          "scikit-learn (random forest, XGBoost, Ridge, model selection)",
          "Giotto-tda or ripser (persistent homology of polytope fans)",
          "PyTorch / JAX (optional: for fast batched geometry computations)",
          "Ray Tune or Optuna (hyperparameter optimization backend)",
          "pandas, numpy, scipy (data wrangling, statistics)"
        ],
        "computational": true,
        "estimated_effort": "4-6 weeks: 1 week curating & validating training set (~5,000 polytopes with expensive cohomology verification via CyTools); 1 week building component library & constraint checker; 1.5 weeks running synthesis (24 GPU-hours per search iteration × ~5 searches); 1 week evaluating on held-out test set & statistical testing; 1 week robustness analysis & interpretation.",
        "data_requirements": "Kreuzer-Skarke database access (104 three-generation polytopes, ~50,000 scanned polytopes with h¹¹, Hodge numbers, line-bundle cohomology vectors pre-computed). For 5,000-polytope training set: vertex matrices, dual polytopes, face lattices, Gorenstein indices, κ_{abc} intersection numbers, c₂ values (all available in KS database or computable via CyTools in <1 min/polytope). A computational cluster with ≥1 GPU and ≥100 GB RAM for synthesis search.",
        "expected_positive": "Best-synthesized pipeline achieves ≥15% MSE reduction on h⁰(L) and h¹¹ prediction (e.g., R² improves from 0.65 to ≥0.74), with 95% CI not overlapping baseline. Top-10 pipelines share ≥2 novel feature interactions (not in hand-crafted baselines), such as 'normalized_lattice_density × face_lattice_depth' or 'log(dual_volume) / Gorenstein_index'. Inference speed is <50 ms/polytope, validating >100× speedup claim for pre-screening 195,000 unscanned polytopes.",
        "expected_negative": "Best-synthesized pipeline achieves <10% MSE reduction (or CI overlaps baseline). Top-10 pipelines are dominated by hand-curated features (normalized density, dual volume, face depth) with no novel feature products or non-obvious orderings. Inference speed is >500 ms/polytope, negating speedup advantage. Robustness analysis shows <4/10 runs produce consistent feature patterns (high variance across random subsets).",
        "null_hypothesis": "H₀: Automated pipeline synthesis does not discover feature engineering strategies that predict h⁰(L) and h¹¹ with lower MSE than hand-crafted baselines; the best synthesized pipeline achieves MSE ≤ MSE_baseline ± confidence interval width or discovers only combinations of features already hand-engineered by domain experts.",
        "statistical_test": "Two-sided permutation test on MSE differences between best-synthesized and best-baseline method (n=10,000 permutations, α=0.05). Report Hedges' g (effect size) and 95% bootstrap confidence intervals on MSE improvement. For robustness, use Benjamini-Hochberg correction if testing multiple top-10 pipelines (adjusted α=0.05).",
        "minimum_detectable_effect": "R² improvement of ≥0.09 points (from 0.65 → 0.74) or absolute MSE reduction of ≥8% on h⁰/h¹¹ prediction. For a baseline MSE of ~2.5 (typical on h⁰ in [0,10]), minimum detectable effect is ≥0.2 MSE units. Power analysis: assuming moderate effect size (Cohen's d=0.4), with n=500 test samples and α=0.05 (two-sided), we achieve >90% power to detect this difference.",
        "statistical_power_notes": "Test set size n=500 (stratified by h¹¹ quintiles) provides 100 samples per stratum. For h⁰ prediction (continuous target, range ~[0,10]), assuming baseline MSE=2.5 and synthesized MSE=2.3 (8% improvement), Cohen's d ≈ 0.4. Two-sided t-test with α=0.05, power=0.90 requires n≈180/group; we have n=500 overall, giving >99% power. For MSE comparison via permutation test (non-parametric), n=500 provides robust p-value estimation. Cross-validation during synthesis uses 5-fold stratified CV on 5,000 train samples, ensuring each fold has ~1,000 polytopes per stratum for stable performance estimates.",
        "limitations": [
          "Synthesis is constrained to the component library curated by experts; if a novel feature type (e.g., a quantum-geometric invariant) exists outside the library, it cannot be discovered. Mitigation: periodically add new components based on domain literature.",
          "Training data (~5,000 polytopes) is a small fraction of the 195,000+ unscanned space; synthesized pipelines may not generalize to unverified regions of the landscape. Mitigation: test robustness on held-out test set stratified by h¹¹; external validation on a separate scanning campaign (future work).",
          "Inference speed target (<50 ms/polytope) may be tight if the synthesized pipeline includes expensive operations (e.g., persistent homology computation). Mitigation: add computational cost as a hard constraint in the synthesis search; penalize pipelines exceeding the budget.",
          "The definition of 'hand-crafted baselines' may be subjective; if baselines already encode the discovered features, the comparison is not fair. Mitigation: explicitly list baseline features before running synthesis; require synthesized pipelines to include ≥1 feature not in any baseline.",
          "Ground truth h⁰(L) and h¹¹ values are expensive to compute; may contain errors for some polytopes. Mitigation: cross-validate cohomology vectors using multiple independent CyTools scans for a 10% random sample of training data.",
          "Replication across different AutoML frameworks (TPOT vs. Auto-sklearn) is not tested; results may be framework-dependent. Mitigation: run synthesis with two independent tools if resources allow."
        ],
        "requires_followup": "If the synthesized pipeline discovers a feature engineering strategy with strong predictive signal (R² > 0.70), validate the top-3 discovered pipelines by computing h⁰(L) and h¹¹ via expensive toric-resolution on 50 polytopes from the h¹¹=18 unscanned stratum (currently bottleneck). Compare predicted vs. verified values to confirm out-of-sample accuracy and confirm that the pre-screening speedup (>100×) translates to real computational savings in a production scanning campaign. This wet-lab-equivalent step (expensive geometric computation) would fully validate the practical impact of the pipeline synthesis method."
      },
      "keywords": [
        "automated machine learning",
        "pipeline synthesis",
        "Calabi-Yau topology prediction",
        "polytope feature engineering",
        "hyperparameter optimization",
        "constraint-aware AutoML"
      ],
      "gap_similarity": 0.7228508591651917,
      "gap_distance": 9,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "benchmark study",
      "gap_concept_b": "Benchmark dataset",
      "source_question": "Does the absence of a standardized, validated CY3 dataset with ground-truth h⁰ and Hodge number labels prevent reliable benchmarking of ML architectures, and can constructing such a dataset enable systematic comparison of polytope-to-cohomology prediction methods?",
      "statement": "We hypothesize that a standardized, computationally validated benchmark dataset with stratified sampling across polytope complexity, Gorenstein index, and Hodge diamond class will enable systematic comparison of ML architectures for h⁰ prediction, and that such benchmarking will reveal that graph neural networks outperform classical regressors by at least 15% in ROC-AUC when controlling for feature informativeness.",
      "mechanism": "The mechanism operates on two levels: (1) *infrastructure causality*: a curated, ground-truth dataset enables controlled comparison that isolates architecture effects from confounds (data quality, feature engineering, train/test contamination), which fragmented ad-hoc studies cannot achieve; (2) *inductive bias causality*: GNNs encode polytope face lattice adjacency and Mori cone structure as implicit features, whereas classical regressors require explicit hand-crafted feature extraction—this architectural difference should produce larger gains on high-complexity polytopes where lattice-geometric structure is most predictive of cohomology. The causal chain is: benchmark dataset → removes evaluation confounds → enables fair architecture comparison → reveals inductive bias advantages.",
      "prediction": "When trained and evaluated on a stratified benchmark dataset of 3,000 polytopes (500 per Gorenstein index ∈ {1, 2, 3, 4, 5, 6}) with 30 line bundles per polytope (total 90k h⁰ labels), a graph neural network architecture will achieve ≥68% ROC-AUC for binary classification (h⁰ ≥ 3 vs. h⁰ < 3) on a held-out test set, compared to ≤53% for random forest and ≤55% for XGBoost when all three use identical features (polytope vertex matrix, face lattice incidence, Gorenstein index, h¹¹ class).",
      "falsifiable": true,
      "falsification_criteria": "The hypothesis is refuted if, after controlling for feature set (same input to all models), the GNN ROC-AUC is <60% or falls within 3% of the random forest baseline (i.e., GNN ≤ RF + 0.03), or if ROC-AUC for both GNN and RF exceeds 70%, indicating that explicit feature engineering (not architecture) drives performance and GNNs provide no structural advantage.",
      "minimum_effect_size": "GNN ROC-AUC ≥ 68% with RF ≤ 55%; absolute difference ≥ 13 percentage points on the same validation set; statistical significance at p < 0.01 by DeLong test (two-sided) on 5-fold cross-validation averaged performance; power ≥ 90% to detect 13pp ROC-AUC difference with n=600 test samples (expected from 3,000 dataset × 0.2 test split).",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Construct a curated, ground-truth CY3 benchmark dataset by computing h⁰ for line bundles on 3,000 polytopes via toric cohomology (using CyTools or SageMath), stratify by Gorenstein index and h¹¹ class, standardize feature extraction into a fixed schema (polytope matrix, face lattice, intersection numbers), and perform systematic head-to-head comparison of 5 ML architectures (random forest, XGBoost, MLP, GNN, transformer) on the same train/val/test splits using ROC-AUC and F1 as primary metrics.",
        "steps": [
          "Query Kreuzer-Skarke database and PALP-generated polytopes; filter for 3,000 reflexive polytopes stratified by Gorenstein index g ∈ {1, 2, 3, 4, 5, 6} (500 per class) and h¹¹ distribution representative of known CY3 Hodge pairs (e.g., h¹¹ ∈ {13, 18, 25, 39, 50, 101, 128}).",
          "For each polytope: compute Mori cone generators, second Chern class c₂, intersection tensor κ_{abc}, and dual polytope structure using CyTools or Sage; extract GLSM charge matrix and face lattice incidence matrix.",
          "For each of 30 randomly chosen line bundles per polytope (total 90,000 bundles): compute h⁰(L), h¹(L), h²(L), h³(L) using toric sheaf cohomology (Bott algorithm or direct resolution via Macaulay2/Singular). Record ground-truth labels. [CPU-intensive: ~2–4 weeks on 64-core cluster, or ~3 months on single workstation.]",
          "Standardize feature extraction into tabular format: (polytope ID, Gorenstein index, h¹¹, h²¹, lattice point density, face lattice statistics [max depth, num faces, Euler char], normalized intersection numbers, polytope volume, normalized dual volume). Store as HDF5 + CSV for reproducibility.",
          "Split dataset: stratified train/val/test (60/20/20) preserving Gorenstein index and h¹¹ distributions; use k-fold (k=5) cross-validation.",
          "Implement reference models: Random Forest (100 trees, max depth=15), XGBoost (max depth=6, learning_rate=0.1, n_estimators=500), MLP (3 hidden layers 128→64→32, ReLU, dropout=0.3), GNN (3-layer graph convolution with face lattice as input graph, node features = face dimension + volume), Transformer (4-head attention, 2 layers, polytope vertices as token sequence).",
          "Train all models with identical hyperparameter search (Bayesian optimization, 50 trials per model) on binary task: h⁰(L) ≥ 3 vs. h⁰(L) < 3 (threshold chosen to match string generation count, ~50% class balance).",
          "Evaluate: ROC-AUC, F1 score, precision, recall, confusion matrix on held-out test set. Compute 95% confidence intervals via stratified bootstrap (1,000 resamples). Use DeLong test for pairwise ROC-AUC significance (two-sided, α=0.01).",
          "Ablation study: retrain GNN with progressively removed features (remove face lattice → use polytope matrix only; remove Gorenstein index; remove all topological features). Document how performance degrades, confirming that lattice structure is the driver of GNN advantage.",
          "Analyze failure modes: identify polytope classes (by h¹¹, Gorenstein index, fan complexity) where all models underperform; visualize polytope-level error scatter.",
          "Release dataset, code (PyTorch/TensorFlow implementations), baseline results, and interactive leaderboard (Hugging Face Spaces or public repo)."
        ],
        "tools": [
          "CyTools (polytope combinatorics and toric cohomology)",
          "SageMath or Macaulay2 (toric sheaf cohomology computation, exact h⁰ ground truth)",
          "Kreuzer-Skarke PALP database (polytope source)",
          "PyTorch Geometric (GNN implementation)",
          "scikit-learn (random forest, preprocessing)",
          "XGBoost Python library",
          "HuggingFace Transformers or PyTorch (transformer baseline)",
          "Optuna (hyperparameter search)",
          "scikit-learn metrics (ROC-AUC, DeLong test)",
          "Pandas, NumPy, HDF5 (data handling and versioning)"
        ],
        "computational": true,
        "estimated_effort": "8–12 weeks: 4 weeks ground-truth computation (parallelizable; can scale to 2 weeks on 64-core cluster), 2 weeks feature extraction and dataset curation, 3 weeks model training and cross-validation, 1 week ablation and failure analysis, 1 week documentation and release.",
        "data_requirements": "Kreuzer-Skarke database (free, ~500k polytopes); CyTools (free, open-source); Sage + Macaulay2 (free); compute budget: ~10k core-hours for ground-truth h⁰ computation (achievable on shared university cluster or via cloud SPOT instances). Output: ~1–2 GB dataset (3k polytopes × 30 bundles × feature vectors + labels).",
        "expected_positive": "GNN achieves 68–72% ROC-AUC; random forest achieves 50–56% ROC-AUC; XGBoost achieves 52–58% ROC-AUC; all models trained on identical features; difference significant at p < 0.01 by DeLong test; GNN-RF difference 12–22 pp; ablation study confirms that face lattice incidence accounts for ≥8pp of GNN advantage; GNN outperforms RF across ≥5 of 6 Gorenstein classes.",
        "expected_negative": "GNN ROC-AUC <60% or within 3pp of RF; all models converge to 65%+ ROC-AUC (indicating explicit features dominate and architecture irrelevant); RF+hand-crafted lattice features matches GNN (indicating no GNN advantage); DeLong test p > 0.05; ablation study shows removing face lattice has <2pp impact on GNN; GNN fails on high-Gorenstein polytopes (g ≥ 5), indicating method sensitivity to polytope type.",
        "null_hypothesis": "H₀: When trained on identical features derived from polytope combinatorics, GNN and classical regression architectures (random forest, XGBoost) achieve statistically indistinguishable ROC-AUC (|ROC-AUC_GNN − ROC-AUC_RF| ≤ 0.03) for h⁰ ≥ 3 binary classification, indicating that inductive bias confers no advantage over explicit feature engineering.",
        "statistical_test": "Primary: Two-sided DeLong test (non-parametric, comparing ROC curves) for GNN vs. RF and GNN vs. XGBoost, α = 0.01, applied to 5-fold cross-validation average ROC-AUC. Secondary: paired t-test on per-fold AUC differences (n=5 folds, robust to small n given large per-fold test set ~600 samples). One-sided Wilcoxon signed-rank test for ablation study (GNN with lattice features vs. without), α = 0.05.",
        "minimum_detectable_effect": "Absolute ROC-AUC difference ≥ 13 percentage points (GNN vs. RF), effect size r = 0.30 (medium). Power analysis: with ~600 test samples per fold, standard error of AUC ≈ 0.015, 13pp difference = 0.13 / 0.015 ≈ 8.7σ, giving >99% power at α=0.01. For smaller differences (5pp ≈ 0.05), power = 85% with same n and α.",
        "statistical_power_notes": "Assumed effect size (prior from literature): GNN outperforms classical methods by 10–15% on graph-structured problems; assumed correlation between polytope features and h⁰ ≈ moderate (r ∈ 0.3–0.5, based on prior CY3-ML studies). Sample size n=3,000 polytopes (60/20/20 split → 600 test per cross-val fold) is driven by: (1) requirement to stratify across 6 Gorenstein indices (≥100 per class), (2) h¹¹ coverage (≥200 per major class to detect class-level interactions), (3) power >80% to detect 13pp ROC difference (achieved with n=600; smaller differences would require n>2,000). For 5-fold CV, effective test pool size is 3,000 × 0.2 = 600 unique samples per fold, giving SE(AUC) ≈ √(AUC(1−AUC)/n) ≈ 0.015 at AUC ≈ 0.65, sufficient to resolve 13pp differences with >95% power (Z-test, one-sided).",
        "limitations": [
          "Ground-truth h⁰ computation is CPU-intensive; feasibility limited to ~3,000–5,000 polytopes before computational cost becomes prohibitive. Larger dataset (e.g., 10,000 polytopes) would require external computing allocation or GPU acceleration of toric cohomology (not yet standardized).",
          "Feature selection is not data-driven; face lattice depth, lattice point density, etc. are chosen a priori from algebraic geometry domain knowledge. Hidden features (e.g., subtle fan geometry properties) may exist but are not captured, potentially limiting model performance ceiling.",
          "Line bundle selection (30 per polytope) is random; may not represent the distribution of line bundles naturally arising in string compactifications (e.g., those with small volume or canonical properties). Systematic sampling by bundle properties could change results.",
          "GNN architecture is generic (standard graph convolution); polytope-specific inductive biases (e.g., geometric group actions, toric symmetries) are not encoded. Custom architectures might show larger or smaller advantages.",
          "Benchmark restricted to binary classification (h⁰ ≥ 3 vs. <3); regression to exact h⁰ values or multi-class (by h⁰ = 1, 2, 3, 4, ≥5) may show different architecture rankings.",
          "Generalization to polytopes outside the Kreuzer-Skarke database (e.g., non-reflexive polytopes, higher dimensions) is untested; benchmark may not reflect out-of-distribution robustness.",
          "Replication by independent teams requires access to CyTools and computational resources; potential friction if CyTools API changes or if toric cohomology algorithm variations produce slightly different h⁰ values (though this would be a separate calibration issue)."
        ],
        "requires_followup": "If GNN demonstrates substantial advantage (≥15pp ROC-AUC), conduct wet-lab style validation by computing h⁰ for a small set of high-confidence GNN predictions (e.g., polytopes where GNN predicts h⁰ ≥ 3 with >90% confidence but RF predicts <2) using independent toric cohomology tools (e.g., direct polyhedral resolution in Singular or Macaulay2, or explicit line bundle stability analysis). This would confirm that GNN gains are not algorithmic artifacts but reflect true geometric understanding. Additionally, apply trained GNN to the remaining 195,000+ unscanned KS polytopes; human expert review of a random subset (n=50–100) of high-confidence predictions would validate practical utility for the screening pipeline."
      },
      "keywords": [
        "Calabi-Yau threefold machine learning",
        "benchmark dataset standardization",
        "graph neural networks cohomology prediction",
        "line bundle h⁰ classification",
        "toric geometry feature extraction",
        "reflexive polytope database"
      ],
      "gap_similarity": 0.7065240144729614,
      "gap_distance": 11,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "Quantized Neural Network",
      "gap_concept_b": "n-BQ-NN",
      "source_question": "Can shift-based quantization (n-BQ-NN) be adapted to predict Calabi-Yau threefold topological invariants (h⁰, h¹¹, h²¹) and line bundle cohomology vectors from polytope combinatorics with sub-millisecond latency on resource-constrained hardware (FPGA/edge devices), while maintaining prediction accuracy within 5% of full-precision models?",
      "statement": "We hypothesize that shift-based binarized neural networks (n-BQ-NN) with post-training quantization and FPGA-native shift-and-accumulate operations achieve sub-millisecond inference latency on polytope-derived topological invariants (h⁰, h¹¹, h²¹) with prediction accuracy within 5% of full-precision models, enabled by weight sparsity and discrete lattice-structure priors inherent to toric geometry inputs.",
      "mechanism": "Shift-based quantization exploits two properties of CY3 prediction: (1) polytope features are natively discrete (lattice point counts, face dimensions, Gorenstein index), reducing quantization-induced distributional shift; (2) binarized weights and 4-8-bit activations map directly to FPGA lookup-table (LUT) shift-and-add operations, eliminating floating-point arithmetic overhead. Knowledge distillation from a full-precision teacher preserves decision boundaries in low-precision space because the underlying polytope feature manifold is lower-dimensional than the input feature space. FPGA LUT depth and shift registers are fundamentally faster for integer operations than CPU vector-floating-point pipelines, yielding 10-100x wall-clock speedup at modest accuracy cost (<5%) when the model is trained on discretization-aware loss functions.",
      "prediction": "A n-BQ-NN model with binarized weights, 4-bit post-ReLU activations, and shift-and-accumulate operations on an FPGA will achieve inference latency <1.0 ms per polytope (batch size 1) while maintaining per-sample h⁰ prediction accuracy >=95% and mean absolute error on h¹¹ <=2 (relative to full-precision baseline), measured across a held-out test set of 2,000 KS polytopes stratified by Hodge pair.",
      "falsifiable": true,
      "falsification_criteria": "If FPGA latency exceeds 1.5 ms per polytope, OR if quantized model h⁰ prediction accuracy falls below 92% (>3% drop from full-precision baseline), OR if mean absolute error on h¹¹ exceeds 3 (indicating systematic topological misclassification), the hypothesis is refuted. Additionally, if the quantized model achieves <50% h⁰ exact-match accuracy on any Hodge pair stratum (h¹¹ >=50), it fails to preserve decision structure.",
      "minimum_effect_size": "FPGA latency speedup >=10x relative to CPU baseline (full-precision MLP on CPU: ~10-50 ms per polytope, target quantized FPGA: <1 ms); prediction accuracy drop <=5% (h⁰ exact match: 97% full-precision to >=92% quantized); h¹¹ MAE <=2 (threshold for generation-level discrimination: h¹¹ in {13, 19, 25, 31}).",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Build a three-stage pipeline: (1) curate and preprocess approximately 10,000 KS polytopes with ground-truth h⁰(L) and Hodge number labels; (2) train a full-precision baseline MLP, then apply post-training quantization and knowledge-distillation fine-tuning; (3) implement n-BQ-NN on FPGA hardware simulator and real FPGA board, benchmark latency and accuracy against CPU baseline and full-precision models. Measure speedup, accuracy retention, and scalability to batch inference on the full unexplored KS subset.",
        "steps": [
          "Curate dataset: Extract 10,000 polytopes from KS database stratified by h¹¹ in range 13-128. Compute ground truth via CyTools: h⁰(L) via exact cohomology, h¹¹, h²¹, and Euler characteristic. For at least 3 representative line bundle charge vectors per polytope, compute cohomology vectors (h⁰, h¹, h², h³).",
          "Feature engineering: For each polytope, compute polytope-level features (lattice point density, volume, face lattice depth, Gorenstein index, dimension of singular set) and line-bundle-specific features (charge vector relative to Kahler basis, projection onto Mori cone, divisor intersection numbers). Normalize all features to zero mean, unit variance.",
          "Train full-precision baseline: Fit an MLP with 3 hidden layers of 256 neurons each and ReLU activations on 8,000 training polytopes to predict (h⁰, h¹¹, h²¹) and line bundle cohomology vectors. Use MSE loss for Hodge numbers and cross-entropy for h⁰ classification. Validate on 1,000 dev polytopes; test on 1,000 held-out polytopes stratified by Hodge pair.",
          "Post-training quantization: Apply symmetric weight quantization mapping weights to signed 8-bit integers via min-max scaling. Quantize post-ReLU activations to 4 bits via percentile-based clipping at 99th percentile. Compute scaling factors per layer and per-channel activation quantization.",
          "Knowledge distillation fine-tuning: Initialize quantized student network with quantized weights. Train for 10 epochs with distillation loss: alpha L_task(student, labels) + (1-alpha) L_KL(softmax(student logits / T), softmax(teacher logits / T)) where T=4 and alpha=0.7. Use SGD with learning rate 0.01 and cosine annealing.",
          "Implement n-BQ-NN shift logic: Convert binarized weight multiplications to sign-function outputs encoded as 0/1. Replace quantized activations with shift-and-accumulate operations using precomputed bit-shifts for powers of 2 or lookup tables. Generate FPGA HLS code via custom Python-to-Verilog transpiler mapping shift operations to LUT-based adders.",
          "FPGA deployment and latency measurement: Synthesize quantized model onto Xilinx Zynq or equivalent FPGA board. Measure single-polytope inference latency (batch size 1, no data transfer overhead) via on-chip timer repeated 10,000 times. Measure batch latency at sizes 1, 10, 100, 1000 to assess throughput. Compare wall-clock time against CPU baseline (standard MLP on Intel i7 or ARM CPU) for the same inference task.",
          "Accuracy evaluation: On held-out test set of 1,000 polytopes stratified by h¹¹ values, compute: per-polytope exact-match accuracy for h⁰ classification; mean absolute error on h¹¹ and h²¹; per-Hodge-pair accuracy stratification; confusion matrices for h⁰ in {0, 1, 2, 3, >=4}.",
          "Speedup and efficiency metrics: Report latency speedup as (CPU latency) divided by (FPGA latency) in wall-clock milliseconds per polytope. Report accuracy drop as (full-precision baseline accuracy) minus (quantized model accuracy) expressed as percentage points. Report energy efficiency inferred from FPGA power consumption during inference versus CPU power draw.",
          "Ablation study: Train separate quantized models with 4-bit, 8-bit, and 16-bit activation quantization using both full weights and binarized weights. Measure accuracy-latency Pareto frontier to validate that n-BQ-NN with binarized weights and 4-bit activations is the optimal point for this task.",
          "Batch inference on unexplored KS subset: If latency and accuracy thresholds are met, deploy quantized model to pre-screen 50,000 unexplored polytopes at h¹¹=18 stratum for h⁰(L) >=3, predicting which deserve expensive exact cohomology computation. Record model prediction, then validate with ground-truth exact cohomology on a random 1% sample. Report false-positive rate and false-negative rate."
        ],
        "tools": [
          "CyTools in Sage algebra system for ground-truth cohomology computation",
          "Kreuzer-Skarke database for polytope data and Hodge numbers",
          "PyTorch or TensorFlow quantization APIs (torch.quantization, tensorflow.lite quantization)",
          "Xilinx Vivado HLS and Vivado Design Suite for FPGA synthesis and timing closure",
          "FPGA boards such as Xilinx Zynq ZCU104 or Virtex-7 with sufficient LUT and DSP slices",
          "Numpy, Pandas, scikit-learn for feature preprocessing and ablation studies",
          "Custom Python-to-HLS transpiler for automatic shift-and-accumulate code generation",
          "Optuna or Ray Tune for hyperparameter tuning of quantization bit widths"
        ],
        "computational": true,
        "estimated_effort": "12-16 weeks: data curation and ground truth computation (2-3 weeks, parallelizable CyTools calls); baseline training and quantization (3-4 weeks); FPGA synthesis and board bring-up (4-5 weeks); ablation studies and batch inference (2-3 weeks).",
        "data_requirements": "Approximately 10,000 KS polytopes with precomputed Hodge numbers from the KS database. Ground-truth h⁰(L) and line bundle cohomology vectors for at least 3 representative line bundles per polytope computed via CyTools and cached for reuse. Polytope feature matrices (vertex, face lattice, Mori cone) extracted from reflexive polytope data. Total dataset size approximately 500 MB; FPGA bitstream approximately 200 MB.",
        "expected_positive": "Quantized n-BQ-NN achieves <1.0 ms latency per polytope on FPGA, achieves >=92% h⁰ exact-match accuracy, and h¹¹ MAE <=2, demonstrating >=10x speedup over CPU baseline while preserving generalization to held-out Hodge pairs. Batch inference on 50,000 unexplored polytopes completes in <50 seconds, enabling rapid pre-screening for three-generation candidates.",
        "expected_negative": "Quantized model latency exceeds 1.5 ms, or h⁰ accuracy drops below 92%, or h¹¹ MAE exceeds 3, indicating that discrete polytope features do not sufficiently constrain the decision boundary to tolerate binarization. Alternatively, model accuracy catastrophically degrades (>20% drop) on high-h¹¹ polytopes (h¹¹ > 80), suggesting shift-based quantization breaks heterogeneous feature scaling across Hodge pair strata.",
        "null_hypothesis": "H0: Shift-based quantization (n-BQ-NN) does not improve inference latency below 1.5 ms per polytope on FPGA, or does not maintain prediction accuracy within 5% of full-precision, or does not generalize uniformly across all Hodge pair strata. Equivalently, the discrete structure of polytope features provides no synergy with binarization and general-purpose quantization achieves equivalent accuracy-latency trade-offs.",
        "statistical_test": "One-sided Mann-Whitney U test on latency distributions (FPGA n-BQ-NN vs CPU baseline): reject H0 if FPGA latency is significantly less than CPU latency (p < 0.05, n=10000 inferences per configuration). Two-sided equivalence t-test on accuracy drop: reject H0 if quantized model accuracy is not statistically significantly lower than full-precision (p > 0.05 for equivalence test with margin 0.05). Stratified contingency chi-squared test for h⁰ prediction: test independence of quantized versus full-precision predictions per Hodge pair stratum; reject H0 if chi-squared is not significant (p > 0.05), indicating strong agreement.",
        "minimum_detectable_effect": "Latency speedup >=10x (FPGA <1 ms versus CPU approximately 10-50 ms, Cohen's d for latency distribution >1.5 assuming 20% coefficient of variation). Accuracy drop <=5 percentage points equivalence margin on h⁰ exact-match (baseline 97% to >=92% quantized). h¹¹ MAE <=2 representing clinically meaningful threshold separating adjacent generation-level Hodge pairs in explored KS subset.",
        "statistical_power_notes": "For latency comparison: n=10000 inferences per configuration (burn-in 100, measure 9900), two-sided t-test, alpha=0.05, desired power 90%. Assuming full-precision CPU latency mu1=30 ms (sigma=6 ms) and FPGA latency mu2=0.8 ms (sigma=0.16 ms), Cohen's d=4.8, so n=20 samples achieves >99% power; use n=10000 for robustness. For accuracy comparison: observed test set size n=1000 polytopes, h⁰ baseline accuracy p0=0.97, target p1=0.92, one-sided equivalence test with non-inferiority margin 0.05, alpha=0.05, power 80%. Approximate sample size n=100 per group suffices; use n=1000 for stratified per-Hodge-pair analysis across 9 strata of approximately 111 each. For ablation study (binarized vs 8-bit vs 16-bit activations): three groups, one-way ANOVA followed by Tukey HSD post-hoc, alpha=0.05, power 80%, estimated effect size f=0.25 (moderate difference across quantization schemes), n=180 total (60 per group).",
        "limitations": [
          "FPGA synthesis is vendor-specific (Xilinx, Altera, etc.) and may not directly transfer to other hardware platforms; latency claims are specific to target FPGA device family and clock frequency.",
          "Polytope feature engineering is hand-crafted and may not generalize to other discrete geometry problems; hypothesis is tightly coupled to specific structure of toric polytope inputs.",
          "Knowledge distillation requires trained full-precision teacher; if teacher has poor generalization on a new Hodge pair, quantized student will inherit that bias.",
          "Ground-truth h⁰ labels are expensive (CyTools calls) and limited to approximately 10000 polytopes; extrapolation to full 195000+ unexplored polytopes assumes distribution similarity, which may not hold for extreme h¹¹ values.",
          "Batch inference speedup depends on memory bandwidth and data transfer latency; single-polytope latency (<1 ms) may not scale linearly to large batches if FPGA-CPU communication becomes bottleneck.",
          "Quantization-aware training (QAT) would likely yield better accuracy-latency trade-offs than post-training quantization but requires retraining on quantized representations; this experiment prioritizes practicality of rapid deployment."
        ],
        "requires_followup": "If computational results meet latency and accuracy thresholds, the next step is computational validation: for a random 1% sample (approximately 50 polytopes) of the 50000 pre-screened candidates predicted to have h⁰(L) >=3, run exact cohomology computation via CyTools to measure false-positive and false-negative rates. If false-positive rate exceeds 10%, retrain with higher decision threshold or augment training data with hard negatives from near-boundary polytopes."
      },
      "keywords": [
        "shift-based quantization",
        "Calabi-Yau threefolds",
        "line bundle cohomology",
        "FPGA inference",
        "knowledge distillation",
        "toric geometry machine learning"
      ],
      "gap_similarity": 0.6909644603729248,
      "gap_distance": 999,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "Algorithm learning",
      "gap_concept_b": "Policy learning",
      "source_question": "Can meta-learning of RL algorithms (learning-to-learn the exploration-exploitation trade-off itself) yield substantially faster convergence for policy learning in line bundle cohomology prediction, compared to fixed RL algorithms trained on heterogeneous CY3 polytope families?",
      "statement": "We hypothesize that meta-learned RL algorithms that adaptively condition exploration-exploitation schedules and value-function architecture on polytope-intrinsic geometric features (lattice point density, face lattice depth, Gorenstein index) causally enable 3–5× faster convergence to h⁰ ≥ 3 detection compared to fixed RL algorithms, by reducing redundant sampling in geometrically-sparse or geometrically-dense regions.",
      "mechanism": "Heterogeneous CY3 polytope families present polytope-conditional geometry that makes fixed exploration-exploitation trade-offs suboptimal: dense polytopes (high lattice point count) benefit from aggressive local exploitation with shallow value networks, while sparse polytopes require deeper lookahead and higher initial exploration. Meta-learning directly optimizes the RL algorithm itself (learning rates, ε-decay, network depth) as a function of polytope features, allowing the policy to allocate computational budget adaptively. This causal mechanism predicts that algorithm meta-learning reduces sample complexity by 3–5×, because the policy learns to exploit geometry-specific patterns (e.g., line bundle richness correlates with Gorenstein index) rather than treating all polytopes uniformly.",
      "prediction": "A meta-learned RL policy trained on 60% of KS polytopes (stratified by χ and h¹¹ range) will require ≤20% of the wall-clock time and ≤35% of the line-bundle samples to achieve ≥90% F1-score on h⁰ ≥ 3 detection on a held-out test set of 2,000 polytopes (remaining 40%), compared to a fixed PPO or DQN policy with hand-tuned hyperparameters trained on the same training set.",
      "falsifiable": true,
      "falsification_criteria": "If the meta-learned RL policy achieves ≥90% F1-score but requires ≥90% of the wall-clock time or ≥85% of the sample count of the fixed baseline (i.e., speedup ratio <1.1×), OR if the meta-learned policy achieves <85% F1-score on the held-out test set, the hypothesis is refuted. Refutation also holds if the learned algorithm fails to generalize: if validation-set F1 is ≥90% but test-set F1 drops below 80%, indicating algorithm overfitting to training polytope families.",
      "minimum_effect_size": "Wall-clock speedup ≥3× (CI: 2.5–5.5×) and sample-efficiency gain ≥3-fold (≤33% of baseline samples), with test-set h⁰ ≥ 3 F1-score ≥0.90 and <5% drop from validation to test. Equivalently: Cohen's d ≥0.8 on log(wallclock_time) comparison across 2,000 test polytopes (assuming σ ≈ 0.35 on log scale).",
      "novelty": 4,
      "rigor": 5,
      "impact": 5,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Implement a meta-RL framework using gradient-based meta-policy optimization (e.g., MAML applied to RL hyperparameter generation, or evolutionary algorithm search) that learns to produce polytope-conditional RL algorithms. Train on 60% of KS polytope family (stratified by χ and h¹¹ range). Evaluate wall-clock time, sample efficiency, and generalization (F1 score for h⁰ ≥ 3 detection) on 40% held-out test set vs. fixed baselines (PPO, DQN with standard hyperparameters).",
        "steps": [
          "Preprocess KS database: extract polytope features (lattice point density normalized by volume, face lattice depth, Gorenstein index, polytope volume, normalized h¹¹) for all ~50,000 scanned polytopes. Compute oracle labels (h⁰ ≥ 3 for ≥1 line bundle in a pre-computed subset) via CyTools for 5,000 polytopes to define the target prediction task.",
          "Stratified train/val/test split: partition polytopes by χ and h¹¹ range (e.g., h¹¹ ∈ [13–20], [20–50], [50–128]) to ensure diversity. Allocate 3,000 (60%) to training meta-learner, 1,000 (20%) to validation, 1,000 (20%) to test. Compute ground-truth labels for val/test via CyTools.",
          "Formalize RL task: state = polytope feature vector (6-dim); action = discrete choice (query line bundle candidate k ∈ {1…K} OR predict h⁰ ≥ 3 and stop); reward = +1 if h⁰ ≥ 3 found before budget B exhausted, −1 if budget exhausted without finding h⁰ ≥ 3, scaled by budget remaining (B = 50 line bundle queries per polytope). Episode terminates when action='stop' or budget exhausted.",
          "Implement meta-learner: use gradient-based meta-learning (MAML: Finn et al.) or evolutionary strategy (CMA-ES) to optimize a hyperparameter generator g_φ(polytope_features) → (learning_rate η, ε-decay schedule α, value-network depth d, exploration bonus scale β). Inner loop: standard policy gradient (PPO or DQN) trained on single polytope with g_φ-generated hyperparameters for 100 gradient steps. Outer loop (meta-update): aggregate gradient signals across training polytopes, update φ via meta-loss (sample efficiency on val polytopes).",
          "Train fixed baselines on same training set: PPO with hand-tuned hyperparameters (η=3e-4, ε-decay=0.99, depth=2, β=0.01) and DQN (η=1e-4, ε=0.1, ε-decay=0.995, replay buffer size=10k). Use identical environment, wall-clock budget per polytope (10 minutes compute).",
          "Evaluation protocol: for each test polytope, run 5 seeds of (meta-learned policy, PPO baseline, DQN baseline). Measure: (a) wall-clock time to ≥90% confidence h⁰ ≥ 3 detected, (b) number of line-bundle samples to same threshold, (c) final F1-score on binary h⁰ ≥ 3 prediction (threshold confidence ≥0.5). Aggregate across 1,000 test polytopes.",
          "Statistical analysis: paired t-tests (Welch, two-sided, α=0.05) on log-transformed wallclock times and sample counts (meta-learned vs. each baseline). Report 95% CI on speedup ratios. Stratify results by polytope family (χ, h¹¹ range) to assess generalization.",
          "Validation generalization test: re-train meta-learner on disjoint 60% of polytopes (different stratification), evaluate on same 1,000 test polytopes. If test-set F1 drops >5% relative to validation F1, flag overfitting.",
          "Ablation: train meta-learner on (a) only lattice point density, (b) only face lattice depth, (c) only Gorenstein index, (d) all three features. Measure F1 and speedup for each ablation to isolate feature importance.",
          "Reusability check: export learned algorithm g_φ and apply to the 195,000+ unscanned polytopes in KS database. Estimate total computational savings (hours vs. days)."
        ],
        "tools": [
          "CyTools (Calabi-Yau computation suite) for polytope preprocessing and line bundle h⁰ oracle queries",
          "PyTorch / TensorFlow for meta-RL implementation (MAML library, stable-baselines3 for PPO/DQN)",
          "Ray Tune / Optuna for hyperparameter search and meta-training orchestration",
          "Public KS database (Kreuzer-Skarke, ~500k reflexive polytopes, ~50k with computed Hodge numbers)",
          "HPC cluster (or cloud compute: AWS/Google Cloud) for parallel RL training across polytope families (est. 1000s of CPU cores or 10–20 GPUs)",
          "Pandas / NumPy for feature engineering and data wrangling",
          "Scikit-learn for baseline ML models (random forest, XGBoost) as weak comparators"
        ],
        "computational": true,
        "estimated_effort": "8–12 weeks of compute: ~2 weeks feature engineering and baseline training, ~4–6 weeks meta-learning loop (10–20 runs of full RL training across train set with different meta-parameters), ~2–3 weeks test evaluation and statistical analysis. Parallelizable on HPC or cloud; critical path ~3–4 weeks wall-clock if 100+ simultaneous jobs available.",
        "data_requirements": "KS database polytope data (vertices, dual, face lattice, Gorenstein index, lattice point count, volume); CyTools-computed h⁰ for ≥5,000 polytopes (or proxy labels from smaller scan); access to HPC or cloud compute (est. 50,000–100,000 CPU-core hours for meta-training + evaluation).",
        "expected_positive": "Meta-learned RL policy achieves ≥90% F1 on h⁰ ≥ 3 detection AND requires ≤35% of wall-clock time and ≤35% of line-bundle samples of fixed baseline (e.g., speedup ≥2.8×), with <5% F1 drop from validation to test, and speedup consistent across h¹¹ subgroups (ratio 2.5–5.5× within family).",
        "expected_negative": "Meta-learned policy achieves ≥90% F1 but speedup <1.1× (i.e., requires ≥90% of baseline time/samples), OR achieves <85% F1 on test set, OR test-set F1 drops ≥5% from validation (overfitting to training families), OR learned hyperparameters do not generalize: same g_φ trained on different 60% of polytopes yields <2× speedup on original test set.",
        "null_hypothesis": "H₀: Wall-clock convergence time and sample efficiency are independent of polytope-intrinsic geometric features; meta-learned RL algorithms achieve equivalent or slower convergence than fixed algorithms, and learned hyperparameters do not generalize across polytope families (speedup ratio μ_meta / μ_fixed ≈ 1.0).",
        "statistical_test": "Two-sided paired Welch t-test on log(wallclock_time) and log(sample_count) for meta-learned vs. fixed baseline across 1,000 test polytopes, α = 0.05. Report 95% CI on speedup ratio (exponentiated mean log difference). Bonferroni correction for dual tests: α_adj = 0.025. Stratified analysis: separate t-tests for each h¹¹ range (5 groups) with Benjamini-Hochberg FDR control, q < 0.05. Cohen's d computed from paired differences; d ≥ 0.8 required for 'large' effect (primary success criterion).",
        "minimum_detectable_effect": "Cohen's d ≥ 0.8 on log(wallclock_time), equivalent to ~55% reduction in median time, achievable with n=1,000 test polytopes at power 0.95 (assuming σ_paired ≈ 0.35 on log scale, correlation r ≈ 0.5 between meta-learned and baseline times). Speedup ratio ≥2.5× (lower bound of 95% CI) is scientifically meaningful for a 100× potential savings goal (195k polytopes).",
        "statistical_power_notes": "Sample size n=1,000 test polytopes (1,000 polytopes × 5 seeds = 5,000 independent RL runs) yields power ≥0.95 for detecting Cohen's d = 0.8 at α=0.05 (two-sided t-test). Seed averaging (5 replicates per polytope) reduces noise and accounts for stochasticity in RL training. For meta-learning convergence, use 3,000 training polytopes with meta-batch size 32 (3,000 / 32 ≈ 94 meta-iterations); stop meta-training when validation F1 plateaus for 10 consecutive meta-epochs (overfitting criterion).",
        "limitations": [
          "RL training is stochastic; speedup estimates may be noisy unless averaged over many seeds (5+ per polytope) and large test set (1,000+). Mitigation: use stratified sampling and non-parametric tests.",
          "Meta-learning assumes train and test polytope families are sufficiently similar (same χ, overlapping h¹¹ ranges). If test polytopes have radically different geometry (e.g., χ >> −6), generalization may fail. Mitigation: report family-stratified results and test on χ ≠ −6 polytopes as OOD generalization check.",
          "h⁰ ≥ 3 ground truth is expensive to compute (CyTools enumeration ~1–10 min per polytope); only ~5,000 labels available. If label set is biased (e.g., skewed toward h¹¹ ≥ 50), F1 estimates may not reflect true population. Mitigation: prioritize label collection across h¹¹ ranges; use semi-supervised learning (confidence thresholding on unlabeled polytopes) to augment labels.",
          "Wall-clock time comparisons depend on hardware (CPU model, parallelization); results may not transfer to different clusters or cloud providers. Mitigation: normalize by wall-clock time on a standard benchmark task (e.g., PPO on CartPole) run on the same hardware.",
          "Meta-learner may overfit if meta-training polytope set is too small (<3,000) or too homogeneous. Mitigation: use cross-family validation (train on χ = −6, test on χ = −8) and data augmentation (polytope feature perturbation).",
          "Feature engineering (lattice point density, face lattice depth, Gorenstein index) is domain-specific; generalization to other geometric ML problems (e.g., K3 surfaces, elliptic fibrations) is unclear. Mitigation: document feature choice and ablate to identify transferable components."
        ],
        "requires_followup": "Computational proxy is primary. If meta-learned algorithm yields speedup ≥2.5×, validate by: (1) running learned algorithm on 1,000 new unscanned KS polytopes, computing h⁰ for discovered bundles via CyTools to confirm F1 ≥0.90; (2) deploying learned algorithm as live pre-screener in the community CyTools pipeline (F-theory/heterotic community) for 6-month production run, measuring actual speedup on full KS database scan. This wet-lab equivalent is recommended if speedup threshold crossed, as it validates real-world utility; otherwise, additional computational experiments (e.g., transfer learning, few-shot meta-learning) are sufficient."
      },
      "keywords": [
        "meta-learning",
        "reinforcement learning algorithms",
        "Calabi-Yau polytopes",
        "Hodge numbers",
        "algorithm adaptation",
        "line bundle cohomology"
      ],
      "gap_similarity": 0.6495609879493713,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.65,
      "final_score": 4.65
    },
    {
      "gap_concept_a": "Omniglot dataset",
      "gap_concept_b": "Omniglot database",
      "source_question": "Can few-shot learning benchmarks like Omniglot be systematically adapted to predict rare topological invariants (h⁰, Hodge numbers) in the Calabi-Yau landscape, where labeled examples are sparse and expensive to compute?",
      "statement": "We hypothesize that few-shot meta-learning models (prototypical networks trained on 3–5 labeled CY3 examples per task) will predict h⁰(L) ≥ 3 line bundle existence with ≥75% accuracy on held-out polytopes, outperforming single-task supervised baselines by ≥15 percentage points when trained on ≤500 total labeled examples.",
      "mechanism": "Few-shot meta-learning transfers task-level inductive biases learned across diverse polytope families (stratified by χ and Gorenstein index) to novel query polytopes. By optimizing for rapid adaptation from minimal support sets, the meta-learner captures task-invariant polytope-geometry features (vertex norm, face lattice structure, normalized lattice point density) that correlate with line bundle richness, bypassing the need for large labeled datasets. Prototypical networks specifically learn an embedding space where polytopes are clustered by topological phenotype, allowing query examples to be classified via nearest-neighbor logic in that space.",
      "prediction": "A prototypical network trained on 5 meta-training tasks (each with 4 support + 16 query CY3 examples, stratified by h¹¹ bin) will achieve ≥75% accuracy predicting h⁰(L) ≥ 3 on 100 held-out test polytopes. This accuracy will exceed a fully-supervised XGBoost baseline trained on the same 500 labeled examples by ≥15 percentage points (i.e., XGBoost achieves ≤60% accuracy under the same data constraint).",
      "falsifiable": true,
      "falsification_criteria": "If prototypical network accuracy on the held-out test set is <70%, or if the accuracy gain over the supervised baseline is <10 percentage points, the hypothesis is refuted. Additionally, if the meta-learner's uncertainty (distance from query to nearest prototype) does NOT correlate with prediction error (Spearman ρ < 0.4), the causal claim about learned task-invariant features fails.",
      "minimum_effect_size": "Accuracy ≥75% on test set (binary h⁰(L) ≥ 3 classification); accuracy gap ≥15 percentage points vs. supervised XGBoost baseline; Spearman ρ ≥ 0.4 between model uncertainty and error for active-learning validation.",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Construct a meta-learning task distribution from the Kreuzer-Skarke database by stratifying CY3s by χ and Gorenstein index, then train prototypical networks on support sets of 3–5 labeled examples with polytope-geometry descriptors as features. Compare zero-shot and few-shot accuracy against single-task supervised baselines (XGBoost, MLP) on held-out query polytopes, and validate meta-learner uncertainty against prediction error for active-learning targeting.",
        "steps": [
          "Curate a task distribution: partition the KS database by χ and Gorenstein index; select 5–10 tasks (task = a Hodge-pair cohort), each with ≥50 hand-verified CY3s from CyTools with h⁰(L) labels from exact computation.",
          "Extract polytope descriptors: for each CY3, compute vertex matrix Frobenius norm, face lattice depth (max face dimension), normalized lattice point density (# interior lattice points / polytope volume), dual polytope codimension, and Mori cone rank. Normalize all features to zero mean, unit variance within each task.",
          "Partition data: for each task, randomly select 4 support examples (labeled), 16 query examples (labeled for meta-training), and 10 query examples (unlabeled for held-out test). Repeat to create 5 meta-training tasks and reserve 1 task entirely for test evaluation.",
          "Train prototypical network: implement a 3-layer fully-connected embedding network (input: polytope descriptor vector, hidden layers: 64, 32 units; output: 16-dim embedding). Train via episodic sampling (500 episodes) on meta-training tasks using Euclidean distance in embedding space; optimize with Adam (lr=0.001) and cross-entropy loss on support/query splits.",
          "Train supervised baselines: fit XGBoost and 3-layer MLP on all 500 labeled meta-training examples (pooled across tasks) using the same polytope features; tune hyperparameters (XGBoost max_depth, learning_rate; MLP hidden layers) via 5-fold cross-validation on training data.",
          "Evaluate zero-shot and few-shot accuracy: compute test-set accuracy (h⁰(L) ≥ 3 vs. < 3 binary classification) for: (a) prototypical network, (b) XGBoost baseline, (c) MLP baseline. Report confusion matrices, precision, recall, and F1 for each.",
          "Validate uncertainty–error correlation: for the prototypical network, compute the distance from each test query to its nearest prototype in embedding space (model uncertainty). Bin test examples by uncertainty quartile and compute accuracy per bin. Compute Spearman rank correlation between uncertainty and prediction error (binary: correct/incorrect) across all test examples.",
          "Ablation study: retrain prototypical network using only polytope descriptors (polytope-only), then with polytope + GLSM charge matrix features (polytope + GLSM), and compare accuracy. This isolates whether geometry alone is sufficient or if charge information improves meta-learning.",
          "Transfer learning validation: fine-tune the prototypical network on 3 support examples from a new task (not in meta-training), then evaluate on 10 query examples from that task. Repeat for 3 held-out tasks and report average accuracy to confirm rapid adaptation.",
          "Active learning simulation: use prototypical network uncertainty to rank the remaining ~300 unlabeled test examples by prediction confidence. Simulate 5 rounds: in each round, select the 20 lowest-confidence examples, add them to the labeled set (treat CyTools computation as ground truth), retrain the meta-learner, and report accuracy on remaining unlabeled examples. Compare to random selection and passive supervised learning curves."
        ],
        "tools": [
          "CyTools (CY3 database and exact h⁰ computation for ground truth)",
          "Kreuzer-Skarke database (polytope library and Hodge number labels)",
          "PyTorch (prototypical network implementation)",
          "scikit-learn (XGBoost, MLP baseline, feature normalization)",
          "pandas / numpy (data manipulation and stratification)",
          "scipy.stats (Spearman correlation for uncertainty validation)",
          "matplotlib / seaborn (accuracy plots, confusion matrices, ablation results)"
        ],
        "computational": true,
        "estimated_effort": "3–4 weeks compute time: ~2 days feature extraction (polytope descriptors × 500 examples), ~1 week prototypical network hyperparameter search and episodic training (500 episodes × 5 tasks), ~2 days baseline training and evaluation, ~3 days active learning simulation and uncertainty analysis. Total wall-clock ~40 GPU hours if using single NVIDIA V100; easily parallelizable.",
        "data_requirements": "CyTools database access (≥500 hand-verified CY3s with h⁰(L) labels); Kreuzer-Skarke database polytope data (reflexive polytope vertices, face lattice). Approximately 500 GB of stored polytope data; computational bottleneck is exact h⁰ computation (already completed by CyTools team). All code and minimal feature tables can be released publicly.",
        "expected_positive": "Prototypical network achieves ≥75% accuracy on held-out test set, outperforming XGBoost and MLP baselines by ≥15 percentage points. Model uncertainty (distance to nearest prototype) correlates with prediction error (Spearman ρ ≥ 0.4). Active learning selects examples that, when labeled and added to training set, improve held-out accuracy faster than random selection. Ablation shows polytope geometry alone (without GLSM charge) is sufficient for ≥70% accuracy.",
        "expected_negative": "Prototypical network accuracy <70% on test set, or baseline-beating margin <10 percentage points. Uncertainty does not correlate with error (Spearman ρ < 0.3). Active learning selection is indistinguishable from random sampling (curves overlap within error bars). Ablation reveals that GLSM charge is necessary for good performance, suggesting geometric descriptors alone are insufficient and the hypothesis about task-invariant polytope features is incomplete.",
        "null_hypothesis": "H₀: Few-shot meta-learning provides no sample-efficiency advantage over single-task supervised learning for CY3 line bundle prediction. Formally, the accuracy of prototypical networks trained on ≤500 labeled examples equals (within 10 percentage points) the accuracy of XGBoost / MLP trained on the same budget.",
        "statistical_test": "Two-proportion z-test comparing test-set accuracy of prototypical network vs. XGBoost baseline; one-tailed test with α=0.05, H₁: prototypical accuracy − XGBoost accuracy > 0.10. Spearman rank correlation test: H₀: ρ(uncertainty, error) = 0 vs. H₁: ρ > 0.4, α=0.05 two-tailed. For active learning, paired t-test comparing improvement rate (accuracy gain per labeled example) for active selection vs. random selection across 5 rounds, α=0.05.",
        "minimum_detectable_effect": "Accuracy difference ≥15 percentage points (prototypical vs. XGBoost) with n=100 test examples, two-proportion z-test power=0.80 requires effect size Δp≥0.15. For Spearman correlation, ρ≥0.4 requires n≥30 test examples (achieved; power≥0.90). For active learning, Cohen's d≥0.6 (medium effect) for improvement-rate difference, requiring n≥30 examples per round (achieved).",
        "statistical_power_notes": "Test set: 100 held-out polytopes; expected prototypical network accuracy 75%, XGBoost 60%. Two-proportion z-test: Δp=0.15, α=0.05 one-tailed, power=0.90 requires n≥80 per group (achieved with n=100 total test examples split evenly). Spearman correlation: n=100 test examples provides power>0.99 to detect ρ≥0.4 at α=0.05. Active learning: 5 rounds × 20 examples per round = 100 examples sampled; comparison of improvement slopes across 5 rounds via repeated-measures ANOVA, power=0.85 for medium effect (f=0.3).",
        "limitations": [
          "Ground truth (h⁰(L) labels) restricted to ~500 verified examples from CyTools; extrapolation to the full KS database (195,000+ unexplored polytopes) is indirect and requires that task-distribution assumptions (stratification by χ, Gorenstein index) are representative.",
          "Polytope descriptors are hand-engineered; no guarantee that vertex norm, face lattice depth, etc. capture all topologically relevant information. Hypothesis relies on these features being sufficient; if h⁰ depends on higher-order combinatorial properties, meta-learning may not help.",
          "Meta-learning assumes task structure (Hodge-pair cohorts) is meaningful; if Hodge number is not a stable task label (e.g., if h¹¹ fluctuates unpredictably within a Gorenstein-index class), episodic training will fail.",
          "Prototypical networks assume Euclidean geometry in embedding space; if true polytope similarity is non-Euclidean or multi-modal, nearest-neighbor classification will be suboptimal. Comparison to learned metrics (e.g., Siamese networks) not performed.",
          "Active learning validation is simulation-based; actual CyTools computation cost and wall-clock time per example not modeled. Rank order of selected examples may be correct, but computational speedup is not directly measured.",
          "Transferability across polytope families outside meta-training distribution not tested; results may not generalize to χ ≠ −6 or Gorenstein index ranges not in training.",
          "Batch effects: if hand-verified CY3 labels in CyTools have systematic errors or incomplete h⁰ scans, ground truth labels are noisy, which will artificially inflate baseline error rates and overestimate meta-learning advantage."
        ],
        "requires_followup": "To fully validate the hypothesis, an independent CyTools run on a held-out set of ~200 new polytopes (not used for training or test evaluation) would be needed to confirm that active-learning selections indeed reduce computational burden in wall-clock hours. This requires ~4–6 weeks of CyTools computation and is beyond the scope of the primary computational experiment. However, if the correlation between prototypical network uncertainty and error (Spearman ρ ≥ 0.4) is strong, this computational validation is justified."
      },
      "keywords": [
        "few-shot meta-learning",
        "Calabi-Yau line bundles",
        "prototypical networks",
        "Kreuzer-Skarke database",
        "sample-efficient learning"
      ],
      "gap_similarity": 0.748268723487854,
      "gap_distance": 6,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "unlabeled data",
      "gap_concept_b": "labeled data",
      "source_question": "Can self-supervised or semi-supervised learning on the unlabeled polytope space (195,000+ unscanned KS entries) improve prediction of h⁰(L) and Hodge numbers without expensive ground-truth computation, and does the structure of unlabeled polytope embeddings encode sufficient signal to guide label acquisition?",
      "statement": "We hypothesize that self-supervised pretraining on unlabeled polytope divisor graphs using contrastive learning (GraphCL) encodes sufficient topological and combinatorial signal to initialize supervised h⁰(L) and h¹¹ predictors that achieve ≥90% of fully-supervised performance using only 20% of labeled training data, and that learned embeddings exhibit structure enabling active learning to reduce labeling burden by ≥3-fold.",
      "mechanism": "Self-supervised contrastive learning on polytope graphs (via augmented divisor-graph pairs and face-lattice similarity) discovers low-dimensional manifold structure intrinsic to reflexive polytope space, independent of expensive cohomology labels. This learned representation captures combinatorial constraints (Mori cone, intersection number patterns, Gorenstein geometry) that correlate with h⁰ and h¹¹ without explicit supervision. Transfer-initialized supervised models then require fewer labeled examples to converge because the embedding already encodes polytope relatedness; active learning further exploits embedding uncertainty to prioritize high-information samples, reducing redundant labeling.",
      "prediction": "Supervised models initialized with embeddings from self-supervised pretraining will achieve mean absolute percentage error (MAPE) ≤12% on h⁰(L) prediction and ≤10% on h¹¹ prediction using only 10,000 labeled polytopes (~20% of 50k labeled set), compared to baseline supervised-only MAPE of ≥18% and ≥15% respectively at the same label budget. Active learning on top of pretrained embeddings will reduce total labeled samples needed to reach 95% of fully-supervised performance from 50,000 to ≤15,000 (≥3.3× speedup).",
      "falsifiable": true,
      "falsification_criteria": "If transfer-initialized models (with 10k labels) achieve MAPE >15% on h⁰(L) or >12% on h¹¹, OR if active learning reduces the label requirement by <2× compared to random sampling, the hypothesis is refuted. Additionally, if learned embeddings show no significant clustering structure (silhouette score <0.3) correlating with Hodge-number or bundle-richness quartiles, the mechanism (manifold structure encoding polytope relatedness) is falsified.",
      "minimum_effect_size": "MAPE ≤12% on h⁰(L) and ≤10% on h¹¹ at 20% labeled-data budget; R² ≥0.85 for h¹¹ prediction on test set; silhouette score ≥0.4 for embedding clusters separated by Hodge number quartile; active-learning acceleration ratio ≥3.0 (labeled-sample reduction factor).",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Construct a computational pipeline: (1) build unlabeled polytope feature and graph representations from all ~195k KS entries; (2) pretrain a GraphCL-based GNN on the unlabeled set using contrastive divisor-graph augmentations; (3) initialize supervised regressors (MLP, gradient boosting) with pretrained embeddings and train on subsampled labeled sets (10%, 20%, 50% of 50k); (4) compare test accuracy to supervised-only baselines and measure label efficiency; (5) implement uncertainty-based active learning on pretrained embeddings and measure cumulative labeling cost to reach target performance.",
        "steps": [
          "Extract and standardize polytope features from KS database for all ~195,000 entries: normalized lattice-point density, Gorenstein index, face-lattice depth (via CyTools), Mori-cone dimension, second Chern class c₂, and vertex-adjacency graph structure.",
          "Construct toric divisor graphs for each polytope: nodes = toric divisors (facets of polytope), edges = intersection structure (from Mori cone relations). Store as edge lists and degree sequences.",
          "Implement GraphCL self-supervised loss: for each polytope, generate two augmentations of its divisor graph (random edge-dropping, node-feature perturbation); train GNN encoder to maximize agreement between augmented pair embeddings using NT-Xent loss (temperature τ=0.1). Train for 200 epochs on full unlabeled set using Adam optimizer, learning rate 0.001, batch size 256.",
          "Extract 64-dimensional pretrained embeddings from GNN encoder for all 195k polytopes.",
          "Subsample labeled set (50,497 polytopes with computed h⁰, h¹¹) into 5 random splits: 10%, 20%, 30%, 50%, 100% of full labeled set.",
          "For each split, train two supervised models: (i) supervised-only baseline (GNN encoder trained from scratch on labeled data only), (ii) transfer-initialized model (use pretrained embeddings as fixed features or as GNN initialization). Train with mean absolute percentage error (MAPE) loss. Use 80–20 train-test split, cross-validation on train set.",
          "Measure test-set MAPE for h⁰(L) and h¹¹ predictions. Plot learning curves: label budget vs. MAPE. Record 95%-performance label threshold for both approaches.",
          "Evaluate embedding quality: compute silhouette score, Davies-Bouldin index, and calinski_harabasz score on pretrained embeddings, stratified by h¹¹ quartile and h⁰-richness class. Test cluster separation using UMAP visualization.",
          "Implement uncertainty sampling: for each unlabeled polytope, train an ensemble of 5 supervised models on current labeled set and compute prediction variance of h⁰(L) and h¹¹. Select top-k highest-variance samples for labeling (simulated by querying ground truth). Iteratively retrain and measure cumulative labeled samples vs. test accuracy. Compare to random sampling baseline.",
          "Report acceleration ratio (unlabeled-samples-queried / fully-supervised-at-same-performance) and 95%-performance label threshold for active learning vs. random."
        ],
        "tools": [
          "CyTools (polytope geometry, cohomology oracle for ground truth)",
          "PyTorch, PyTorch Geometric (GNN implementation, GraphCL loss)",
          "DGL (Deep Graph Library, divisor-graph construction)",
          "scikit-learn (supervised baselines, clustering metrics, active learning utilities)",
          "UMAP (embedding visualization)",
          "Kreuzer-Skarke database (~195k reflexive polytopes, ~50k with computed h⁰, h¹¹)",
          "High-performance compute cluster (GPU for GNN pretraining: ~24–48 GPU hours estimated)"
        ],
        "computational": true,
        "estimated_effort": "4–6 weeks: (1) feature extraction & graph construction ~1 week; (2) GraphCL pretraining ~1 week (2–4 A100 GPU-days); (3) supervised training & learning curves ~1 week; (4) embedding analysis & active learning ~1–2 weeks; (5) final aggregation & reporting ~1 week.",
        "data_requirements": "Kreuzer-Skarke database (195,496 reflexive polytopes, polytope vertex/edge/face data); CyTools-computed h⁰(L), h¹¹, h²¹ for ~50k labeled polytopes; dual polytope and Mori-cone generators for feature extraction. All publicly available.",
        "expected_positive": "Transfer-initialized models reach MAPE ≤12% (h⁰) and ≤10% (h¹¹) at 10k labels (20% budget); pretrained embeddings show silhouette score ≥0.4 when stratified by Hodge quartile; active learning achieves 95%-performance threshold at ≤15k labels (~3.3× speedup vs. random sampling).",
        "expected_negative": "Transfer-initialized models fail to outperform supervised-only baseline (MAPE >15% at 20% label budget); learned embeddings show silhouette score <0.3 (no topological structure captured); active learning acceleration <2.0× (learning is not guided by embeddings).",
        "null_hypothesis": "H₀: Self-supervised pretraining on unlabeled polytopes provides no improvement over supervised training from scratch. Equivalently, (i) transfer-initialized MAPE ≥ supervised-only MAPE at all label budgets ≤50%, (ii) learned embeddings encode no polytope-relatedness signal (silhouette <0.2), and (iii) active learning achieves no better label efficiency than random sampling (acceleration ratio ≈1.0).",
        "statistical_test": "Two-sided paired t-test comparing MAPE curves (transfer vs. supervised-only) across 5 label-budget regimes; α=0.05, n=10 random train-test splits per regime. Effect size: Cohen's d = (μ_supervised − μ_transfer) / σ_pooled > 0.5 required. For active learning, Mann-Whitney U test on cumulative labeled samples to reach 95% performance (α=0.05); one-tailed hypothesis that active learning reduces label count.",
        "minimum_detectable_effect": "ΔMAPE ≥2% absolute (e.g., 18% → 16%) between supervised-only and transfer-initialized at 20% label budget (Cohen's d ≈0.4–0.5 depending on variance). For active learning, median reduction in labeled samples ≥3-fold (from 50k to ≤15k to reach 95% of fully-supervised performance). For embedding quality, silhouette score difference ≥0.3 between random divisor-graph shuffle and pretrained embeddings.",
        "statistical_power_notes": "Primary comparison: paired MAPE measurements across n=10 independent train-test splits (label budget 20%) at α=0.05, two-tailed. Assumed effect size δ=2% MAPE (small-to-medium effect), σ≈3% (based on prior ML studies on KS database). Power analysis: to achieve 80% power with these parameters requires n≥10 splits, which are feasible. For active learning, n=5 random seed trials with 50 iteration steps each (active vs. random sampling). Convergence criterion for GNN pretraining: validation contrastive loss plateau (<0.1% change over 20 epochs).",
        "limitations": [
          "Ground truth (h⁰, h¹¹) labels are computed via CyTools, which itself is a black-box oracle with unknown approximation error; any systematic bias in CyTools will propagate to evaluation.",
          "Unlabeled set (195k polytopes) is not truly unlabeled but rather partially labeled in the literature; selection bias toward 'interesting' polytopes may skew learned manifold structure.",
          "Divisor-graph augmentations (edge dropping, feature perturbation) are heuristic; no principled theory guarantees they capture polytope-invariant structure.",
          "Active learning simulation uses ground truth to measure uncertainty, which assumes queries are always answered correctly; real active learning would require additional CyTools computation.",
          "Transfer initialization assumes supervised task (h⁰, h¹¹ prediction) is sufficiently similar to contrastive task; domain gap not quantified.",
          "Hyperparameter tuning (GNN architecture, contrastive temperature, active-learning budget) performed on held-out validation set; risk of overfitting to validation set metrics.",
          "Computational cost of full pretraining on 195k polytopes is high; reproducibility requires significant GPU time."
        ],
        "requires_followup": "No wet-lab follow-up required; this is a computational study. However, confirmation would require: (1) validation on a held-out polytope subset not used during pretraining or hyperparameter tuning; (2) deployment on newly-computed CY3s to prospectively test whether active-learning prioritization reduces real cohomology-computation time; (3) cross-validation across multiple cohomology oracles (CyTools, alternative implementations) to rule out oracle-specific biases."
      },
      "keywords": [
        "self-supervised learning polytopes",
        "contrastive graph neural networks",
        "semi-supervised Calabi-Yau prediction",
        "active learning label efficiency",
        "transfer learning line bundle cohomology"
      ],
      "gap_similarity": 0.6587511301040649,
      "gap_distance": 4,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "RL algorithm",
      "gap_concept_b": "DRL algorithms",
      "source_question": "Can deep reinforcement learning (DRL) algorithms implemented in modular libraries like ChainerRL be systematically applied to sequential line bundle construction on CY3s, and does the choice of DRL algorithm (policy gradient vs. Q-learning vs. actor-critic) significantly affect the discovery rate of bundles with h⁰ ≥ 3 compared to greedy or random baseline policies?",
      "statement": "We hypothesize that actor-critic deep reinforcement learning algorithms (A3C, PPO) with entropy regularization and target networks will discover line bundles with h⁰ ≥ 3 on Calabi-Yau threefolds at least 3× more efficiently (bundles found per cohomology evaluation) than greedy baseline policies, because the entropy bonus reduces premature convergence to locally-greedy charge vectors while experience replay and advantage normalization stabilize learning in the sparse-reward regime of high-dimensional Mori cone spaces.",
      "mechanism": "Actor-critic methods maintain separate policy (actor) and value (critic) networks. The critic estimates state value, allowing the actor to learn advantage functions A(s,a) = Q(s,a) - V(s), which reduces gradient variance in sparse-reward settings. Entropy regularization penalizes deterministic policies, forcing exploration of charge-vector regions that greedy charge-magnitude maximization ignores. Target networks delay value updates, stabilizing training when the polytope-descriptor state space is high-dimensional and reward (h⁰ ≥ 3 found) is rare. Together, these mechanisms cause actor-critic methods to discover high-h⁰ bundles faster than Q-learning variants (DQN) that struggle with function approximation instability in discrete action spaces, and vastly faster than greedy or random baselines that lack learned exploration.",
      "prediction": "PPO and A3C will each achieve ≥50% discovery rate (find h⁰ ≥ 3 bundle) within 300 cohomology evaluations on a held-out test set of 100 CY3 polytopes with diverse h¹¹ ∈ [18, 60], while DQN and Double DQN will achieve <35% within the same budget, and greedy and random baselines will achieve <15% — representing a 3-fold to 5-fold efficiency gain for actor-critic over DQN and >10-fold over baselines.",
      "falsifiable": true,
      "falsification_criteria": "If PPO or A3C discovery rate is ≤35% within 300 cohomology evaluations on the test set, OR if both DQN variants achieve >45% discovery rate (showing Q-learning is not fundamentally disadvantaged), the hypothesis is refuted. The hypothesis requires actor-critic superiority to be both statistically significant (χ² test, p<0.05 on 2×2 contingency table: algorithm × success/failure) and practically relevant (≥3-fold efficiency gap).",
      "minimum_effect_size": "≥3-fold reduction in median cohomology evaluations to first h⁰ ≥ 3 discovery (actor-critic median ≤100 evals vs. DQN median ≥300 evals), OR ≥50% absolute improvement in discovery rate (PPO/A3C ≥50% vs. greedy ≤15% at fixed eval budget), with Cliff's d (non-parametric effect size for ordinal metrics) ≥0.5.",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Implement canonical DRL algorithms (PPO, A3C, DQN, Double DQN, Dueling DQN) from ChainerRL or StableBaselines3 in a shared Markov decision process (MDP) environment that models line bundle construction as sequential integer-vector selection from Mori cone generators. Train each algorithm on a labeled dataset of 500–800 (polytope, bundle, h⁰) tuples from CyTools, then evaluate discovery rate and sample efficiency on a held-out test set of 100 diverse CY3 polytopes.",
        "steps": [
          "Step 1: Formalize MDP: state = (polytope vertex matrix hash, face lattice depth, Gorenstein index, current charge vector, step count); action space = integer vectors in Mori cone spanning basis (dimension 2–8 depending on polytope); reward = +10 if h⁰ ≥ 3 achieved, -0.01 per step (sparse reward with small cost-per-step to encourage efficiency); episode termination = h⁰ ≥ 3 found OR max 200 steps reached.",
          "Step 2: Preprocess polytope features: normalize reflexive polytope vertex matrix to unit hypercube, compute Gorenstein index, face lattice depth, and volume; embed in 64-dim vector via MLP. State representation = [polytope_embedding; charge_vector_onehot; step_count] (~150 dims total).",
          "Step 3: Implement five DRL agents in Python (StableBaselines3 or ChainerRL): (a) PPO with entropy coefficient β=0.01, (b) A3C with 8 parallel workers, (c) DQN with experience replay buffer size 50k, (d) Double DQN (target network update every 1000 steps), (e) Dueling DQN. Use identical 2-layer ReLU network (256 units per layer) for all policy/value functions. Learning rate = 1e-3, batch size = 64.",
          "Step 4: Train each agent independently on 500 labeled (polytope, line bundle, h⁰ value) samples from CyTools database. For each CY3 polytope, allow agent 200 steps to construct a charge vector. At step t, query h⁰ via cached cohomology table (no live computation during training). Log per-episode reward, cumulative discovery rate, and wall-clock time.",
          "Step 5: Evaluate on held-out test set: 100 CY3 polytopes (disjoint from training; stratified by h¹¹ range [18–60]) × 5 runs per algorithm (different random seeds) = 500 test episodes per algorithm. For each test episode, measure: (A) number of cohomology queries to first h⁰ ≥ 3 discovery, (B) final discovery success (0/1) within 300 queries, (C) wall-clock time, (D) final charge vector Euclidean norm and alignment with Mori generators.",
          "Step 6: Implement baselines: (i) Greedy (at each step, select Mori generator that maximizes charge magnitude while remaining in Kähler cone), (ii) Random (sample uniformly from Mori cone integer lattice points), (iii) Evolutionary strategy (random population of 20 charge vectors, mutation-based selection). Run baselines on same test set.",
          "Step 7: Ablation studies on PPO and A3C: train variants with (a) entropy coefficient β = 0, 0.001, 0.01, 0.1, (b) target network disabled (on-policy only), (c) experience replay disabled (for A3C: worker independence), (d) advantage normalization disabled. Measure discovery rate on 50-polytope subset.",
          "Step 8: Statistical testing: Chi-squared test on 2×2 contingency tables (algorithm × success) to compare discovery rates; Mann-Whitney U test on cohomology query counts (ordinal, non-normal); Kruskal-Wallis H-test across all five DRL algorithms. Report 95% confidence intervals on median queries and discovery rate via bootstrap.",
          "Step 9: Sensitivity analysis: retrain best algorithm (expected: PPO or A3C) on training set sizes 200, 500, 800, 1200; evaluate generalization to unseen Hodge pairs (h¹¹ ranges [61–100]) not in training or test splits."
        ],
        "tools": [
          "StableBaselines3 (PPO, DQN, A3C implementations)",
          "ChainerRL (alternative DRL library; Dueling DQN, A3C)",
          "CyTools (Calabi-Yau threefold toolkit; cohomology computation)",
          "Kreuzer-Skarke database (reflexive polytopes)",
          "Python ≥3.8, PyTorch ≥1.9, NumPy, SciPy, Pandas",
          "Ray Tune (distributed hyperparameter tuning, optional)",
          "Matplotlib, Seaborn (visualization of learning curves and discovery rates)"
        ],
        "computational": true,
        "estimated_effort": "3–4 weeks compute + development. ~500 training episodes × 5 algorithms × 200 steps per episode = 500k environment steps (~2–3 h per algorithm on GPU). Evaluation: 500 test episodes × 5 algorithms (~4 h total). Ablation: 10 variants × 50 polytope subset (~2 h). Debugging/iteration: 1 week.",
        "data_requirements": "500–800 labeled (polytope ID, line bundle charge vector, h⁰ value) tuples from CyTools database with computed cohomology. Polytope vertex matrices and face lattices from Kreuzer-Skarke (∈ 195k polytope candidate pool). Mori cone generators pre-computed for each polytope (available in CyTools). Estimated storage: <10 GB (polytope data + cohomology cache).",
        "expected_positive": "PPO or A3C achieve ≥50% discovery rate within 300 cohomology evaluations; DQN/Double DQN achieve 25–40%; greedy and random <15%. Median cohomology query count: PPO/A3C ~80–120, DQN ~200–250, baselines ~500+. Entropy ablation shows that β ∈ {0.001, 0.01} significantly outperforms β=0 (p<0.05). Wall-clock time: PPO/A3C <5 min per test episode; DQN <8 min; baselines <1 min (greedy/random are fast but less accurate).",
        "expected_negative": "PPO and A3C discovery rate ≤35% within 300 evals (hypothesis refuted). Alternatively: DQN achieves ≥45% discovery, showing Q-learning is not fundamentally disadvantaged in this domain (refutes mechanism). Or: ablation shows entropy regularization and target networks are not load-bearing (β=0 and target-network-disabled variants perform ≥90% as well as full algorithm), suggesting the gain is due to other factors (confounds mechanism).",
        "null_hypothesis": "H₀: All five DRL algorithms (PPO, A3C, DQN, Double DQN, Dueling DQN) achieve equal discovery rates and sample efficiency. There is no statistically significant difference in median cohomology evaluations to first h⁰ ≥ 3 discovery across algorithms (α = 0.05, Kruskal-Wallis H test).",
        "statistical_test": "Primary: Kruskal-Wallis H-test across five DRL algorithms on cohomology query counts (ordinal, non-parametric; α=0.05). Secondary: χ² test on 5×2 contingency table (algorithm × discovery success/failure; α=0.05). Post-hoc pairwise comparisons: Dunn's test with Bonferroni correction. Effect size: Cliff's d (non-parametric ordinal effect size; threshold d≥0.5 for 'large'). Comparison to baselines: Mann-Whitney U test (DRL best vs. greedy, DRL best vs. random; α=0.05 each).",
        "minimum_detectable_effect": "Median cohomology query count difference ≥100 between actor-critic (expected ~100) and DQN (expected ~250), with 80% power, n=5 runs per algorithm per test polytope = 25 replicates per algorithm. Non-parametric test suitable for ordinal data. For discovery rate (binary): 15% absolute difference (50% vs. 35%) is detectable at 80% power with n=100 test episodes per algorithm (proportion test, two-tailed, α=0.05).",
        "statistical_power_notes": "Test set: 100 holdout CY3 polytopes × 5 random seeds per algorithm × 5 algorithms = 2500 total test runs. Ordinal outcome (cohomology queries to first discovery) analyzed via non-parametric Kruskal-Wallis (n=100 per algorithm, robust to skewness). Binary outcome (discovery within 300 evals) analyzed via χ² test with expected cell counts ≥5. Expected effect size (actor-critic median 100 vs. DQN median 250 evals) corresponds to Cliff's d ≈0.6–0.8, giving >90% power. For ablation studies (7 variants of entropy β + target network + replay): n=50 polytope subset per variant, 3 seeds = 150 runs per variant, sufficient for Mann-Whitney U pairwise tests (α=0.05, power=0.80).",
        "limitations": [
          "Cohomology computation cost: during training, h⁰ values are cached from CyTools pre-computed table, not live. Real deployment requires integration with cohomology solvers (Macaulay2, Singular), introducing latency. Test evaluation uses cached h⁰; generalization to newly-computed bundles untested.",
          "MDP formalization: charge vector state space is restricted to integer Mori cone lattice points (~10^3–10^5 per polytope). Not all lattice points correspond to valid line bundles; invalid actions terminate episode. This is not modeled in the baseline MDP; a practical implementation must handle invalid-action penalties or masking, which could affect DQN more than policy-gradient methods.",
          "Hodge pair diversity: test set limited to h¹¹ ∈ [18, 100]. Generalization to h¹¹ > 100 (rare, up to 128 in KS) untested. Models trained on 500–800 samples may overfit to h¹¹ distribution in training set.",
          "Polytope feature engineering: normalized vertex matrix + Gorenstein index + face lattice depth are chosen a priori. Optimal features for DRL policy learning unknown; alternative feature sets (persistent homology, spectral polytope invariants) not explored.",
          "Baseline algorithms: greedy policy (charge magnitude maximization) is physics-motivated but may not be the strongest classical baseline. Genetic algorithms, simulated annealing, or Bayesian optimization not tested.",
          "Reproducibility: StableBaselines3 implementation can vary across PyTorch/CUDA versions. All seeds and hyperparameters must be logged; code released on GitHub for full replication.",
          "Sample efficiency measurement: assumes each cohomology evaluation costs equal wall-clock time. In practice, polytopes with different h¹¹ may have different h⁰ computation costs; this variation is not accounted for."
        ],
        "requires_followup": "Wet-lab analog (if needed): For top-performing DRL policy discovered in computational phase, implement a physics validation step using Macaulay2 or the cohomology solver in CyTools to compute h⁰ for a sample of 50–100 high-confidence bundles predicted by the trained agent. Confirm that h⁰ ≥ 3 predictions match live cohomology computation. If >85% of predicted bundles validate, the DRL method is ready for practical deployment on the full KS database (195k+ unscanned polytopes). If <70%, retrain with live cohomology in the loop (expensive but higher fidelity). No wet-lab experiment required if the goal is algorithmic comparison only."
      },
      "keywords": [
        "deep reinforcement learning",
        "Calabi-Yau line bundle search",
        "actor-critic algorithms",
        "sparse reward exploration",
        "DRL framework benchmarking",
        "policy gradient methods"
      ],
      "gap_similarity": 0.629780650138855,
      "gap_distance": 5,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "Few-shot learning",
      "gap_concept_b": "miniImageNet dataset",
      "source_question": "Can few-shot learning frameworks trained on miniImageNet-scale polytope datasets enable accurate prediction of Hodge numbers and line bundle cohomology for CY3s with <100 labeled examples per Hodge pair class, and does transfer learning from visual feature spaces to geometric invariants improve sample efficiency?",
      "statement": "We hypothesize that meta-learning frameworks trained on episodic mini-batches of polytope geometric invariants (Gorenstein index, lattice point density, face lattice depth) can predict h¹¹ and h⁰ ≥ 3 line bundle existence with >80% accuracy using <10 labeled examples per Hodge pair class, and that this few-shot approach transfers across unseen h¹¹ values with ≥15% higher sample efficiency than standard supervised regression.",
      "mechanism": "Few-shot meta-learning optimizes a task-agnostic feature embedding such that small perturbations in support set composition (5–10 examples per class) yield stable predictions on query polytopes from the same distribution. By training on episodic tasks stratified by Hodge number class, the meta-learner learns to extract invariant geometric signatures (lattice density, Gorenstein index, face complexity) that generalize across h¹¹ bins. Transfer learning exploits shared structure in the polytope space: if h¹¹ ∈ [13, 50] and h¹¹ ∈ [50, 128] occupy overlapping feature regions, a meta-learner trained on one bin requires fewer examples to classify the other than a scratch-trained model.",
      "prediction": "A prototypical network or MAML meta-learner trained on 30 episodic tasks (stratified by h¹¹ decile) will achieve ≥80% F1-score predicting h⁰ ≥ 3 on held-out test polytopes using only 5 support examples per class, and will improve few-shot accuracy on unseen h¹¹ bins by ≥15 percentage points (Δ F1 ≥ 0.15) compared to a baseline supervised classifier fine-tuned on the same 5-example budget.",
      "falsifiable": true,
      "falsification_criteria": "If the meta-learner achieves <75% F1-score on h⁰ ≥ 3 prediction with 5 support examples, OR if few-shot transfer accuracy on unseen h¹¹ bins improves by <8 percentage points compared to fine-tuned baseline, the hypothesis is refuted. Additionally, if the meta-learned embedding exhibits <0.3 cosine similarity correlation with the theoretical lattice-density–Gorenstein-index pair, the causal mechanism claim is falsified.",
      "minimum_effect_size": "F1 ≥ 0.80 on h⁰ ≥ 3 prediction with k_shot = 5; Δ F1 ≥ 0.15 (meta-learner minus baseline); cosine similarity ≥ 0.30 between learned embedding and hand-engineered geometric invariants.",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Curate an episodic mini-dataset from the Kreuzer-Skarke database (10k labeled polytopes × geometric invariants), stratify into 30–50 meta-tasks by h¹¹ bin, train prototypical networks and MAML on 5–10-shot episodes, evaluate transfer to held-out h¹¹ values and compare sample efficiency against fine-tuned supervised baselines.",
        "steps": [
          "Extract and validate the 10k fully-computed CY3 polytopes from the KS database; compute/fetch Gorenstein index, lattice point density, face lattice depth, Mori cone generators, and intersection numbers κ_abc using CyTools.",
          "Normalize geometric invariants to zero mean, unit variance. Encode each polytope as a d-dimensional feature vector: concatenate normalized Gorenstein index, lattice point density, face lattice depth, log-volume, second Chern class c₂, and top-10 Mori cone generators (dimensionality d ≈ 20–30).",
          "Label each polytope with two binary targets: (1) h¹¹ ∈ [13, 50] vs. [50, 128]; (2) h⁰ ≥ 3 for some line bundle L (inferred from cohomology data in the KS database or computed subset). Stratify the 10k polytopes into 30 meta-tasks: one per h¹¹ decile or h¹¹ bin, each containing 200–400 polytopes.",
          "For each meta-task m ∈ {1, …, 30}, construct 500 episodic training tasks: sample 5 or 10 support examples uniformly from task m, sample 50 query examples from the same task (disjoint from support). Repeat for validation (500 episodes) and test (500 episodes, held-out polytopes).",
          "Train prototypical networks (Proto-Nets) on episodic tasks: optimize distance-weighted classification in learned embedding space using squared Euclidean distance, 5-way-5-shot and 5-way-10-shot regimes. Use 2-layer MLP with 64 hidden units; learn task-specific task adaptation layers if using MAML variant.",
          "Train MAML (Model-Agnostic Meta-Learning) baseline: inner loop 1 gradient step, outer loop 1000 iterations, learning rate 0.01 inner / 0.001 outer. Compare 5-shot and 10-shot regimes.",
          "Train non-meta baselines: (a) XGBoost regression on 5 support examples per class + fine-tuning on query set; (b) logistic regression with L2 regularization; (c) random forest with 100 trees. Report F1-score on same 5-example and 10-example budgets.",
          "Evaluate all models on 5 held-out test meta-tasks (unseen h¹¹ bins). Report F1-score, precision, recall, and area under ROC curve (AUC) for h⁰ ≥ 3 prediction. Measure sample efficiency as F1-score vs. k_shot curves; compute Δ F1 = (meta-learner F1 at k=5) − (baseline F1 at k=5).",
          "Compute learned embedding representation for all 10k polytopes using the trained meta-learner. Extract 2D UMAP or t-SNE projection; visualize by h¹¹ value and h⁰ ≥ 3 label. Compute Spearman rank correlation between each embedding dimension and (lattice point density, Gorenstein index, face lattice depth); report strongest correlations.",
          "On success (Δ F1 ≥ 0.15, F1 ≥ 0.80), apply the trained meta-learner to screen 5,000 unlabeled polytopes from the remaining 195k unscanned set (stratified by h¹¹ ≈ 18). Rank by predicted h⁰ ≥ 3 confidence. Select top 100 high-confidence and 100 low-confidence predictions for ground-truth validation via CyTools cohomology computation.",
          "Report confusion matrices, calibration plots, and failure case analysis: identify polytope classes (e.g., specific h¹¹ ranges or lattice geometries) where the meta-learner underperforms, and cross-check against the theoretical lattice-density–Gorenstein-index mechanism."
        ],
        "tools": [
          "Kreuzer-Skarke polytope database (10k labeled subset)",
          "CyTools (toric geometry computations: Gorenstein index, face lattice, Mori cone, cohomology)",
          "PyTorch with learn2learn library (prototypical networks, MAML implementation)",
          "scikit-learn (baseline classifiers: XGBoost, logistic regression, random forest)",
          "UMAP or t-SNE (embedding visualization)",
          "NumPy, pandas (data preprocessing, normalization)"
        ],
        "computational": true,
        "estimated_effort": "6–8 weeks compute: 1 week data curation + feature extraction; 2–3 weeks meta-learning model training and hyperparameter search (100s of GPU hours); 1–2 weeks evaluation, embedding analysis, and visualization; 1 week unlabeled polytope screening and ground-truth spot checks.",
        "data_requirements": "10k fully-computed polytopes from KS database with Hodge numbers, Gorenstein indices, face lattice, Mori cone data (available via CyTools or KS publication data). Estimated 500 MB–2 GB for feature tensors + episodic data structures.",
        "expected_positive": "Meta-learner achieves F1 ≥ 0.80 on h⁰ ≥ 3 prediction with 5 support examples; Δ F1 ≥ 0.15 vs. fine-tuned baseline; embedding dimensions correlate with lattice-density–Gorenstein-index at Spearman r ≥ 0.30; top-100 high-confidence predictions show ≥75% ground-truth agreement on validation sample.",
        "expected_negative": "Meta-learner F1 < 0.75 at 5-shot; Δ F1 < 0.08; embedding shows <0.2 correlation with hand-engineered invariants; transfer to unseen h¹¹ bins collapses (F1 drops >20 points); validation sample shows <60% agreement, suggesting meta-learner has not captured geometric structure.",
        "null_hypothesis": "H₀: Few-shot meta-learning on polytope geometric invariants provides no improvement in sample efficiency or cross-h¹¹ transfer compared to standard supervised classifiers fine-tuned on k examples. The meta-learner F1-score at k=5 is not significantly different from (non-meta baseline F1 at k=5) and transfer accuracy on unseen h¹¹ bins is not significantly higher than a zero-shot prediction.",
        "statistical_test": "Two-sided paired t-test (meta-learner F1 vs. baseline F1 at fixed k) with Bonferroni correction for multiple comparisons across k ∈ {5, 10} and two targets (h⁰ ≥ 3, h¹¹ class); alpha = 0.05. Mann-Whitney U test for non-parametric comparison of embedding correlation with theoretical invariants (alpha = 0.05). Binomial exact test for validation sample agreement (≥75% vs. random guess 50%; one-sided, alpha = 0.05, n=100).",
        "minimum_detectable_effect": "Δ F1 ≥ 0.15 (15 percentage points) between meta-learner and baseline at k=5, with 80% statistical power and assumed baseline F1 ≈ 0.65 and meta-learner F1 ≈ 0.80. For embedding correlation: Spearman ρ ≥ 0.30 with n=10k polytopes, two-sided test, alpha=0.05.",
        "statistical_power_notes": "Episode-level evaluation: 500 test episodes × 50 query examples per episode = 25k query predictions per held-out task, providing large effective sample size (high power). For 5 held-out test meta-tasks, assume true F1 difference of 0.15 (beta ≈ 0.20, power ≈ 0.80) under two-sided paired t-test. Embedding correlation: n = 10k polytopes, assuming ρ_true ≈ 0.35, yields power >0.99 (one-sided, alpha=0.05). No additional sample size calculation needed; existing 10k polytope database is sufficient.",
        "limitations": [
          "Feature engineering is hand-crafted (Gorenstein index, lattice point density, face lattice depth); does not compare against learned representations from scratch or graph neural networks on toric divisor graphs.",
          "Line bundle cohomology target (h⁰ ≥ 3) is inferred from a subset of the KS database; full validation requires expensive CyTools computation, limiting ground-truth sample to ~200 unlabeled polytopes.",
          "Transfer evaluation is limited to h¹¹ bins present in the training split; true generalization to exotic Hodge pairs not in training data is not tested.",
          "Meta-task stratification by h¹¹ decile is arbitrary; alternative task definitions (e.g., by Gorenstein index or volume) are not explored.",
          "Episodic training assumes imbalanced class ratios (h⁰ ≥ 3 may be rare) are handled implicitly; explicit class-balanced sampling not described."
        ],
        "requires_followup": "Computational study; no wet-lab work required. However, ground-truth validation of the top-100 high-confidence and low-confidence predictions via CyTools cohomology computation (1–2 weeks additional compute) is essential to validate the practical utility of the meta-learner for accelerating the KS screening pipeline. If computational results confirm Δ F1 ≥ 0.15 and embedding correlation ≥ 0.30, recommend a follow-up study deploying the meta-learner on the full 195k unscanned set, screening ~10,000 candidates, and selectively computing ground-truth for the top 500 high-uncertainty polytopes to maximize information gain."
      },
      "keywords": [
        "few-shot learning polytopes",
        "meta-learning Calabi-Yau Hodge numbers",
        "prototypical networks toric geometry",
        "episodic training reflexive polytopes",
        "transfer learning algebraic geometry"
      ],
      "gap_similarity": 0.5986653566360474,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "FPGA Implementation",
      "gap_concept_b": "Vector Processing Element",
      "source_question": "Can optimized Vector Processing Element (VPE) architectures on FPGAs significantly accelerate the inference latency of neural networks trained to predict line bundle cohomology (h⁰, h¹, h², h³) and Hodge numbers on Calabi-Yau threefolds, and if so, what is the pareto frontier of throughput vs. model accuracy trade-off for pre-screening tasks in the KS database?",
      "statement": "We hypothesize that FPGA-based Vector Processing Elements optimized for low-precision dot products on normalized polytope features (int8 quantization + memory-access pipelining for sparse Mori cone generators) will reduce line bundle cohomology inference latency by ≥50× relative to CPU baseline while maintaining h⁰ prediction accuracy ≥95% of the float32 software model, enabling real-time pre-screening of the remaining 195,000+ Kreuzer-Skarke polytopes.",
      "mechanism": "FPGA VPE acceleration exploits two complementary properties: (1) polytope feature vectors (normalized lattice densities, Gorenstein index, face lattice depth) are low-magnitude integers amenable to int8 quantization without loss of discriminative signal; (2) inference workloads (matrix-vector products in the trained neural network forward pass) are memory-bandwidth-limited on CPUs but compute-bound on FPGAs when memory access patterns are pipelined to match the sparsity pattern of line bundle charge vectors. Quantization + pipelining shift the bottleneck from memory to on-chip compute, enabling massive throughput scaling and energy efficiency gains.",
      "prediction": "A specialized FPGA VPE design implementing int8 dot products with pipelined sparse indexing will achieve ≥50× latency reduction (latency_cpu / latency_fpga ≥ 50) for single-polytope inference on a mid-range FPGA (Xilinx Zynq or Intel Arria 10) compared to an optimized x86-64 CPU baseline, while maintaining top-1 accuracy on the h⁰ ≥ 3 binary classification task ≥95% of the software float32 model accuracy.",
      "falsifiable": true,
      "falsification_criteria": "Measured latency reduction is <30× (i.e., latency_cpu / latency_fpga < 30), or h⁰ ≥ 3 classification accuracy drops >5 percentage points below the software baseline (e.g., from 92% to <87%), or the FPGA design cannot be implemented within a single mid-range FPGA device without exceeding LUT/DSP/BRAM resource limits.",
      "minimum_effect_size": "latency_fpga ≤ latency_cpu / 50 (≥50× speedup); h⁰ classification accuracy ≥ baseline_accuracy − 5 percentage points; energy per inference ≤ 1/100 of CPU baseline (≤1 mJ for single inference).",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Implement a hardware-software co-design pipeline: (1) profile existing trained ML models to identify compute and memory bottlenecks; (2) design and synthesize baseline (float32) and optimized (int8 + sparse pipelining) FPGA VPE variants; (3) benchmark end-to-end latency, throughput, energy, and accuracy on a curated validation set of ~1000 polytopes with known h⁰ labels from the KS database; (4) compare to CPU and GPU baselines.",
        "steps": [
          "Profile the trained baseline neural network model (e.g., MLP or XGBoost on polytope features) on CPU to measure: (i) arithmetic intensity (FLOPs per memory byte); (ii) memory bandwidth utilization; (iii) precision requirements for each layer (determine if int8 suffices); (iv) sparsity in feature vectors and weight matrices. Use VTune, perf, or nsys profilers.",
          "Quantize the trained model from float32 to int8 using post-training quantization (PTQ) with calibration on a representative subset of KS polytopes (~500 samples). Measure accuracy drop on a held-out validation set (1000 polytopes). If drop >5%, apply quantization-aware training (QAT) and retrain for ≤10 epochs.",
          "Implement three FPGA VPE designs in HLS (Xilinx Vivado HLS or Intel OneAPI) targeting a Xilinx Zynq UltraScale+ or Intel Arria 10: (i) Baseline: standard systolic array of MAC units, float32, no memory optimization; (ii) Optimized-Basic: int8 quantization, pipelined memory reads; (iii) Optimized-Sparse: int8 + sparse indexing (zero-skipping) for Mori cone generator vectors.",
          "Synthesize all three designs to RTL (Verilog/VHDL), place & route, and measure: (i) latency (clock cycles to complete single inference); (ii) throughput (inferences per second at target clock frequency); (iii) resource utilization (LUTs, DSPs, BRAMs); (iv) power consumption (via power analysis tool or in-system measurement).",
          "Create a validation dataset: randomly sample 1000 polytopes from the KS database (stratified by h¹¹ to ensure Hodge diversity). Compute ground truth h⁰ for each using CyTools or existing CY3-ML labels.",
          "Deploy the quantized int8 model and all three FPGA designs (via bitstreams) on physical hardware (Zynq + Arria 10 evaluation boards). Run inference on the 1000-polytope validation set on each design.",
          "Measure per-inference latency, throughput (polytopes/second), and energy (power × latency) for each design. Compare to CPU baseline (batch-size-1 inference on a modern x86 CPU, e.g., Intel i7 or AWS c5.4xlarge) and GPU baseline (NVIDIA T4 or A100 with batch size 1 or 32, whichever is more representative of real deployment constraints).",
          "Evaluate h⁰ ≥ 3 binary classification accuracy on the FPGA designs by running inference and comparing predicted class to ground truth. Compute precision, recall, and top-1 accuracy.",
          "Construct a Pareto frontier: plot throughput vs. h⁰ classification accuracy for all designs (baseline CPU, GPU, FPGA-Baseline, FPGA-Optimized-Basic, FPGA-Optimized-Sparse). Identify the design(s) that dominate in the latency–accuracy space.",
          "Extrapolate: if the optimized FPGA design achieves ≥50× speedup on the 1000-polytope set, estimate the wall-clock time to pre-screen the full remaining 195,000+ KS polytopes (accounting for batch inference and I/O overhead). Project energy consumption and carbon footprint."
        ],
        "tools": [
          "Xilinx Vivado HLS or Intel OneAPI for HLS-to-RTL synthesis",
          "Xilinx Zynq UltraScale+ or Intel Arria 10 evaluation board (FPGA hardware)",
          "CyTools (for ground-truth line bundle cohomology computation and polytope feature extraction)",
          "Kreuzer-Skarke database (polytope labels and topology)",
          "Trained ML baseline model (e.g., scikit-learn RandomForest or XGBoost for h⁰ prediction)",
          "PyTorch or TensorFlow for post-training quantization and QAT",
          "Xilinx Vivado Power Analyzer or Intel PowerPlay for power estimation",
          "Intel VTune, Linux perf, or NVIDIA nsys for CPU/GPU profiling",
          "Python benchmarking scripts (latency measurement via timeit or custom timing harness on FPGA)",
          "CyMetric or similar for feature normalization and preprocessing"
        ],
        "computational": true,
        "estimated_effort": "8–12 weeks: (1) profiling & quantization, 2–3 weeks; (2) HLS design & synthesis, 3–4 weeks (includes multiple iterations); (3) hardware bring-up & benchmarking, 2–3 weeks; (4) analysis & extrapolation, 1 week. Parallelizable phases reduce wall-clock time.",
        "data_requirements": "Kreuzer-Skarke database (reflexive polytopes, ~500,000 examples); CyTools output (polytope features, h⁰ labels for ≥1000 polytopes); trained ML baseline model (weights and architecture); FPGA evaluation boards (Zynq/Arria); compute cluster for synthesis (overnight turnarounds for P&R).",
        "expected_positive": "FPGA-Optimized-Sparse design achieves latency_fpga ≤ 20 µs for single-polytope inference (latency_cpu ≈ 1–2 ms on CPU), yielding 50–100× speedup. h⁰ ≥ 3 classification accuracy remains ≥91% (if baseline is 96%). Power per inference ≤ 1 mJ. Pareto frontier shows FPGA designs dominating all CPU and GPU single-batch baselines.",
        "expected_negative": "FPGA designs fail to exceed 20× speedup over CPU, or h⁰ classification accuracy drops to <85%, or the int8 quantized model shows accuracy degradation >8% and QAT cannot recover it within the project budget. Resource utilization exceeds 95% of the FPGA (no headroom for scaling). GPU baseline (NVIDIA T4 with batch size 32) matches or exceeds FPGA latency per inference when amortized.",
        "null_hypothesis": "H₀: There is no statistically significant difference in single-inference latency, throughput, or h⁰ prediction accuracy between the FPGA-Optimized VPE design and the CPU/GPU baseline implementations, controlling for clock frequency and batch size. Any observed difference is within measurement noise (≤20% relative standard error).",
        "statistical_test": "One-way ANOVA (latency) and Mann–Whitney U test (accuracy, if distribution is non-normal) on n_inferences = 1000 repeated single-polytope inferences per design. Null hypothesis rejected if p < 0.05 and effect size (latency speedup ratio or accuracy difference) meets minimum_effect_size threshold. Power = 0.90, assuming effect size ≥50× latency reduction.",
        "minimum_detectable_effect": "≥50× reduction in median single-inference latency (latency_fpga / latency_cpu ≤ 0.02); h⁰ classification accuracy within 5 percentage points of software baseline (e.g., if software = 96%, FPGA ≥ 91%); energy per inference ≤ 1 mJ (vs. ~100 mJ on CPU). These thresholds are required for practical pre-screening of the full KS database.",
        "statistical_power_notes": "Sample size n = 1000 polytopes in validation set. For latency comparison: assume median latency_cpu = 1 ms, target median latency_fpga = 20 µs (50× reduction), standard deviation σ ≈ 10 µs. At α = 0.05 (two-tailed), power = 0.95 to reject H₀. For accuracy: assume baseline h⁰ accuracy = 96%, FPGA accuracy = 91% (5% drop threshold), binomial proportion test with n = 1000, α = 0.05, power = 0.90. Repeated inferences (1000 runs per design) ensure stable variance estimates.",
        "limitations": [
          "Validation limited to 1000 polytopes; full 195,000+ polytope pre-screening remains extrapolated and untested in situ.",
          "Quantization (int8) is dataset-specific; PTQ calibration on one subset may not generalize to polytopes with extreme h¹¹ values (h¹¹ > 100) outside the training distribution.",
          "FPGA latency includes host-to-device communication overhead (PCIe); true end-to-end latency depends on system integration (not captured if only measuring on-chip cycles).",
          "GPU baseline comparison is single-batch (batch size 1); modern GPU inference often requires batch size ≥8–32 to amortize launch overhead; fair comparison requires normalizing by batch size or deploying with realistic batching.",
          "Power measurements are design-level estimates (Xilinx Power Analyzer); actual silicon power may vary by ±20% due to manufacturing process variation and temperature.",
          "No cross-FPGA vendor validation; design is targeted to Xilinx/Intel; replicating on other vendors (e.g., Lattice, Microsemi) would require re-synthesis and may yield different speedups.",
          "Does not address model generalization to out-of-distribution polytopes (e.g., Calabi-Yau 4-folds, or new enumerate methods not covered by KS database)."
        ],
        "requires_followup": "Wet-lab equivalent: Deploy the validated FPGA accelerator on a production enumerative search system (e.g., a distributed polytope-generation pipeline used by the CyTools collaboration) and measure wall-clock time and energy to screen 10,000–50,000 new polytopes. Cross-validate that FPGA-predicted h⁰ ≥ 3 candidates match (≥90% recall) those confirmed by full CyTools computation. If successful, productize the FPGA design as an open-source hardware module (e.g., via the Vivado IP Catalog or OSH Park) for community use in landscape searches."
      },
      "keywords": [
        "FPGA acceleration",
        "neural network inference",
        "Calabi-Yau line bundle prediction",
        "quantization",
        "vector processing element"
      ],
      "gap_similarity": 0.5972116589546204,
      "gap_distance": 3,
      "approved": null,
      "critic_novelty": null,
      "critic_rigor": null,
      "critic_impact": null,
      "critic_note": "",
      "composite_score": 4.25,
      "final_score": 4.25
    },
    {
      "gap_concept_a": "class imbalance learning",
      "gap_concept_b": "majority class",
      "source_question": "Does weighted sampling or loss reweighting of the majority class (high h¹¹, high h⁰ polytopes) during training improve line bundle existence prediction precision-recall tradeoffs for rare three-generation CY3s (χ = -6) compared to standard unweighted cross-entropy?",
      "statement": "We hypothesize that cost-sensitive reweighting of majority-class (h⁰ < 3) polytopes combined with focal loss during training improves line bundle existence prediction (binary h⁰ ≥ 3) precision-recall tradeoffs by at least 15 percentage points in area under the precision-recall curve (AUPRC) compared to standard unweighted cross-entropy, when evaluated on held-out three-generation (χ = −6) CY3s in the Kreuzer-Skarke database.",
      "mechanism": "Standard unweighted cross-entropy loss treats majority-class (negative: h⁰ < 3) and minority-class (positive: h⁰ ≥ 3) polytopes equally, forcing the model to allocate most representational capacity to the 95% majority. Cost-sensitive reweighting and focal loss down-weight easy, high-confidence majority examples and up-weight hard minority examples, allowing the model to learn polytope combinatorial features (Gorenstein index, face lattice depth, Mori cone geometry) that specifically distinguish rare line-bundle-rich geometries. This mechanism shifts the decision boundary in polytope feature space away from the majority cluster toward a precision-recall frontier tuned for detecting rare positive cases.",
      "prediction": "On a stratified holdout test set of 500 h¹¹-balanced three-generation CY3s with ground-truth h⁰ labels, the focal-loss + inverse-frequency reweighting model will achieve AUPRC ≥ 0.72 (with baseline unweighted cross-entropy at ≤ 0.57) and false-negative rate ≤ 8% (vs. baseline ≥ 18%), corresponding to a 15+ percentage-point gain in AUPRC and 50%+ relative reduction in missed bundles.",
      "falsifiable": true,
      "falsification_criteria": "If the reweighted focal-loss model achieves AUPRC < 0.62 on the stratified holdout, or if false-negative rate exceeds 14% at any fixed recall threshold ≥ 90%, the hypothesis is refuted. Additionally, if the computational cost of reweighting + focal loss training (including hyperparameter tuning via cross-validation) exceeds 3× the cost of baseline training without yielding ≥ 10 percentage-point AUPRC gain, the practical utility claim is falsified.",
      "minimum_effect_size": "Δ(AUPRC) ≥ 0.15 (focal-loss + reweighting vs. baseline); false-negative rate reduction ≥ 50% relative to baseline at fixed recall ≥ 90%; confidence interval on AUPRC difference excludes zero at α = 0.05 (two-sided).",
      "novelty": 3,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Construct a binary classification pipeline (h⁰ ≥ 3 vs. h⁰ < 3) on labeled CY3 polytope features from the KS database. Compare unweighted baseline (logistic regression + XGBoost) against three reweighting schemes (inverse class frequency, focal loss γ=2, and threshold-moving) via stratified cross-validation and held-out test evaluation, measured by AUPRC, false-negative rate, and computational cost. Validate top-ranked predictions on a curated unlabeled subset using CyTools computation.",
        "steps": [
          "Extract polytope feature vectors for ~5,000 labeled CY3s with verified h⁰ counts from CyTools/KS database: vertex count, face counts by dimension, Gorenstein index, Mori cone generators (count and norms), dual polytope statistics, second Chern class c₂, normalized lattice point density, GLSM charge matrix rank.",
          "Construct binary outcome: h⁰ ≥ 3 (positive, ~5% of sample) vs. h⁰ < 3 (negative, ~95%), restricted to χ = −6 polytopes.",
          "Perform stratified train-val-test split (60–20–20) stratified by h¹¹ decile to control for confounding between Hodge numbers and line bundle richness.",
          "Train baseline unweighted logistic regression and XGBoost classifier on 3,000 training polytopes; evaluate on 1,000 validation polytopes using precision, recall, F1, AUPRC, and false-negative rate at fixed recall thresholds (80%, 90%, 95%).",
          "Apply three reweighting schemes to the same training set: (a) inverse class-frequency weights w_c = n_total / (n_classes × n_c); (b) focal loss with γ=2, α=0.25; (c) probability threshold-moving post-hoc (shift decision boundary from 0.5 to optimal threshold on validation set to maximize F2 or minimize false-negative rate).",
          "Re-train logistic regression and XGBoost with each reweighting scheme; report AUPRC, false-negative rate, training time (CPU seconds) on validation set.",
          "Perform 5-fold stratified cross-validation across all six configurations (3 reweighting × 2 models) on 4,000 combined train+val polytopes; compute mean and 95% CI for AUPRC and false-negative rate.",
          "Evaluate all six models on held-out test set (1,000 polytopes, h¹¹-stratified); report primary metric (AUPRC ± 95% CI), false-negative rate, and computational cost (training + inference time).",
          "Select best-performing model (highest AUPRC with false-negative rate ≤ 10%); rank 10,000 unlabeled polytopes from the 195k+ backlog by predicted probability of h⁰ ≥ 3.",
          "Sample 100 top-ranked predictions (rank 1–1,000) uniformly; compute ground-truth h⁰ using CyTools (Sage); measure agreement rate and computational cost reduction (CPU-hours saved vs. exhaustive search) on the sampled subset.",
          "Conduct sensitivity analysis: re-run focal loss with γ ∈ {1, 2, 3} and weight exponent on inverse frequency reweighting; measure robustness of AUPRC and false-negative rate to hyperparameter choice."
        ],
        "tools": [
          "Python 3.10+; scikit-learn for baseline classification",
          "XGBoost library for gradient-boosted trees with custom loss functions",
          "PyTorch or TensorFlow for focal loss implementation (or sklearn-compatible wrapper)",
          "CyTools (Calabi-Yau Tools) for ground-truth h⁰ computation via Sage",
          "Kreuzer-Skarke database (mirror or local copy) for polytope coordinates and labeled h¹¹, χ data",
          "Pandas/NumPy for feature engineering and data wrangling",
          "Matplotlib/Seaborn for precision-recall curves, confusion matrices, and ablations",
          "Scikit-learn metrics: precision_recall_curve, auc, confusion_matrix, roc_auc_score"
        ],
        "computational": true,
        "estimated_effort": "3–4 weeks: feature extraction (~3–5 days, I/O-bound on CyTools), baseline model training & validation (~5 days, <1 CPU per config), reweighting implementation & cross-validation (~7 days), unlabeled set ranking & ground-truth validation (~5 days, requires CyTools compute on 100 polytopes), sensitivity analysis & write-up (~3 days).",
        "data_requirements": "~5,000 labeled CY3 polytope vertices and face lattice (from KS database / CyTools); ground-truth h⁰ counts for each (from cohomology computation); unlabeled 10,000–20,000 polytope features from unscanned portion of KS backlog (combinatorial data only, no cohomology needed a priori).",
        "expected_positive": "Focal-loss + inverse-frequency reweighting model achieves AUPRC ≥ 0.72 (vs. baseline ≤ 0.57) on held-out test set; false-negative rate ≤ 8% at recall ≥ 90%; top-ranked 100 predictions on unlabeled set agree with CyTools ground truth at ≥ 85% rate; computational cost reduction by factor ≥ 50× (ranked triage + selective CyTools verification vs. exhaustive search).",
        "expected_negative": "Focal-loss + inverse-frequency reweighting model achieves AUPRC < 0.62 on held-out test set; false-negative rate > 14% at recall ≥ 90%; top-ranked predictions on unlabeled set agree with CyTools ground truth at < 70% rate; computational cost reduction < 10×; reweighting + cross-validation overhead exceeds 3× baseline training cost without commensurate AUPRC gain.",
        "null_hypothesis": "H₀: Cost-sensitive reweighting and focal loss do not improve line bundle existence prediction (h⁰ ≥ 3) precision-recall tradeoffs beyond unweighted cross-entropy. Specifically, AUPRC(focal + reweight) − AUPRC(baseline unweighted) ≤ 0.10, or false-negative rate(focal + reweight) at fixed recall ≥ 90% is ≥ baseline false-negative rate − 0.05 (i.e., no clinically meaningful improvement).",
        "statistical_test": "Two-sided paired bootstrap t-test on 5-fold stratified cross-validation AUPRC differences between reweighted and baseline models (B = 10,000 bootstrap resamples); alpha = 0.05; report 95% CI on Δ(AUPRC). McNemar's test on false-negative rate (binary: model A > model B in FNR, yes/no) at fixed recall = 90% across 5 folds. One-way ANOVA on held-out test AUPRC across six configurations (3 reweighting × 2 model types) to test for interaction; post-hoc Tukey HSD if p < 0.05.",
        "minimum_detectable_effect": "Δ(AUPRC) = 0.15 at α = 0.05, power ≥ 0.80, assuming σ(AUPRC) ≈ 0.08 (from pilot data or prior CY3-ML regression studies). With 5 folds × 1,000 test samples per fold, expected detectable difference in false-negative rate ≥ 5 percentage points (e.g., 18% → 13%) at α = 0.05, power = 0.85.",
        "statistical_power_notes": "5-fold stratified cross-validation on 4,000 labeled samples (800 test per fold) with ~40 positives and 760 negatives per fold. Assuming baseline AUPRC = 0.55 ± 0.08 (SD) and reweighted AUPRC = 0.70 ± 0.08, paired t-test on 5 fold-level AUPRC differences has t = (0.15) / (0.08 / √5) ≈ 4.2, p < 0.01, power ≈ 0.95. For held-out test (1,000 samples, ~50 positives, 950 negatives), precision-recall curve is estimated with ≥500 positive predictions per model (sufficient for stable AUPRC estimation per Davis & Goadrich 2006). No multiple-comparison correction needed if focal-loss + inverse-frequency is designated a priori as primary hypothesis; sensitivity analysis (γ sweep) uses Bonferroni α' = 0.05 / 3 = 0.017.",
        "limitations": [
          "Labeled CY3 dataset (5,000 polytopes) may exhibit selection bias if h⁰ ≥ 3 bundles were preferentially computed; true population minority rate unknown.",
          "Feature engineering uses low-level polytope combinatorics (vertex count, Gorenstein index); richer topological/geometric features (persistent homology, Kähler cone geometry) may be necessary for minority-class separation and are not tested here.",
          "Ground-truth h⁰ via CyTools is computationally expensive (~hours per polytope for large faces); validation on 100 unlabeled polytopes is sampling check, not full holdout.",
          "Focal loss and inverse-frequency weighting are standard techniques; novelty is in domain application and quantified impact on precision-recall, not in method invention.",
          "Hyperparameter tuning (focal γ, weight exponents, threshold) is performed on validation set; contamination of test set by indirect tuning is possible if cross-validation is not strictly stratified.",
          "Assumption that polytope combinatorics alone (without algebraic geometry features like intersection numbers or c₂) can predict line bundle existence may be optimistic; negative result would require richer features (e.g., Mori cone structure, GLSM charge matrix)."
        ],
        "requires_followup": "Wet-lab follow-up is not required; this is a computational study. However, if the trained reweighted model is deployed on production backlog (10k–50k unlabeled polytopes), a downstream CyTools verification pipeline must be implemented to confirm top-ranked predictions at scale. This would involve: (1) rank predicted h⁰ ≥ 3 probability; (2) selectivity threshold (e.g., top 1% = 500 polytopes); (3) parallel CyTools batch compute on 500 ranked candidates (feasible on multi-node cluster); (4) measure true-positive rate and computational cost savings (ratio of predictions screened vs. verified). If AUPRC or false-negative rate on the validation sample falls short (< 0.65 AUPRC or > 12% FNR), richer feature engineering—incorporating Mori cone generators, dual polytope statistics, and GLSM charge matrix rank—should be tested in a follow-up computational experiment before production deployment."
      },
      "keywords": [
        "class imbalance learning",
        "focal loss",
        "cost-sensitive classification",
        "line bundle cohomology prediction",
        "Calabi-Yau machine learning",
        "precision-recall tradeoff"
      ],
      "gap_similarity": 0.605048656463623,
      "gap_distance": 999,
      "approved": null,
      "critic_novelty": 2,
      "critic_rigor": 3,
      "critic_impact": 2,
      "critic_note": "Cost-sensitive reweighting + focal loss for imbalanced binary classification is well-established; applying it to Calabi-Yau line bundle prediction is domain-specific but not mechanistically novel, and the hypothesis lacks quantified justification for why polytope geometry should respond differently ",
      "composite_score": 3.9,
      "final_score": 3.075
    },
    {
      "gap_concept_a": "minority class",
      "gap_concept_b": "majority class",
      "source_question": "Does explicit minority-class resampling or cost-sensitive learning on line bundle cohomology prediction improve generalization to rare high-generation-number Calabi-Yau threefolds (h⁰ ≥ 3), and how do reweighting strategies rank-order polytope families by their true bundle-richness phenotype rather than training-set prevalence?",
      "statement": "We hypothesize that cost-sensitive learning with focal loss and minority-class reweighting on line bundle cohomology prediction reduces the false-negative rate for h⁰ ≥ 3 polytopes by >40% relative to baseline XGBoost, and that this reweighting explicitly ranks polytope families by true bundle-richness phenotype (measured by oracle h⁰ values) rather than training-set prevalence, achieving Spearman ρ > 0.65 for predicted h⁰ ranking on held-out h⁰ ≥ 3 minority test set.",
      "mechanism": "Standard supervised models optimize accuracy across the full class distribution, which is dominated by low-bundle examples (h⁰ ≤ 2), causing the decision boundary to shift away from the minority class (h⁰ ≥ 3) phenotype. Focal loss down-weights easily-classified majority examples and up-weights minority-class misclassifications; explicit class-weight balancing reorients the objective to prioritize recall and ranking precision on rare high-generation polytopes. This mechanistically shifts the learned polytope-to-bundle map from a prevalence-biased predictor toward one that captures the intrinsic geometric signatures of bundle richness.",
      "prediction": "Reweighted focal-loss models will achieve at least 40% reduction in false-negative rate (FNR) for h⁰ ≥ 3 examples compared to baseline unweighted XGBoost (e.g., FNR drops from 35% to below 21%), while maintaining precision at least 0.70 on predicted h⁰ ≥ 3 polytopes; simultaneously, the ranking of polytopes by predicted h⁰ on the minority test set will show Spearman correlation ρ at least 0.65 with oracle h⁰ values computed via direct cohomology.",
      "falsifiable": true,
      "falsification_criteria": "If reweighted models achieve less than 20% improvement in FNR (FNR remains above 30%), or if Spearman ρ for h⁰ ranking on the minority test set drops below 0.50, or if precision on predicted h⁰ ≥ 3 falls below 0.60, the hypothesis is refuted. Additionally, if synthetic minority generation via SMOTE or WAE fails to improve ranking correlation (ρ increases by less than 0.05 post-augmentation), the mechanistic claim that minority-class signal is recoverable from geometry is falsified.",
      "minimum_effect_size": "FNR improvement at least 40% absolute (e.g., 35% to below 21%); Spearman ρ at least 0.65 for h⁰ ranking on h⁰ ≥ 3 minority test set; precision at least 0.70 on minority positive predictions; AUC-PR (area under precision-recall curve) at least 0.50 on minority class.",
      "novelty": 4,
      "rigor": 5,
      "impact": 4,
      "replication_risk": "low",
      "experiment": {
        "approach": "Partition the Kreuzer-Skarke database (or expanded CyTools scan cohort) by line bundle multiplicity (h⁰ = 0, 1, 2, ≥3) to quantify class imbalance. Retrain XGBoost and MLP models using focal loss, class-weight balancing, and stratified k-fold cross-validation. Validate on a held-out test set enriched in minority examples (h⁰ ≥ 3) by measuring false-negative rate, precision-recall curves, and ranking correlation against oracle cohomology values. Generate synthetic minority polytopes using SMOTE and Wasserstein autoencoders conditioned on cheap polytope features to test whether minority-class signal is recoverable.",
        "steps": [
          "Segment the KS database by h⁰ value (0, 1, 2, ≥3); compute class frequencies and imbalance ratio; document Hodge number distribution and polytope features (lattice-point density, Gorenstein index, face-lattice depth).",
          "For a random 70% training subset: train baseline XGBoost with default class weights; train focal-loss MLP with gamma=2.0; train cost-weighted XGBoost with class_weight balancing using stratified 5-fold CV to optimize F1 and AUC-PR, not accuracy.",
          "On held-out test set (30% of data) stratified to contain at least 200 h⁰ ≥ 3 examples: compute true-positive rate, false-negative rate, precision, recall, F1 score, and AUC-PR for h⁰ ≥ 3 binary classification; compute Spearman rank correlation between predicted h⁰ values and oracle h⁰ on at least 100 minority examples.",
          "Apply SMOTE to training set conditioned on lattice-point density and Gorenstein index; retrain weighted XGBoost and focal-loss MLP; measure FNR and Spearman ρ improvement on held-out test.",
          "Train Wasserstein autoencoder on full polytope training set with 32-dimensional latent space; generate synthetic minority polytopes by sampling from h⁰ ≥ 3 conditioned latent distribution; validate syntheticity via geometric constraints; retrain weighted XGBoost on augmented training data; measure FNR and Spearman ρ.",
          "Compute baseline model FNR and Spearman ρ as control; tabulate improvements across reweighting strategies with 95% CI via bootstrap on minority test set.",
          "Simulate pre-screening pipeline: rank all polytopes by predicted h⁰; measure recall at fixed FPR (10%, 5%, 1%) to compute wall-clock speedup relative to exhaustive cohomology computation."
        ],
        "tools": [
          "Kreuzer-Skarke database or expanded CyTools polytope scan",
          "XGBoost with class_weight and hyperparameter tuning",
          "PyTorch with focal loss implementation",
          "imbalanced-learn library for SMOTE",
          "scikit-learn for metrics and cross-validation",
          "Wasserstein autoencoder in PyTorch with Sliced Wasserstein distance",
          "CyTools for polytope combinatorics and oracle cohomology computation",
          "Matplotlib and seaborn for visualization"
        ],
        "computational": true,
        "estimated_effort": "3-4 weeks: 1 week data wrangling and oracle cohomology, 1 week baseline and reweighted models, 1 week SMOTE and WAE, 3-5 days ranking validation, 3-5 days manuscript.",
        "data_requirements": "Kreuzer-Skarke database (380,638 polytopes) or expanded CyTools scan. Required per polytope: vertex matrix, face lattice, dual polytope, Mori cone generators, second Chern class, lattice-point density, Gorenstein index, h⁰ labels. Minimum: 30,000 polytopes with h⁰ labels; at least 300 examples with h⁰ ≥ 3.",
        "expected_positive": "Focal-loss and cost-weighted XGBoost achieve FNR at most 21% (at least 40% improvement vs baseline ~35%), precision at least 0.70, and Spearman ρ at least 0.65 on minority test. SMOTE improves ρ by at least 0.05. WAE synthetic polytopes improve FNR by additional at least 5% while respecting reflexive polytope constraints. Pre-screening achieves at least 50× speedup.",
        "expected_negative": "Reweighted models show FNR improvement less than 20% (FNR above 30%), Spearman ρ below 0.50, or precision below 0.60. SMOTE and WAE synthetic polytopes violate constraints or fail to improve ranking (Δρ less than 0.05). Cost-weighting and focal loss do not reduce false-negative rate, falsifying the mechanistic hypothesis.",
        "null_hypothesis": "H0: There is no significant difference in false-negative rate, precision, or ranking correlation (Spearman ρ) between baseline unweighted XGBoost and cost-sensitive or focal-loss models on the minority (h⁰ ≥ 3) test set. The minority-class phenotype is not predictable from polytope combinatorics.",
        "statistical_test": "Two-sided bootstrap t-test on FNR difference with alpha=0.05, 10,000 bootstrap replicates; Spearman rank correlation test (H0: ρ=0) with alpha=0.05 on n at least 100 minority examples; precision difference 95% confidence interval via stratified CV. Bonferroni-corrected alpha=0.017 across three primary outcomes.",
        "minimum_detectable_effect": "FNR improvement at least 10 percentage points (35% to 25%) detectable with n_minority at least 200, alpha=0.05, power=0.80; Spearman ρ at least 0.50 detectable with n at least 100, alpha=0.05, power=0.80; AUC-PR at least 0.45 detectable with n_minority at least 150, power=0.80.",
        "statistical_power_notes": "Primary outcome: FNR reduction on h⁰ ≥ 3 classification. Assumed baseline FNR=35%, target FNR=21%. Binomial test: n_minority_test at least 200, alpha=0.05 two-sided, power at least 0.80 yields detectable FNR difference at least 10 percentage points. Secondary: Spearman ρ at least 0.65 on h⁰ regression. Pearson r=0.65, n at least 100, alpha=0.05, power at least 0.80. Tertiary: AUC-PR at least 0.50. Logistic regression: n_minority at least 150, log-odds per feature at least 0.5, alpha=0.05, power at least 0.80. Allocate at least 300 minority examples to test set (70/30 split implies total n_minority at least 1000) for stable estimates. Train on augmented set (up to 2000 synthetic minority) and validate on original held-out test to avoid leakage.",
        "limitations": [
          "Oracle h⁰ requires expensive cohomology computation; computed on random stratified sample (n at least 100 minority) rather than full test set, introducing sampling variance. Sensitivity analysis via subsampling will quantify uncertainty.",
          "SMOTE and WAE operate in high-dimensional discrete space; synthetic polytopes may violate realizability constraints. Validate via geometric feasibility checks and report violation fraction.",
          "Class imbalance defined as h⁰ ≥ 3 vs ≤2 may obscure continuous heterogeneity. Report results separately for h⁰ in [3,5] and h⁰ at least 6.",
          "Focal loss and cost-weighting sensitive to hyperparameters; all tuning via stratified CV on training set only, test set held blind until final comparison.",
          "Transferability to new polytope datasets not addressed; external validation would strengthen claims but is out-of-scope.",
          "Wall-clock speedup assumes constant per-cohomology cost and ignores parallelization; actual speedup may vary with infrastructure."
        ],
        "requires_followup": "No wet-lab follow-up required. Computational experiment is self-contained. Validation step: if strong ranking correlation is achieved (Spearman ρ greater than 0.70), experimentalists could perform targeted cohomology computations on top 100 polytopes predicted by reweighted model to confirm h⁰ predictions in a small prospective sample, confirming practical utility for pre-screening."
      },
      "keywords": [
        "class imbalance algebraic geometry",
        "focal loss cohomology prediction",
        "minority-class reweighting Calabi-Yau",
        "line bundle phenotype ranking",
        "cost-sensitive learning reflexive polytopes",
        "false-negative rate optimization"
      ],
      "gap_similarity": 0.7653199434280396,
      "gap_distance": 999,
      "approved": null,
      "critic_novelty": 2,
      "critic_rigor": 3,
      "critic_impact": 2,
      "critic_note": "This applies standard cost-sensitive learning techniques (focal loss, class reweighting) to a domain-specific prediction task (line bundle cohomology on polytopes); the core ML methods are well-established and the application, while specialized, does not establish a novel mechanistic principle about",
      "composite_score": 4.25,
      "final_score": 3.25
    },
    {
      "gap_concept_a": "Event-based sensors",
      "gap_concept_b": "Asynchronous processing",
      "source_question": "Can event-based neuromorphic sensors enable asynchronous prediction of Calabi-Yau threefold topological invariants (h⁰, h¹¹) by streaming polytope combinatorial features at their native temporal resolution, and does this asynchronous processing reduce computational latency compared to batch neural network screening?",
      "statement": "We hypothesize that asynchronous event-driven spiking neural networks trained on incrementally revealed polytope combinatorial features achieve ≥10× reduction in wall-clock latency and ≥5× reduction in mean energy per prediction (mJ) compared to batch XGBoost screening, while maintaining ≥95% sensitivity (true positive rate) for identifying Calabi-Yau threefolds with h⁰ ≥ 3.",
      "mechanism": "Event-based neuromorphic processors eliminate redundant batch recomputation by encoding polytope combinatorial discoveries (face lattice expansions, Mori cone generators, vertex confirmations) as discrete spike events that trigger incremental state updates in spiking neural circuits. This replaces synchronous matrix multiplication over the full dense feature matrix with asynchronous event routing and sparse synaptic operations, reducing both memory bandwidth and arithmetic operations per prediction. The causal chain is: polytope feature discovery (cause) → spike event emission → incremental neural state evolution → classification output; latency and energy decrease because spikes are emitted only when new combinatorial information arrives, not at fixed batch intervals.",
      "prediction": "For a fixed test set of 1,000 Kreuzer-Skarke polytopes with ground-truth h⁰ values: (1) the SNN-based asynchronous pipeline will complete 1,000 screening decisions in ≤2.5 seconds wall-clock time, versus ≥25 seconds for batch XGBoost on CPU; (2) total inference energy on a Loihi-2 neuromorphic chip will be ≤0.2 mJ per prediction (vs. ≥1.0 mJ for XGBoost on standard hardware); (3) sensitivity (true positive rate for h⁰ ≥ 3) will be ≥95%, with specificity ≥90%.",
      "falsifiable": true,
      "falsification_criteria": "If, on a held-out test set of 1,000 polytopes: (a) the SNN achieves wall-clock latency ≥25 seconds (i.e., <4× speedup over batch), OR (b) sensitivity for h⁰ ≥ 3 detection falls below 90%, OR (c) inference energy on neuromorphic hardware exceeds 0.8 mJ per prediction, the hypothesis is refuted. Any single violation of these three criteria falsifies the claim.",
      "minimum_effect_size": "≥10× latency reduction (wall-clock time) OR ≥5× energy reduction (mJ per prediction) at the same sensitivity floor (≥95% TPR, ≥90% specificity). Formally: latency_SNN ≤ 2.5 s / latency_XGBoost ≥ 25 s, and energy_SNN ≤ 0.2 mJ / energy_XGBoost ≥ 1.0 mJ.",
      "novelty": 4,
      "rigor": 4,
      "impact": 4,
      "replication_risk": "medium",
      "experiment": {
        "approach": "Design and train a spiking neural network that ingests polytope combinatorial features as discrete spike events (rather than dense batches), benchmark latency and energy against batch XGBoost/MLP on a curated KS test set with ground-truth h⁰ labels, and validate inference on neuromorphic hardware emulation (NEST/Brian2) and hardware specifications (Loihi-2 energy model).",
        "steps": [
          "Curate a labeled benchmark set: 1,000 KS polytopes with known h⁰ values; split 60% train / 20% validation / 20% test. Stratify by h⁰ class (h⁰ < 3 vs. h⁰ ≥ 3) to ensure balanced representation.",
          "Discretize polytope state: define event types — (i) vertex confirmed, (ii) face discovered, (iii) Mori cone generator added, (iv) intersection number computed. Encode each polytope as a temporal sequence of event tuples: (feature_type, feature_index, sequence_order, timestamp). Assign synthetic timestamps proportional to combinatorial discovery order (DFS/BFS traversal of face lattice).",
          "Encode polytope vertices, face lattice depth, Gorenstein index, and Mori cone rank as static feature vectors; construct a spiking input layer that converts each feature into a Poisson spike train (firing rate ∝ feature value). Emit events asynchronously in sequence order, simulating real-time polytope scanning.",
          "Design SNN architecture: input layer (event-triggered Poisson population) → hidden layer (LIF neurons, ~256 units, with recurrent connectivity to accumulate state) → output layer (2 spiking neurons, one per class h⁰ < 3 vs. h⁰ ≥ 3). Use surrogate gradient descent (e.g., Norse or Brian2 package) to train on spike rasters for 200 epochs with Adam optimizer.",
          "Train SNN: minimize binary cross-entropy on event-augmented training set. Use early stopping on validation accuracy. Record: (a) final validation accuracy, (b) mean spike count per neuron during inference (proxy for energy), (c) inference time per sample on CPU (standard Python runtime).",
          "Develop batch baseline: train XGBoost (500 trees, max_depth=8) and MLP (2 hidden layers, 256 units each, ReLU) on the same train/val split using static dense feature matrix (no temporal encoding). Measure: (a) validation accuracy, (b) CPU inference time per sample, (c) peak memory usage during batch inference.",
          "Benchmark on test set (n=200): run both SNN and baselines 100 times; record mean wall-clock latency, standard deviation, and per-sample CPU time. For XGBoost/MLP, measure energy via hardware power meter (CPU+DRAM power × runtime) or RAPL (Running Average Power Limit) API on Intel CPUs.",
          "Emulate neuromorphic execution: use NEST simulator to deploy the trained SNN on a virtual Loihi-2 chip (128 cores, 128k neurons per core). Record: (a) spike traffic (events per millisecond), (b) estimated energy budget using Loihi-2 datasheet values (pJ/spike for synaptic operations), (c) inference latency on the neuromorphic simulator.",
          "Report sensitivity/specificity: on test set, compute true positive rate (h⁰ ≥ 3 correctly classified), false positive rate, and per-class F1 score for both SNN and baselines. Require SNN sensitivity ≥95% and specificity ≥90% to claim success.",
          "Ablation study: retrain SNN with (i) only vertex/edge features (no face lattice), (ii) no temporal event structure (synchronous batch), (iii) reduced SNN depth (1 hidden layer). Measure accuracy drop to isolate contribution of asynchronous event processing.",
          "Reproducibility: release code (Brian2/Norse scripts), trained SNN weights (as PyTorch/TensorFlow checkpoints), test polytope data (vertex matrices, h⁰ labels), and neuromorphic simulation configs (NEST/Loihi-2 parameters) as a public GitHub repository."
        ],
        "tools": [
          "Brian2 (Python spiking neural network simulator)",
          "Norse (PyTorch-native SNN library for surrogate gradient training)",
          "NEST (Neural Simulation Tool for neuromorphic validation)",
          "CyTools (Kreuzer-Skarke polytope database queries)",
          "XGBoost, scikit-learn (batch baselines)",
          "PyTorch (for SNN weight management)",
          "RAPL / power-meter (CPU energy profiling)",
          "Loihi-2 emulator / Intel Neuromorphic Research Community chip specs"
        ],
        "computational": true,
        "estimated_effort": "6–8 weeks: (1) data curation + preprocessing (1 week), (2) SNN architecture design and training (2 weeks), (3) baseline implementation and benchmarking (1.5 weeks), (4) neuromorphic emulation and energy profiling (1.5 weeks), (5) ablation studies and reproducibility package (1 week).",
        "data_requirements": "1,000 labeled Kreuzer-Skarke polytopes with ground-truth h⁰ values (obtainable via CyTools + Macaulay2 cohomology computations or curated from prior ML literature datasets). Static features: polytope vertex matrix (dimension ≤4, ~10–50 vertices per polytope), face lattice (stored as incidence matrix), Mori cone generators (~5–20 per polytope), Gorenstein index, second Chern class. Event sequences: synthetic temporal stamps inferred from face lattice DFS order.",
        "expected_positive": "SNN achieves ≥10× wall-clock speedup (≤2.5 s for 1,000 samples vs. ≥25 s XGBoost), ≥5× energy reduction on Loihi-2 (≤0.2 mJ vs. ≥1.0 mJ baseline), and maintains ≥95% sensitivity / ≥90% specificity for h⁰ ≥ 3 classification. Ablation study shows temporal event structure contributes ≥5% absolute accuracy improvement.",
        "expected_negative": "SNN latency remains ≥25 s (no speedup), OR sensitivity drops <90%, OR energy consumption on neuromorphic hardware exceeds 0.8 mJ per prediction. Alternatively, asynchronous event encoding provides <3% accuracy improvement over batch, suggesting the framework offers no practical advantage.",
        "null_hypothesis": "H₀: Asynchronous event-driven spiking neural networks and batch XGBoost/MLP screening achieve statistically equivalent latency, energy, and accuracy on Calabi-Yau threefold h⁰ classification. No speedup (latency ratio = 1), no energy savings (energy ratio = 1), and no accuracy difference (AUROC difference = 0).",
        "statistical_test": "Paired t-test on wall-clock latency (100 runs per method, α=0.05, one-tailed: SNN < baseline). Two-proportion z-test for sensitivity/specificity (H₀: sensitivity_SNN = 0.90, two-tailed α=0.05). Mann-Whitney U test for energy (non-normal distribution, α=0.05, one-tailed). Bonferroni correction across three tests: α_corrected = 0.05/3 ≈ 0.017 per test.",
        "minimum_detectable_effect": "Wall-clock latency: ≥10× speedup (latency_SNN ≤ 2.5 s, latency_XGBoost ≥ 25 s) with 100 repetitions and assumed σ ≈ 0.5 s per method (t-test power ≥0.90 for difference of 22.5 s). Sensitivity: maintain ≥95% with 200 test samples; single-proportion z-test requires n=200 for 95% CI around p=0.95. Energy: ≥5× reduction on neuromorphic chip (0.2 mJ vs. 1.0 mJ); ordinal comparison on hardware energy model.",
        "statistical_power_notes": "Latency: n=100 paired observations (100 repeated inferences on fixed test set), assumed σ=0.5 s per condition, target effect size Δ=22.5 s, power=0.90, α=0.05 (one-tailed). This detects a 22.5-s difference with <0.01 probability of Type II error. Sensitivity: n=200 test samples, target p=0.95 with 95% CI [0.92, 0.98], sufficient to reject H₀: p=0.90 (one-proportion z-test, power≈0.85). Energy: semi-quantitative via Loihi-2 datasheet (pJ/spike × spike count); convergence on neuromorphic simulator requires ≥5 runs with stable spike count estimates (coefficient of variation <5%).",
        "limitations": [
          "Event sequence construction uses synthetic timestamps inferred from face lattice discovery order; real polytope scanning order may vary, affecting temporal validity.",
          "SNN training relies on surrogate gradients, which approximate true spiking gradients; convergence may be slower and accuracy ceiling lower than standard ANNs.",
          "Neuromorphic energy estimates are based on Loihi-2 datasheets; actual hardware deployment (if available) may differ due to routing overhead and spike traffic variability.",
          "Test set is modest (n=200); generalization to all 195k+ unexplored KS polytopes is unvalidated.",
          "Baseline comparison (XGBoost/MLP) uses standard CPUs; fairness comparison requires matching computational substrate (e.g., GPU for ANNs, or energy-normalized per-operation metrics).",
          "No wet-lab validation of h⁰ predictions; assumes CyTools/Macaulay2 ground truth is correct."
        ],
        "requires_followup": "If computational SNN benchmark is successful (≥10× latency, ≥95% sensitivity), next step is deployment on physical neuromorphic hardware (Intel Loihi-2 chip via INRC program, or SpiNNaker2) to validate energy claims and measure actual wall-clock latency under real chip constraints. Additionally, extend to full h⁰ vector (h⁰, h¹, h², h³) and Hodge numbers (h¹¹, h²¹) to broaden applicability beyond binary screening. Final integration: embed SNN screener in live CyTools database pipeline to validate false-negative rate on newly discovered polytopes."
      },
      "keywords": [
        "spiking neural networks",
        "asynchronous event processing",
        "neuromorphic hardware",
        "Calabi-Yau topological invariants",
        "polytope machine learning",
        "low-power screening"
      ],
      "gap_similarity": 0.6298991441726685,
      "gap_distance": 5,
      "approved": null,
      "critic_novelty": 2,
      "critic_rigor": 3,
      "critic_impact": 2,
      "critic_note": "The hypothesis applies well-established neuromorphic acceleration principles (event-driven SNNs, sparse operations) to a niche domain (Calabi-Yau screening); the mechanism is standard and the combinatorial feature encoding is underspecified, making this primarily an engineering application rather th",
      "composite_score": 4.0,
      "final_score": 3.125
    }
  ]
}