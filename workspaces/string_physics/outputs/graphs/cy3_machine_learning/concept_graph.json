{
  "domain": "cy3_machine_learning",
  "query": "machine learning Calabi-Yau cohomology neural network Hodge numbers Kreuzer-Skarke database reflexive polytopes graph neural network toric geometry line bundle cohomology reinforcement learning string vacua topological machine learning period integrals Yukawa couplings\n",
  "concepts": [
    {
      "label": "Official Statistics",
      "description": "Authoritative statistical data and analysis produced by government agencies for policy-making and public discourse.",
      "aliases": [
        "official statistical reporting"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5015,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Data Source Change",
      "description": "Transition or substitution of input data sources used in statistical production pipelines.",
      "aliases": [
        "changing data sources",
        "data source switching"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0742,
      "burt_constraint": 0.1256,
      "effective_size": 8.0,
      "structural_hole": true
    },
    {
      "label": "Machine Learning",
      "description": "Computational techniques for automated learning from data to perform prediction, classification, or analysis tasks.",
      "aliases": [
        "ML",
        "machine learning models"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Concept Drift",
      "description": "Temporal change in the statistical properties or relationships of data distributions over time.",
      "aliases": [
        "drift",
        "distributional shift"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5037,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Data Quality",
      "description": "Degree to which data sources meet standards of accuracy, reliability, completeness, and validity for statistical analysis.",
      "aliases": [
        "data accuracy",
        "data integrity",
        "data preparation",
        "data quality"
      ],
      "source_papers": [
        "2306.04338v1",
        "2006.16189v4"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Bias",
      "description": "Systematic error or skew in data or models that deviates from ground truth or true population parameters.",
      "aliases": [
        "statistical bias",
        "systematic error"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Robustness",
      "description": "Ability of statistical techniques and data sourcing pipelines to maintain integrity and reliability under perturbation or change.",
      "aliases": [
        "model robustness",
        "system robustness",
        "model stability",
        "generalization robustness"
      ],
      "source_papers": [
        "2306.04338v1",
        "2404.12511v1"
      ],
      "centrality": 0.0791,
      "burt_constraint": 0.5012,
      "effective_size": 2.3968,
      "structural_hole": false
    },
    {
      "label": "Monitoring",
      "description": "Continuous surveillance and assessment of data sources, model performance, and statistical outputs for anomalies or degradation.",
      "aliases": [
        "performance monitoring",
        "data monitoring"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Statistical Validity",
      "description": "Property that a statistical measure or inference correctly represents or measures the intended concept or population.",
      "aliases": [
        "validity",
        "measurement validity"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Data Availability",
      "description": "Accessibility, coverage, and timeliness of data sources required for statistical production.",
      "aliases": [
        "data accessibility"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Ethical Governance",
      "description": "Framework of principles and regulations ensuring ethical conduct, transparency, and accountability in data and statistical practices.",
      "aliases": [
        "ethics",
        "regulatory compliance"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Public Perception",
      "description": "Societal trust, confidence, and understanding of the credibility and relevance of official statistics.",
      "aliases": [
        "public trust",
        "statistical credibility"
      ],
      "source_papers": [
        "2306.04338v1"
      ],
      "centrality": 0.0068,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "supervised machine learning",
      "description": "A class of machine learning algorithms trained on labeled data to predict or classify outcomes, with direct application to biological and physical prediction tasks.",
      "aliases": [
        "supervised learning",
        "supervised ML",
        "supervised training"
      ],
      "source_papers": [
        "2006.16189v4",
        "2201.12150v2",
        "1609.04846v1",
        "1909.10086v3"
      ],
      "centrality": 0.1623,
      "burt_constraint": 0.1673,
      "effective_size": 6.0,
      "structural_hole": true
    },
    {
      "label": "machine learning validation",
      "description": "The systematic process of assessing the performance, generalizability, and limitations of trained ML models through rigorous evaluation protocols.",
      "aliases": [
        "model validation",
        "ML assessment"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0818,
      "burt_constraint": 0.2002,
      "effective_size": 5.0,
      "structural_hole": true
    },
    {
      "label": "DOME framework",
      "description": "A structured methodology for documenting machine learning pipelines via four core components: Data, Optimization, Model, and Evaluation.",
      "aliases": [
        "DOME"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0159,
      "burt_constraint": 0.5015,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "model performance",
      "description": "Quantitative and qualitative measures of how well a trained model predicts or classifies outcomes on held-out test data.",
      "aliases": [
        "predictive performance",
        "model accuracy",
        "forecasting accuracy",
        "predictive accuracy",
        "generalization error",
        "algorithm performance",
        "accuracy",
        "test performance",
        "inference accuracy",
        "classification accuracy",
        "detection performance",
        "prediction accuracy"
      ],
      "source_papers": [
        "2006.16189v4",
        "2502.01654v1",
        "2302.08893v4",
        "2201.12150v2",
        "1811.04380v1",
        "2004.02396v1",
        "1905.04749v2",
        "2404.12511v1"
      ],
      "centrality": 0.5022,
      "burt_constraint": 0.0836,
      "effective_size": 13.0,
      "structural_hole": true
    },
    {
      "label": "model generalization",
      "description": "The ability of a trained model to perform accurately on unseen data from the same or related distributions, avoiding overfitting.",
      "aliases": [
        "generalizability",
        "out-of-sample performance"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "evaluation metrics",
      "description": "Standardized numerical measures (e.g., accuracy, precision, recall, AUC) used to quantify model performance on validation and test sets.",
      "aliases": [
        "performance metrics",
        "validation metrics"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "model limitations",
      "description": "Systematic characterization of failure modes, data dependencies, and contexts where a trained model is unreliable or inapplicable.",
      "aliases": [
        "model failure modes",
        "limitations assessment"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "biological application",
      "description": "Use of machine learning methods to solve predictive, classification, or optimization problems in biological and biomedical domains.",
      "aliases": [
        "biology ML",
        "biomedical prediction"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "hyperparameter optimization",
      "description": "Systematic tuning of algorithm metaparameters (learning rate, regularization, tree depth) to maximize model performance on validation data.",
      "aliases": [
        "hyperparameter tuning",
        "algorithm optimization",
        "hyperparameter settings"
      ],
      "source_papers": [
        "2006.16189v4",
        "2201.12150v2",
        "1905.07435v1"
      ],
      "centrality": 0.2449,
      "burt_constraint": 0.3751,
      "effective_size": 3.0,
      "structural_hole": true
    },
    {
      "label": "reproducibility",
      "description": "The ability to independently recreate or verify the results, predictions, and conclusions of a machine learning study from documented methods and code.",
      "aliases": [
        "computational reproducibility",
        "result verification",
        "replicability"
      ],
      "source_papers": [
        "2006.16189v4",
        "1912.03905v2"
      ],
      "centrality": 0.0145,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "community standards",
      "description": "Agreed-upon best practices, methodological guidelines, and documentation protocols adopted across a research community to ensure consistency and quality.",
      "aliases": [
        "best practices",
        "methodological standards"
      ],
      "source_papers": [
        "2006.16189v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "air pollution prediction",
      "description": "Task of forecasting future concentrations of atmospheric pollutants using machine learning and meteorological data.",
      "aliases": [
        "air pollutant concentration prediction",
        "AP prediction"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "LSTM recurrent neural network",
      "description": "Long short-term memory architecture capturing temporal dependencies in sequential time-series data for pollution forecasting.",
      "aliases": [
        "LSTM RNN",
        "recurrent neural network"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.044,
      "burt_constraint": 0.251,
      "effective_size": 4.0,
      "structural_hole": false
    },
    {
      "label": "transfer learning",
      "description": "Machine learning technique that reuses knowledge from data-rich stations to improve prediction at stations with sparse observations.",
      "aliases": [
        "domain adaptation"
      ],
      "source_papers": [
        "2502.01654v1",
        "1909.10086v3"
      ],
      "centrality": 0.0391,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "air quality monitoring station",
      "description": "Ground-based sensor network collecting temporal measurements of air pollutant concentrations and meteorological variables.",
      "aliases": [
        "AQMS",
        "monitoring station"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "air pollutant concentration",
      "description": "Measured quantity of atmospheric pollutants (e.g., PM, NO₂, SO₂) at a given location and time.",
      "aliases": [
        "APS",
        "pollutant level"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "meteorological data",
      "description": "Auxiliary atmospheric measurements (temperature, humidity, wind speed, pressure) serving as predictive features for pollution forecasting.",
      "aliases": [
        "weather data",
        "meteorological variables"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.502,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "pre-trained neural network",
      "description": "Neural network weights initialized from training on data-rich source domain before fine-tuning on sparse target domain.",
      "aliases": [
        "pre-trained model",
        "transfer learning model",
        "foundation model"
      ],
      "source_papers": [
        "2502.01654v1",
        "1905.10345v1"
      ],
      "centrality": 0.0228,
      "burt_constraint": 0.439,
      "effective_size": 2.3269,
      "structural_hole": false
    },
    {
      "label": "temporal dependency",
      "description": "Statistical relationship between past and future pollutant concentrations, captured by recurrent architectures.",
      "aliases": [
        "time-series correlation",
        "sequential pattern"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0068,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "data sparsity",
      "description": "Condition where air quality monitoring stations have incomplete or limited observations for certain pollutants or time periods.",
      "aliases": [
        "sparse observations",
        "incomplete data"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "knowledge transfer",
      "description": "Process of leveraging learned representations from data-abundant source stations to improve generalization at target stations with limited data.",
      "aliases": [
        "cross-station transfer"
      ],
      "source_papers": [
        "2502.01654v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Active learning",
      "description": "Machine learning paradigm that selectively queries the most informative unlabeled instances to minimize labeling cost while maximizing model performance.",
      "aliases": [
        "selective sampling",
        "query learning"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.2072,
      "burt_constraint": 0.215,
      "effective_size": 5.7014,
      "structural_hole": true
    },
    {
      "label": "Data stream",
      "description": "Continuous sequence of data points arriving sequentially over time, typically in unbounded quantities and high velocity.",
      "aliases": [
        "streaming data",
        "online data"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Online active learning",
      "description": "Active learning applied to data streams where selection and labeling decisions are made in real-time as observations arrive.",
      "aliases": [
        "stream-based active learning"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.3528,
      "effective_size": 3.5264,
      "structural_hole": false
    },
    {
      "label": "Pool-based active learning",
      "description": "Active learning strategy that selects informative instances from a finite, closed pool of unlabeled data.",
      "aliases": [
        "batch active learning"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Query strategy",
      "description": "Algorithm or heuristic used to select which unlabeled instances are most informative for labeling.",
      "aliases": [
        "selection criterion",
        "sampling strategy"
      ],
      "source_papers": [
        "2302.08893v4",
        "2107.02363v4"
      ],
      "centrality": 0.1675,
      "burt_constraint": 0.2509,
      "effective_size": 4.0,
      "structural_hole": true
    },
    {
      "label": "Labeling cost",
      "description": "Resource expense (time, money, human effort) associated with obtaining ground truth labels for data instances.",
      "aliases": [
        "annotation cost",
        "labeling expense"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.7419,
      "effective_size": 1.05,
      "structural_hole": false
    },
    {
      "label": "Informativeness",
      "description": "Measure of how much an unlabeled instance reduces model uncertainty or improves predictive performance when labeled.",
      "aliases": [
        "information value",
        "query utility"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Unlabeled data",
      "description": "Instances without ground truth labels, typically abundant and inexpensive to collect.",
      "aliases": [
        "unannotated data"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Real-world applications",
      "description": "Practical deployment scenarios of machine learning systems in production environments outside controlled settings.",
      "aliases": [
        "practical implementations"
      ],
      "source_papers": [
        "2302.08893v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Learning curves",
      "description": "Graphical representations showing the performance of a learning algorithm as a function of a resource such as training set size or iteration count.",
      "aliases": [
        "performance curves",
        "training curves"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0746,
      "burt_constraint": 0.167,
      "effective_size": 6.0,
      "structural_hole": true
    },
    {
      "label": "Algorithm selection",
      "description": "Process of choosing the most suitable machine learning algorithm for a given task based on performance criteria and resource constraints.",
      "aliases": [
        "model selection",
        "algorithm choice",
        "algorithm selection"
      ],
      "source_papers": [
        "2201.12150v2",
        "2404.12511v1",
        "1812.09225v4"
      ],
      "centrality": 0.0846,
      "burt_constraint": 0.251,
      "effective_size": 4.0,
      "structural_hole": true
    },
    {
      "label": "Data acquisition",
      "description": "Process of gathering and collecting training examples to build machine learning models.",
      "aliases": [
        "data collection",
        "sample gathering"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Early stopping",
      "description": "Regularization technique that terminates model training when performance on a validation set stops improving.",
      "aliases": [
        "stopping criterion"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Learning curve models",
      "description": "Mathematical or statistical models that predict algorithm performance as a function of available resources.",
      "aliases": [
        "curve prediction models",
        "performance models"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Training set size",
      "description": "Number of labeled examples used to train a machine learning model; a key resource variable affecting performance.",
      "aliases": [
        "sample size",
        "dataset size"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Reference performance",
      "description": "Baseline or target performance level against which a learning algorithm's performance is compared in decision-making contexts.",
      "aliases": [
        "baseline performance",
        "performance threshold"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Decision making",
      "description": "Process of making choices regarding algorithm selection, resource allocation, or training termination based on performance predictions.",
      "aliases": [
        "decision support"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Resource allocation",
      "description": "Assignment of computational and data resources (e.g., training examples, budget) to optimize algorithm performance.",
      "aliases": [
        "budget allocation",
        "resource management"
      ],
      "source_papers": [
        "2201.12150v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Reinforcement learning",
      "description": "Machine learning paradigm where agents learn to maximize cumulative reward through interaction with an environment via trial and error.",
      "aliases": [
        "RL",
        "reinforcement learning agents"
      ],
      "source_papers": [
        "1705.05172v1",
        "1905.10345v1"
      ],
      "centrality": 0.0863,
      "burt_constraint": 0.2214,
      "effective_size": 5.0,
      "structural_hole": true
    },
    {
      "label": "Computational emotion models",
      "description": "Formal algorithmic representations of emotional states and processes that can be implemented in artificial agents and robots.",
      "aliases": [
        "emotion models",
        "affective computing models"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0612,
      "burt_constraint": 0.1115,
      "effective_size": 9.0,
      "structural_hole": true
    },
    {
      "label": "Decision-making architecture",
      "description": "The underlying computational framework governing how an agent selects and executes actions in response to environmental states.",
      "aliases": [
        "agent architecture"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Homeostasis",
      "description": "A regulatory mechanism maintaining system equilibrium; used as a foundational dimension for deriving computational emotions in agents.",
      "aliases": [
        "homeostatic balance"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Appraisal theory",
      "description": "Emotion theory positing that emotional responses arise from cognitive evaluation of situations relative to agent goals and concerns.",
      "aliases": [
        "appraisal",
        "cognitive appraisal"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Intrinsic motivation",
      "description": "Internal drive to pursue goals for inherent satisfaction rather than external reward, important for exploration in RL agents.",
      "aliases": [
        "intrinsic drives"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Model-based reinforcement learning",
      "description": "RL approach where agents build and use internal models of environment dynamics to plan future trajectories.",
      "aliases": [
        "model-based RL",
        "planning",
        "planning-based RL"
      ],
      "source_papers": [
        "1705.05172v1",
        "1905.10345v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Action selection",
      "description": "The process by which an agent chooses which action to execute given its current state and learned policy.",
      "aliases": [
        "policy selection",
        "action choice"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Human-robot interaction",
      "description": "Field studying communication and collaborative dynamics between humans and robotic agents, often mediated by affective signals.",
      "aliases": [
        "HRI",
        "human-agent interaction"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Social signals",
      "description": "Communicative cues expressed by an agent to convey internal state information to human observers and influence their behavior.",
      "aliases": [
        "affective signals",
        "emotional expressions"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Learning efficiency",
      "description": "Rate and effectiveness with which an agent acquires useful behavioral policies and knowledge from experience.",
      "aliases": [
        "learning speed",
        "sample efficiency"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Motivation",
      "description": "Internal drive or goal-directedness influencing agent behavior; can arise from both intrinsic and extrinsic sources.",
      "aliases": [
        "motivational state"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Affective modelling",
      "description": "Research discipline focused on formalizing and validating emotion theories through computational implementations.",
      "aliases": [
        "affective science",
        "computational affect"
      ],
      "source_papers": [
        "1705.05172v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Few-shot learning",
      "description": "Machine learning paradigm for training on novel tasks with minimal labeled examples, typically 1–5 samples per class.",
      "aliases": [
        "low-shot learning",
        "k-shot learning"
      ],
      "source_papers": [
        "1905.06549v2",
        "1905.07435v1"
      ],
      "centrality": 0.144,
      "burt_constraint": 0.3339,
      "effective_size": 3.0,
      "structural_hole": true
    },
    {
      "label": "Meta-learning",
      "description": "Training strategy that learns to learn across diverse tasks, enabling rapid adaptation to new tasks with few examples.",
      "aliases": [
        "learning to learn"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Task-adaptive projection",
      "description": "Episode-specific linear transformation of learned feature embeddings to condition the model on task characteristics.",
      "aliases": [
        "task-conditional projection"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.1213,
      "burt_constraint": 0.5597,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Embedding space",
      "description": "High-dimensional learned feature representation where data points are mapped for distance-based classification.",
      "aliases": [
        "feature space",
        "representation space",
        "vector representation",
        "embedding space"
      ],
      "source_papers": [
        "1905.06549v2",
        "1812.09225v4"
      ],
      "centrality": 0.1155,
      "burt_constraint": 0.3823,
      "effective_size": 3.0,
      "structural_hole": true
    },
    {
      "label": "Reference vectors",
      "description": "Per-class prototype or centroid vectors learned during meta-training to serve as class representatives in the embedding space.",
      "aliases": [
        "class prototypes",
        "support prototypes"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Episode-based training",
      "description": "Training protocol where each iteration simulates a few-shot task with support and query sets, mimicking test-time conditions.",
      "aliases": [
        "episodic training"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Distance metric",
      "description": "Learned or fixed similarity measure (e.g. Euclidean, cosine) used to compare query embeddings against reference vectors.",
      "aliases": [
        "similarity metric",
        "metric learning"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Query set",
      "description": "Unlabeled or test examples within an episode used to evaluate classification performance after task adaptation.",
      "aliases": [
        "test set",
        "query examples"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Support set",
      "description": "Small labeled training set provided at test time for a novel task, used to define reference vectors and task structure.",
      "aliases": [
        "few-shot examples",
        "support examples"
      ],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Omniglot dataset",
      "description": "Benchmark dataset of 1,623 handwritten character classes from 50 alphabets, widely used for few-shot learning evaluation.",
      "aliases": [
        "Omniglot"
      ],
      "source_papers": [
        "1905.06549v2",
        "1905.07435v1"
      ],
      "centrality": 0.0002,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "miniImageNet dataset",
      "description": "Few-shot learning benchmark derived from ImageNet with 100 classes and 600 images per class, split into 64 train / 16 val / 20 test classes.",
      "aliases": [],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "tieredImageNet dataset",
      "description": "Large-scale few-shot learning benchmark with 608 ImageNet classes grouped into 34 higher-level categories for improved train-test distribution similarity.",
      "aliases": [],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Generalization",
      "description": "Model's ability to achieve high accuracy on novel tasks and datasets not seen during training.",
      "aliases": [
        "out-of-distribution generalization",
        "generalization"
      ],
      "source_papers": [
        "1905.06549v2",
        "2301.08028v4"
      ],
      "centrality": 0.0019,
      "burt_constraint": 0.5017,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "TapNet",
      "description": "Neural network architecture augmented with task-adaptive projection layers for improved few-shot learning through episode-based meta-training.",
      "aliases": [],
      "source_papers": [
        "1905.06549v2"
      ],
      "centrality": 0.0155,
      "burt_constraint": 0.2009,
      "effective_size": 5.0,
      "structural_hole": false
    },
    {
      "label": "ResNet architecture",
      "description": "Deep residual neural network with skip connections enabling training of very deep networks for image classification.",
      "aliases": [
        "ResNet",
        "residual network"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "dynamic routing",
      "description": "Mechanism that selectively activates computational units based on input characteristics rather than applying fixed transformations.",
      "aliases": [
        "dynamic selection",
        "adaptive routing"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0607,
      "burt_constraint": 0.3909,
      "effective_size": 3.4857,
      "structural_hole": true
    },
    {
      "label": "computational unit",
      "description": "A learned transformation or layer module that can be selectively activated in the neural network.",
      "aliases": [
        "CU",
        "transformation unit"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.5394,
      "effective_size": 2.3455,
      "structural_hole": false
    },
    {
      "label": "image classification",
      "description": "Computer vision task of assigning an input image to one of several predefined semantic categories.",
      "aliases": [
        "image labeling",
        "visual recognition"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0274,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "input-dependent path",
      "description": "A sequence of activated computational units that varies based on the characteristics of the input data.",
      "aliases": [
        "adaptive path",
        "class-specific route"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5017,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "learned transformations",
      "description": "A set of parameterized functions trained during neural network optimization to map input to output space.",
      "aliases": [
        "learned kernels",
        "trainable filters"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.8567,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "CIFAR-10 dataset",
      "description": "Benchmark dataset of 60,000 32×32 RGB images across 10 object classes widely used for image classification evaluation.",
      "aliases": [
        "CIFAR-10",
        "CIFAR-10.1"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "skip connections",
      "description": "Architectural element allowing gradients and information to bypass layers, enabling training of deeper networks.",
      "aliases": [
        "residual connections",
        "shortcuts"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "network depth",
      "description": "Number of sequential layers in a neural network; increased depth can improve representational capacity but complicates training.",
      "aliases": [
        "depth",
        "number of layers"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "class-specific learning",
      "description": "Phenomenon where a neural network learns differentiated representations or processing strategies for different semantic classes.",
      "aliases": [
        "class-dependent routing",
        "semantic differentiation"
      ],
      "source_papers": [
        "1811.04380v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Machine learning interpretability",
      "description": "Methods and frameworks for explaining which features and mechanisms drive decisions in ML models.",
      "aliases": [
        "explainability",
        "interpretable ML",
        "model transparency"
      ],
      "source_papers": [
        "2304.02381v2",
        "2404.12511v1"
      ],
      "centrality": 0.1454,
      "burt_constraint": 0.335,
      "effective_size": 3.0,
      "structural_hole": true
    },
    {
      "label": "Loss landscape",
      "description": "The high-dimensional energy surface defined by the loss function as a function of model parameters.",
      "aliases": [
        "error surface",
        "optimization landscape"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.015,
      "burt_constraint": 0.334,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Energy landscape methods",
      "description": "Physical sciences techniques for identifying conserved invariants and order parameters within minima of high-dimensional potential energy surfaces.",
      "aliases": [
        "landscape analysis",
        "minima characterization"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.034,
      "burt_constraint": 0.3342,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Conserved weights",
      "description": "Model parameters that remain invariant or highly similar across multiple minima of the loss landscape, indicating structural importance.",
      "aliases": [
        "weight invariants",
        "stable parameters"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0275,
      "burt_constraint": 0.2502,
      "effective_size": 4.0,
      "structural_hole": false
    },
    {
      "label": "Feature relevance",
      "description": "The degree to which input data features influence or drive model predictions and decision making.",
      "aliases": [
        "feature importance",
        "feature saliency"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5017,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Order parameters",
      "description": "Variables that characterize the essential collective behavior of a system, borrowed from molecular and condensed matter physics.",
      "aliases": [
        "collective variables",
        "coordinate invariants"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Model decision making",
      "description": "The computational process by which a trained ML model generates predictions from input data.",
      "aliases": [
        "prediction mechanism",
        "inference process"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5015,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Loss minima",
      "description": "Local or global optima of the loss function corresponding to trained model states.",
      "aliases": [
        "critical points",
        "stationary solutions"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Physics-inspired approach",
      "description": "Methodology that applies concepts and techniques from physical sciences to solve problems in machine learning.",
      "aliases": [
        "physics-ML analogy"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.1046,
      "burt_constraint": 0.253,
      "effective_size": 4.0,
      "structural_hole": true
    },
    {
      "label": "Model decision drivers",
      "description": "The underlying features, weights, or mechanisms that fundamentally cause or determine model outputs.",
      "aliases": [
        "decision factors",
        "causal mechanisms"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Input data features",
      "description": "Attributes or dimensions of the raw or processed data fed into a machine learning model.",
      "aliases": [
        "input variables",
        "features"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Synthetic examples",
      "description": "Artificially constructed test cases used to validate and demonstrate model interpretability methods.",
      "aliases": [
        "toy problems",
        "controlled experiments"
      ],
      "source_papers": [
        "2304.02381v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Real-world application",
      "description": "Practical deployment or demonstration of methods on empirical datasets from domains like medicine, cybersecurity, or autonomous driving.",
      "aliases": [
        "practical validation",
        "domain application",
        "clinical deployment",
        "real-world application"
      ],
      "source_papers": [
        "2304.02381v2",
        "2303.15563v1"
      ],
      "centrality": 0.0729,
      "burt_constraint": 0.502,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Random Neural Networks",
      "description": "A class of neural networks interpretable as queuing networks, with stochastic activation dynamics and trainable parameters for supervised and unsupervised learning.",
      "aliases": [
        "RNNs",
        "random neural systems"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0813,
      "burt_constraint": 0.1676,
      "effective_size": 6.0,
      "structural_hole": true
    },
    {
      "label": "Neural Networks",
      "description": "Computational models inspired by biological neurons, composed of interconnected nodes organized in layers with learnable weights and activation functions.",
      "aliases": [
        "NNs",
        "artificial neural networks",
        "deep learning models"
      ],
      "source_papers": [
        "1609.04846v1",
        "1905.10345v1"
      ],
      "centrality": 0.0561,
      "burt_constraint": 0.5069,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Queuing network",
      "description": "Stochastic system model consisting of service nodes with arrivals and departures, used to analyze resource sharing and performance.",
      "aliases": [
        "queueing network",
        "queuing theory model"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5014,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "First-order learning algorithms",
      "description": "Optimization techniques based on first-order derivatives (gradients) to update model parameters, such as gradient descent and variants.",
      "aliases": [
        "gradient-based methods"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.149,
      "effective_size": 1.0312,
      "structural_hole": false
    },
    {
      "label": "Second-order learning algorithms",
      "description": "Optimization techniques using second-order derivatives (Hessian matrix) to accelerate convergence and improve learning efficiency.",
      "aliases": [
        "Newton-type methods",
        "second-order optimization"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.1821,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Combinatorial optimization",
      "description": "Problem class seeking optimal solutions over discrete or combinatorial solution spaces; RNNs serve as heuristic solvers.",
      "aliases": [
        "discrete optimization"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Resource sharing",
      "description": "System property where multiple agents or jobs compete for limited computational, network, or physical resources.",
      "aliases": [],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Activation dynamics",
      "description": "Temporal evolution rules governing how neurons update their states in response to inputs and internal dynamics.",
      "aliases": [
        "firing dynamics",
        "neural dynamics"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Parameter optimization",
      "description": "Process of adjusting learnable weights and biases in a neural network to minimize a loss function on training data.",
      "aliases": [
        "weight training",
        "model fitting"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0433,
      "effective_size": 1.2,
      "structural_hole": false
    },
    {
      "label": "Learning procedures",
      "description": "Systematic algorithms and methodologies for training neural networks using data, including backpropagation and variants.",
      "aliases": [
        "training algorithms"
      ],
      "source_papers": [
        "1609.04846v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Quantized Neural Network",
      "description": "A neural network with weights constrained to discrete values (e.g., powers of two) to enable efficient compression and hardware implementation.",
      "aliases": [
        "QNN",
        "quantization"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.334,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "n-bit Quantization",
      "description": "Constraint of neural network weights to n-bit precision, reducing model size and computational cost.",
      "aliases": [
        "bit-width quantization",
        "low-precision quantization"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Gradient Vanishing",
      "description": "Problem in backpropagation where gradients become negligibly small, preventing effective weight updates during training.",
      "aliases": [
        "vanishing gradient problem"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0051,
      "effective_size": 1.0286,
      "structural_hole": false
    },
    {
      "label": "Reconstructed Gradient Function",
      "description": "A novel gradient computation method for QNNs that directly computes real gradients rather than approximating expected loss gradients.",
      "aliases": [
        "gradient reconstruction"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.6232,
      "effective_size": 2.3333,
      "structural_hole": false
    },
    {
      "label": "n-BQ-NN",
      "description": "A proposed quantized neural network architecture that replaces multiply operations with shift operations for efficient FPGA inference.",
      "aliases": [
        "shift-based quantized network"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5638,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "FPGA Implementation",
      "description": "Deployment of neural networks on Field-Programmable Gate Arrays for hardware-accelerated inference.",
      "aliases": [
        "FPGA inference",
        "FPGA acceleration"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Shift Vector Processing Element",
      "description": "A hardware processing unit array that replaces multiplications with shift operations to enable efficient FPGA execution.",
      "aliases": [
        "SVPE",
        "SVPE array"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Shift Operation",
      "description": "Bitwise shift instruction that replaces costly multiplication, consuming no DSP resources on FPGAs.",
      "aliases": [
        "bit shift",
        "left/right shift"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Network Compression",
      "description": "Technique to reduce model size and computational requirements while maintaining prediction accuracy.",
      "aliases": [
        "model compression"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Backpropagation Algorithm",
      "description": "Training algorithm that computes gradients by reverse-mode differentiation to update network weights.",
      "aliases": [
        "backprop",
        "reverse-mode differentiation"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0309,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Digital Signal Processing Resource",
      "description": "Dedicated hardware resource on FPGAs for arithmetic operations; shift operations do not consume DSP blocks.",
      "aliases": [
        "DSP",
        "DSP block"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Energy Consumption",
      "description": "Power requirement for inference execution; optimized through shift-based operations and resource efficiency.",
      "aliases": [
        "energy efficiency",
        "power consumption"
      ],
      "source_papers": [
        "2004.02396v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "fake news detection",
      "description": "Machine learning task to classify news articles or content as authentic or fabricated based on textual and contextual features.",
      "aliases": [
        "misinformation detection",
        "disinformation classification"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0972,
      "burt_constraint": 0.1271,
      "effective_size": 9.7771,
      "structural_hole": true
    },
    {
      "label": "pre-trained language models",
      "description": "Deep learning models trained on large text corpora that encode semantic and syntactic knowledge, transferable to downstream tasks like fake news detection.",
      "aliases": [
        "PLMs",
        "transformer language models"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5106,
      "effective_size": 2.3148,
      "structural_hole": false
    },
    {
      "label": "BERT",
      "description": "Bidirectional Encoder Representations from Transformers; a pre-trained transformer model using masked language modeling that achieves state-of-the-art performance on NLP tasks.",
      "aliases": [
        "Bidirectional Encoder Representations from Transformers"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.7516,
      "effective_size": 1.0526,
      "structural_hole": false
    },
    {
      "label": "traditional machine learning",
      "description": "Classical supervised learning methods including logistic regression, random forests, and support vector machines for classification tasks.",
      "aliases": [
        "conventional ML",
        "shallow models"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "deep learning",
      "description": "Neural network-based approaches with multiple layers that learn hierarchical feature representations for complex pattern recognition.",
      "aliases": [
        "deep neural networks",
        "neural network models"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "benchmark study",
      "description": "Comparative empirical evaluation of multiple models or methods on standardized datasets to assess relative performance and generalization.",
      "aliases": [
        "benchmarking",
        "comparative analysis",
        "performance evaluation"
      ],
      "source_papers": [
        "1905.04749v2",
        "1912.03905v2"
      ],
      "centrality": 0.0342,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "dataset bias",
      "description": "Systematic limitation where a model trained on domain-specific data (e.g., political news only) fails to generalize to other domains or data distributions.",
      "aliases": [
        "domain bias",
        "distribution shift"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "news datasets",
      "description": "Collections of labeled news articles used to train and evaluate fake news detection models, varying in size, diversity, and topic coverage.",
      "aliases": [
        "news corpora",
        "labeled news data"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "article topic",
      "description": "Semantic category or subject matter of a news article (e.g., political, health, sports); a dimension along which model performance is analyzed.",
      "aliases": [
        "news category",
        "topic classification"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "article length",
      "description": "Quantitative feature measuring the number of words or characters in a news article; correlated with model performance and content characteristics.",
      "aliases": [
        "text length",
        "article size"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "social media propagation",
      "description": "Spread and sharing of news content across social networks; mechanism by which fake news achieves harmful reach and impact.",
      "aliases": [
        "viral spread",
        "information diffusion"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "low-resource languages",
      "description": "Languages with limited annotated training data and electronic content; pre-trained models show particular advantage due to transfer learning.",
      "aliases": [
        "limited data scenarios",
        "scarce-data languages"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "text classification",
      "description": "NLP task of assigning documents or articles to predefined categories; the underlying framework for fake news detection.",
      "aliases": [
        "document classification",
        "content classification"
      ],
      "source_papers": [
        "1905.04749v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Privacy-Preserving Machine Learning",
      "description": "Machine learning techniques that protect sensitive data privacy throughout the model lifecycle, from training to inference.",
      "aliases": [
        "PPML",
        "privacy-preserving ML"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0676,
      "burt_constraint": 0.2006,
      "effective_size": 5.0,
      "structural_hole": true
    },
    {
      "label": "Healthcare ML",
      "description": "Application of machine learning to healthcare prediction tasks including disease diagnosis, prognosis, and patient treatment.",
      "aliases": [
        "medical ML",
        "clinical ML"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0207,
      "burt_constraint": 0.2506,
      "effective_size": 4.0,
      "structural_hole": false
    },
    {
      "label": "Medical Data",
      "description": "Sensitive patient health information used for training and inference in healthcare machine learning systems.",
      "aliases": [
        "health data",
        "clinical data",
        "patient data"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Model Training",
      "description": "Process of fitting machine learning models to labeled data to learn predictive patterns.",
      "aliases": [
        "training",
        "learning phase"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Model Inference",
      "description": "Process of applying a trained machine learning model to make predictions on new data.",
      "aliases": [
        "inference",
        "prediction phase"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Inference-as-a-Service",
      "description": "Cloud-based service delivery model where pre-trained models provide predictions while protecting underlying data privacy.",
      "aliases": [
        "MLaaS",
        "inference service"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Data Privacy",
      "description": "Protection of sensitive information from unauthorized access, use, or disclosure throughout the ML pipeline.",
      "aliases": [
        "privacy protection",
        "data confidentiality"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.3335,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Disease Diagnosis",
      "description": "Healthcare ML task of identifying the presence and type of disease from patient data and medical tests.",
      "aliases": [
        "diagnosis prediction",
        "disease detection"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Disease Prognosis",
      "description": "Healthcare ML task of predicting the future course, severity, or outcome of a disease for individual patients.",
      "aliases": [
        "prognosis prediction",
        "outcome prediction"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Patient Treatment",
      "description": "Healthcare ML task of recommending or optimizing therapeutic interventions for patient care.",
      "aliases": [
        "treatment planning",
        "therapy recommendation"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "ML Pipeline",
      "description": "End-to-end workflow encompassing data collection, preprocessing, training, validation, and deployment of machine learning systems.",
      "aliases": [
        "ML workflow",
        "ML lifecycle"
      ],
      "source_papers": [
        "2303.15563v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "AutoML",
      "description": "Automatic machine learning systems that automatically design and optimize machine learning pipelines for given datasets and tasks.",
      "aliases": [
        "Automatic Machine Learning"
      ],
      "source_papers": [
        "1905.10345v1"
      ],
      "centrality": 0.211,
      "burt_constraint": 0.1523,
      "effective_size": 7.7463,
      "structural_hole": true
    },
    {
      "label": "Pipeline synthesis",
      "description": "The process of automatically constructing machine learning pipelines by combining and sequencing data preprocessing, feature engineering, and model selection components.",
      "aliases": [
        "pipeline construction",
        "pipeline generation"
      ],
      "source_papers": [
        "1905.10345v1"
      ],
      "centrality": 0.098,
      "burt_constraint": 0.3182,
      "effective_size": 3.527,
      "structural_hole": true
    },
    {
      "label": "Pipeline grammar",
      "description": "Formal specification of valid pipeline structures and component compositions that constrains the search space for AutoML.",
      "aliases": [
        "grammar-based search space"
      ],
      "source_papers": [
        "1905.10345v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Self-play",
      "description": "Training technique where an agent learns by playing against itself, generating diverse training signals and improving generalization.",
      "aliases": [],
      "source_papers": [
        "1905.10345v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Bayesian optimization",
      "description": "Hyperparameter optimization method using Gaussian processes or surrogate models to efficiently explore high-dimensional parameter spaces.",
      "aliases": [
        "BO",
        "information gain"
      ],
      "source_papers": [
        "1905.10345v1",
        "1906.01101v1"
      ],
      "centrality": 0.0601,
      "burt_constraint": 0.5022,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Evolutionary algorithms",
      "description": "Population-based optimization methods inspired by biological evolution that iteratively improve candidate solutions through selection, mutation, and recombination.",
      "aliases": [
        "genetic algorithms",
        "evolutionary computation"
      ],
      "source_papers": [
        "1905.10345v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Classification task",
      "description": "Supervised learning problem of predicting discrete class labels from input features.",
      "aliases": [
        "classification"
      ],
      "source_papers": [
        "1905.10345v1",
        "1905.07435v1"
      ],
      "centrality": 0.0857,
      "burt_constraint": 0.5017,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Regression task",
      "description": "Supervised learning problem of predicting continuous numerical values from input features.",
      "aliases": [
        "regression"
      ],
      "source_papers": [
        "1905.10345v1",
        "1905.07435v1"
      ],
      "centrality": 0.0857,
      "burt_constraint": 0.5017,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Benchmark dataset",
      "description": "Standardized dataset used to evaluate and compare the performance of different machine learning methods and systems.",
      "aliases": [
        "benchmark"
      ],
      "source_papers": [
        "1905.10345v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "meta-reinforcement learning",
      "description": "A machine learning approach that casts the development of better RL algorithms as a learning problem, enabling policies to adapt to new tasks from a task distribution with minimal data.",
      "aliases": [
        "meta-RL"
      ],
      "source_papers": [
        "2301.08028v4"
      ],
      "centrality": 0.0328,
      "burt_constraint": 0.2157,
      "effective_size": 5.6952,
      "structural_hole": false
    },
    {
      "label": "deep reinforcement learning",
      "description": "Machine learning approach combining deep neural networks with reinforcement learning to learn policies from high-dimensional observations through trial-and-error interaction.",
      "aliases": [
        "deep RL",
        "DRL"
      ],
      "source_papers": [
        "2301.08028v4",
        "1912.03905v2"
      ],
      "centrality": 0.0195,
      "burt_constraint": 0.3335,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "policy adaptation",
      "description": "The ability of a learned policy to quickly adjust its behavior to perform well on new tasks using limited data and experience.",
      "aliases": [
        "task adaptation"
      ],
      "source_papers": [
        "2301.08028v4",
        "1905.07435v1"
      ],
      "centrality": 0.0451,
      "burt_constraint": 0.3615,
      "effective_size": 3.5061,
      "structural_hole": false
    },
    {
      "label": "task distribution",
      "description": "A probabilistic distribution over a set of related tasks from which training and evaluation tasks are sampled in meta-learning settings.",
      "aliases": [],
      "source_papers": [
        "2301.08028v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.7503,
      "effective_size": 1.0271,
      "structural_hole": false
    },
    {
      "label": "data efficiency",
      "description": "The ability of a learning algorithm to achieve good performance with a small amount of training data or environmental interactions.",
      "aliases": [],
      "source_papers": [
        "2301.08028v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "learning budget",
      "description": "The amount of available computational or sample data resources allocated for learning on individual tasks or overall training.",
      "aliases": [],
      "source_papers": [
        "2301.08028v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "RL algorithm",
      "description": "A computational procedure designed to learn an optimal policy through interactions with an environment via reward signals.",
      "aliases": [],
      "source_papers": [
        "2301.08028v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Shannon entropy",
      "description": "Information-theoretic measure quantifying uncertainty in probability distributions; used to assess information content and disorder in datasets.",
      "aliases": [
        "information entropy",
        "Shannon information"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.0103,
      "burt_constraint": 0.334,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Rough set theory",
      "description": "Mathematical framework for handling incomplete and uncertain information by partitioning data into equivalence classes and computing lower/upper approximations.",
      "aliases": [
        "rough sets",
        "RST"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.0103,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Machine learning evaluation",
      "description": "Systematic assessment methodology for quantifying predictive performance, robustness, and generalization capacity of machine learning models.",
      "aliases": [
        "model evaluation",
        "performance assessment"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.2884,
      "burt_constraint": 0.1797,
      "effective_size": 6.7541,
      "structural_hole": true
    },
    {
      "label": "Data complexity",
      "description": "Intrinsic structural properties of datasets including dimensionality, noise, class overlap, and boundary characteristics affecting model learning difficulty.",
      "aliases": [
        "problem complexity",
        "dataset properties"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.7902,
      "effective_size": 1.0272,
      "structural_hole": false
    },
    {
      "label": "Uncertainty quantification",
      "description": "Formal methodology for characterizing and measuring epistemic and aleatoric uncertainty in model predictions and data.",
      "aliases": [
        "uncertainty estimation"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Granularity analysis",
      "description": "Examination of the resolution and scale at which data is partitioned and examined, affecting information loss and model discrimination.",
      "aliases": [
        "granularity",
        "partition refinement"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Information structure",
      "description": "Organizational patterns and dependencies inherent in data that reflect underlying relationships and dependencies among variables.",
      "aliases": [
        "data structure",
        "intrinsic structure"
      ],
      "source_papers": [
        "2404.12511v1"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "dendrogram",
      "description": "A tree-structured hierarchical representation of nested clusterings, where each level encodes agglomerative or divisive relationships between data points or clusters.",
      "aliases": [
        "hierarchical tree",
        "dendrographic representation"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0259,
      "burt_constraint": 0.2012,
      "effective_size": 5.0,
      "structural_hole": false
    },
    {
      "label": "single linkage clustering",
      "description": "A hierarchical clustering method that merges clusters based on the minimum distance between any pair of points in different clusters, naturally generating dendrograms.",
      "aliases": [
        "single-link criterion"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Minimax distance",
      "description": "A distance metric derived from dendrograms via single linkage, measuring dissimilarity through specific level and distance functions over the hierarchical structure.",
      "aliases": [
        "minimax metric",
        "bottleneck distance"
      ],
      "source_papers": [
        "1812.09225v4",
        "1904.13223v3"
      ],
      "centrality": 0.0745,
      "burt_constraint": 0.0837,
      "effective_size": 12.0,
      "structural_hole": true
    },
    {
      "label": "level function",
      "description": "A function that assigns heights or scales to nodes in a dendrogram, parameterizing the hierarchical structure and determining inter-point dissimilarity.",
      "aliases": [],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0002,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "distance function",
      "description": "A mathematical function defined over dendrogram nodes that quantifies dissimilarity; combined with a level function to generate distance metrics.",
      "aliases": [
        "metric function"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0002,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "unsupervised representation learning",
      "description": "The task of extracting meaningful feature vectors or embeddings from unlabeled data without explicit ground-truth supervision.",
      "aliases": [
        "unsupervised feature extraction"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0777,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": true
    },
    {
      "label": "ensemble clustering",
      "description": "A meta-approach that aggregates predictions or distance measures from multiple clustering solutions to obtain a more robust consensus.",
      "aliases": [
        "clustering ensemble",
        "consensus clustering"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.5017,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "correlation clustering",
      "description": "An optimization method that partitions objects by maximizing agreement with pairwise similarity/dissimilarity constraints, here applied to aggregation of ensemble predictions.",
      "aliases": [
        "correlative clustering"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "deep representations",
      "description": "Hierarchically composed feature transformations analogous to neural network layers, where multiple representations are stacked sequentially to refine features.",
      "aliases": [
        "multi-layered representations"
      ],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.502,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "graph-based aggregation",
      "description": "A method for combining multiple clustering solutions by constructing a weighted graph encoding consistency of labels across different models, resolved via graph optimization.",
      "aliases": [],
      "source_papers": [
        "1812.09225v4"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Network embedding",
      "description": "Euclidean vector representation of nodes in a network learned via stochastic optimization, enabling application of vector-based algorithms to graph-structured data.",
      "aliases": [
        "graph embedding",
        "node embedding",
        "graph representation learning",
        "manifold embedding"
      ],
      "source_papers": [
        "2107.02363v4",
        "2307.07881v2"
      ],
      "centrality": 0.1491,
      "burt_constraint": 0.1468,
      "effective_size": 7.7862,
      "structural_hole": true
    },
    {
      "label": "Stochastic gradient descent",
      "description": "Iterative optimization algorithm using noisy gradient estimates on subsampled data to train network embedding models at scale.",
      "aliases": [
        "SGD"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.1497,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Exchangeable graph",
      "description": "Random graph model where node labels are exchangeable, enabling asymptotic analysis of embedding distributions under permutation invariance.",
      "aliases": [
        "exchangeable random graph"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Asymptotic distribution",
      "description": "Limiting probability distribution of learned embedding vectors as network size approaches infinity, characterizing their asymptotic behavior and convergence rates.",
      "aliases": [
        "limiting distribution"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.041,
      "burt_constraint": 0.1672,
      "effective_size": 6.0,
      "structural_hole": false
    },
    {
      "label": "Fisher consistency",
      "description": "Statistical property ensuring that the learned embedding loss function recovers the true parameters in the limit of infinite data.",
      "aliases": [
        "consistency property"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Loss function",
      "description": "Objective function optimized during embedding training, whose choice affects both convergence properties and statistical consistency of learned representations.",
      "aliases": [
        "objective function"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Embedding dimension",
      "description": "Dimensionality of the Euclidean space into which network nodes are embedded; affects expressiveness and convergence rates of learned representations.",
      "aliases": [
        "latent dimension"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Node classification",
      "description": "Supervised learning task of predicting node labels using learned embeddings as input features.",
      "aliases": [],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Link prediction",
      "description": "Prediction task of inferring missing or future edges in a network using information from learned node embeddings.",
      "aliases": [],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Node clustering",
      "description": "Unsupervised task of grouping nodes into clusters based on proximity in the learned embedding space.",
      "aliases": [
        "graph clustering"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "node2vec",
      "description": "Popular network embedding method using biased random walks and skip-gram models, exemplifying subsampling-based embedding approaches.",
      "aliases": [],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Convergence rate",
      "description": "Quantitative measure of how quickly learned embedding distributions approach their asymptotic limits, expressed in terms of network size and hyperparameters.",
      "aliases": [],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Latent parameters",
      "description": "Unknown parameters of the generative graph model that determine the true underlying network structure and embedding properties.",
      "aliases": [
        "true parameters"
      ],
      "source_papers": [
        "2107.02363v4"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Minimum spanning tree",
      "description": "A spanning tree of a graph with minimum total edge weight, used to efficiently compute Minimax distances.",
      "aliases": [
        "MST"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Pairwise distance embedding",
      "description": "A technique that embeds pairwise distances into a vector space such that squared Euclidean distances in the new space preserve the original distance metrics.",
      "aliases": [
        "metric embedding",
        "distance embedding"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Eigenvalue decomposition",
      "description": "A linear algebra method that decomposes a matrix into eigenvalues and eigenvectors to extract principal components and features.",
      "aliases": [
        "spectral decomposition",
        "EVD"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "K-nearest neighbor search",
      "description": "A machine learning method that classifies or retrieves objects based on distances to the k nearest neighbors in a feature space.",
      "aliases": [
        "KNN",
        "k-NN search"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Outlier detection",
      "description": "A task in machine learning that identifies anomalous objects that deviate significantly from normal patterns in the data.",
      "aliases": [
        "anomaly detection",
        "outlier identification"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Feature extraction",
      "description": "The process of automatically learning or computing relevant features from raw data that capture underlying patterns and structures.",
      "aliases": [
        "representation learning",
        "feature learning"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Nonparametric methods",
      "description": "Machine learning and statistical approaches that make minimal assumptions about the underlying distribution of data, rather than assuming a fixed parametric form.",
      "aliases": [
        "distribution-free methods"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Base distance measure",
      "description": "A fundamental distance metric (such as Euclidean, Manhattan, or Hamming distance) used as input to compute higher-level distance constructs.",
      "aliases": [
        "underlying metric",
        "primitive distance"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Centered matrix",
      "description": "A distance or similarity matrix that has been mean-centered, typically by subtracting row and column means to remove bias for subsequent analysis.",
      "aliases": [
        "double-centered matrix",
        "mean-centered distance matrix"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Numerical data",
      "description": "Data represented as numerical vectors or arrays, in contrast to categorical, textual, or other non-numeric data types.",
      "aliases": [
        "vector data",
        "numerical feature space"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Graph representation",
      "description": "A representation of data objects and their relationships as nodes and edges in a graph structure.",
      "aliases": [
        "network representation"
      ],
      "source_papers": [
        "1904.13223v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Maximum entropy method",
      "description": "Probabilistic inference technique that constructs distributions satisfying moment constraints while maximizing entropy.",
      "aliases": [
        "MaxEnt",
        "MEMe"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0545,
      "burt_constraint": 0.1671,
      "effective_size": 7.7465,
      "structural_hole": true
    },
    {
      "label": "Moment constraints",
      "description": "Mathematical restrictions on statistical moments (mean, variance, etc.) used to define probability distributions.",
      "aliases": [
        "moment conditions"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5004,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Constrained Bayesian inference",
      "description": "Bayesian inference framework incorporating explicit constraints on the posterior distribution.",
      "aliases": [
        "constrained variational inference",
        "Bayesian optimization with constraints"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Log determinant estimation",
      "description": "Computational problem of efficiently approximating the logarithm of matrix determinants for large-scale problems.",
      "aliases": [
        "fast log-det approximation"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Variational inference",
      "description": "Approximate Bayesian inference method casting posterior inference as an optimization problem.",
      "aliases": [
        "variational methods"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Large-scale machine learning",
      "description": "Machine learning applications operating on datasets or models of massive scale requiring computational efficiency.",
      "aliases": [
        "scalable ML"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.873,
      "effective_size": 1.0757,
      "structural_hole": false
    },
    {
      "label": "Efficient approximation",
      "description": "Mathematical or algorithmic technique enabling fast computation of otherwise intractable quantities.",
      "aliases": [
        "efficient computation"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.873,
      "effective_size": 1.0757,
      "structural_hole": false
    },
    {
      "label": "Entropy maximization",
      "description": "Optimization objective selecting probability distributions with highest entropy subject to given constraints.",
      "aliases": [
        "entropy optimization"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Moment matching",
      "description": "Inference technique fitting probability distributions to match prescribed statistical moments.",
      "aliases": [
        "moment-based inference"
      ],
      "source_papers": [
        "1906.01101v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Graph neural network",
      "description": "Deep learning architecture that learns representations of graph-structured data by iteratively aggregating node features across neighborhoods.",
      "aliases": [
        "GNN"
      ],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.4848,
      "effective_size": 2.4155,
      "structural_hole": false
    },
    {
      "label": "Universal graph embedding",
      "description": "Task-independent graph representation learned across diverse datasets that can be transferred to new classification problems without retraining.",
      "aliases": [
        "DUGNN",
        "deep universal graph embedding"
      ],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0951,
      "burt_constraint": 0.1347,
      "effective_size": 8.8062,
      "structural_hole": true
    },
    {
      "label": "Graph kernel",
      "description": "Kernel method that measures similarity between graphs by comparing their structural properties, used as a multi-task decoder.",
      "aliases": [],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.7637,
      "effective_size": 1.05,
      "structural_hole": false
    },
    {
      "label": "Task-independent representation",
      "description": "Learned embedding that generalizes across multiple downstream tasks without task-specific retraining or fine-tuning.",
      "aliases": [
        "task-agnostic embedding"
      ],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Unsupervised learning",
      "description": "Learning paradigm where patterns are discovered in data without explicit labels, used to initialize universal embeddings.",
      "aliases": [],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Graph classification",
      "description": "Task of assigning categorical labels to entire graphs based on their structural and node features.",
      "aliases": [],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Multi-task learning",
      "description": "Machine learning approach that jointly trains a model on multiple related tasks to improve generalization and knowledge sharing.",
      "aliases": [],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Feature aggregation",
      "description": "Mechanism in graph neural networks that combines information from neighboring nodes to update node representations.",
      "aliases": [
        "neighborhood aggregation"
      ],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Graph encoder",
      "description": "Neural network component that transforms graph structure and node features into fixed-dimensional embeddings.",
      "aliases": [],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Graph decoder",
      "description": "Component that uses learned embeddings to make task-specific predictions, implemented via graph kernels.",
      "aliases": [],
      "source_papers": [
        "1909.10086v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "ChainerRL library",
      "description": "Open-source Python library implementing state-of-the-art deep reinforcement learning algorithms built on the Chainer deep learning framework.",
      "aliases": [
        "ChainerRL"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0497,
      "burt_constraint": 0.1435,
      "effective_size": 7.0,
      "structural_hole": false
    },
    {
      "label": "DRL algorithms",
      "description": "Specific reinforcement learning algorithms (e.g., DQN, A3C, DDPG, PPO) that combine neural network function approximation with temporal-difference or policy-gradient learning.",
      "aliases": [
        "deep RL algorithms",
        "RL methods"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0197,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Policy learning",
      "description": "Process of training an agent to learn an optimal action-selection strategy through interaction with an environment and reward signals.",
      "aliases": [
        "policy optimization",
        "policy training"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Neural network approximation",
      "description": "Use of deep neural networks as function approximators to represent value functions, policies, or Q-functions in learning systems.",
      "aliases": [
        "function approximation",
        "neural network regression"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Agent visualization",
      "description": "Tools and techniques for qualitative inspection and interpretation of trained agent behavior and learned representations.",
      "aliases": [
        "visualization tools",
        "agent analysis"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Chainer framework",
      "description": "Deep learning framework providing computational infrastructure for implementing neural network-based algorithms.",
      "aliases": [
        "Chainer",
        "deep learning framework"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Python implementation",
      "description": "Software realization of algorithms and libraries written in the Python programming language for accessibility and ease of use.",
      "aliases": [
        "Python library",
        "implementation"
      ],
      "source_papers": [
        "1912.03905v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Model-Agnostic Meta-Learning",
      "description": "A meta-learning algorithm that trains a model across multiple tasks to enable rapid adaptation to new tasks with minimal data.",
      "aliases": [
        "MAML"
      ],
      "source_papers": [
        "1905.07435v1"
      ],
      "centrality": 0.3211,
      "burt_constraint": 0.1443,
      "effective_size": 7.0,
      "structural_hole": true
    },
    {
      "label": "Meta-learning rate",
      "description": "Learning rate parameter governing the outer-loop optimization in meta-learning algorithms; controls task-level adaptation.",
      "aliases": [
        "outer learning rate",
        "inner learning rate",
        "Task learning rate"
      ],
      "source_papers": [
        "1905.07435v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Online hyperparameter adaptation",
      "description": "Scheme for automatically adjusting hyperparameters during training without manual tuning; adapts to data characteristics in real-time.",
      "aliases": [
        "adaptive hyperparameter selection"
      ],
      "source_papers": [
        "1905.07435v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5014,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Training stability",
      "description": "Property of a learning algorithm indicating consistent convergence and reduced sensitivity to initialization or parameter choices.",
      "aliases": [],
      "source_papers": [
        "1905.07435v1"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Alpha MAML",
      "description": "Extension of MAML incorporating online hyperparameter adaptation to eliminate manual tuning of meta-learning and task learning rates.",
      "aliases": [],
      "source_papers": [
        "1905.07435v1"
      ],
      "centrality": 0.0127,
      "burt_constraint": 0.3354,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "class imbalance learning",
      "description": "Machine learning task addressing the challenge of training classifiers on datasets where minority class samples are significantly underrepresented relative to majority classes.",
      "aliases": [
        "imbalanced classification",
        "class imbalance problem"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0138,
      "burt_constraint": 0.252,
      "effective_size": 4.0,
      "structural_hole": false
    },
    {
      "label": "random vector functional link network",
      "description": "A feedforward neural network architecture with random hidden layer weights and direct input-to-output connections, offering fast training and good generalization.",
      "aliases": [
        "RVFL",
        "random vector functional link"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0205,
      "burt_constraint": 0.5062,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "intuitionistic fuzzy sets",
      "description": "Generalization of fuzzy sets that explicitly model both membership and non-membership degrees, enabling representation of uncertainty and imprecision.",
      "aliases": [
        "intuitionistic fuzzy logic",
        "Atanassov fuzzy sets"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0069,
      "burt_constraint": 0.5003,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "weighting mechanism",
      "description": "Training procedure that assigns differential importance weights to samples or classes to counteract class imbalance and reduce majority class dominance.",
      "aliases": [
        "class weighting",
        "sample weighting",
        "cost-sensitive learning"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5014,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "GE-IFRVFL-CIL model",
      "description": "Proposed neural network architecture integrating graph embedding, intuitionistic fuzzy theory, and RVFL with weighting for imbalanced classification.",
      "aliases": [
        "graph embedded intuitionistic fuzzy RVFL"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0675,
      "burt_constraint": 0.1463,
      "effective_size": 7.8095,
      "structural_hole": true
    },
    {
      "label": "topological structure preservation",
      "description": "Property of maintaining the local and global geometric relationships and neighbourhood patterns of the original data in the learned representation.",
      "aliases": [
        "manifold structure preservation",
        "geometric structure conservation"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "uncertainty handling",
      "description": "Methodology for managing imprecision, vagueness, and ambiguity in data through mathematical frameworks like fuzzy logic.",
      "aliases": [
        "imprecision management",
        "fuzzy representation"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "KEEL benchmark datasets",
      "description": "Standard collection of imbalanced classification datasets used for evaluating and comparing machine learning algorithms.",
      "aliases": [
        "KEEL imbalanced datasets",
        "benchmark datasets"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "ADNI dataset",
      "description": "Real-world medical imaging dataset (Alzheimer's Disease Neuroimaging Initiative) used for evaluating model performance on practical applications.",
      "aliases": [
        "Alzheimer's Disease Neuroimaging Initiative"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "noise robustness",
      "description": "Capability of a model to maintain performance when data is corrupted by random perturbations such as Gaussian noise.",
      "aliases": [
        "noise tolerance",
        "robustness to noise"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.6498,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "generalization performance",
      "description": "Ability of a trained model to accurately predict on unseen data, reflecting its capacity to learn underlying patterns rather than memorizing training data.",
      "aliases": [
        "generalization error",
        "test performance"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "minority class",
      "description": "The underrepresented class or classes in an imbalanced dataset that comprise a small proportion of training samples.",
      "aliases": [
        "rare class",
        "positive class"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "majority class",
      "description": "The overrepresented class in an imbalanced dataset that comprises the majority of training samples and tends to dominate model training.",
      "aliases": [
        "negative class",
        "dominant class"
      ],
      "source_papers": [
        "2307.07881v2"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Event-based sensors",
      "description": "Neuromorphic sensors that asynchronously report pixel-level changes in intensity, generating sparse spatiotemporal data.",
      "aliases": [
        "dynamic vision sensors",
        "DVS"
      ],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "ALERT module",
      "description": "Asynchronous embedding layer based on PointNet that continuously integrates new events and dismisses old ones via a leakage mechanism.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0005,
      "burt_constraint": 0.2504,
      "effective_size": 4.0,
      "structural_hole": false
    },
    {
      "label": "PointNet embedding",
      "description": "Deep learning architecture for direct point cloud processing without voxelization or grid conversion.",
      "aliases": [
        "PointNet"
      ],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Leakage mechanism",
      "description": "Temporal decay process that allows ALERT to dismiss old events and adapt to continuous event streams.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Vision Transformer",
      "description": "Transformer-based architecture that processes image patches to achieve state-of-the-art vision performance.",
      "aliases": [
        "ViT"
      ],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Patch-based sparsity",
      "description": "Optimization strategy that exploits input sparsity by partitioning event data into patches for efficient processing.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Asynchronous processing",
      "description": "Event-driven computation pipeline that responds to individual sensor events without fixed temporal synchronization.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 0.5062,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Synchronous readout",
      "description": "Flexible mechanism that extracts up-to-date features at arbitrary sampling rates from asynchronous embeddings.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0004,
      "burt_constraint": 0.3339,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Transformer model",
      "description": "Attention-based neural network architecture for sequence processing applied to downstream recognition tasks.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0004,
      "burt_constraint": 0.3336,
      "effective_size": 3.0,
      "structural_hole": false
    },
    {
      "label": "Object recognition",
      "description": "Computer vision task of detecting and classifying objects in visual input data.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Gesture recognition",
      "description": "Machine learning task of classifying human gestures from visual or spatiotemporal sensor input.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0,
      "burt_constraint": 1.0,
      "effective_size": 1.0,
      "structural_hole": false
    },
    {
      "label": "Latency",
      "description": "End-to-end processing delay from sensor input to model output.",
      "aliases": [
        "inference latency"
      ],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5005,
      "effective_size": 2.0,
      "structural_hole": false
    },
    {
      "label": "Sparse spatiotemporal data",
      "description": "High-dimensional data with few non-zero values distributed across space and time dimensions.",
      "aliases": [],
      "source_papers": [
        "2402.01393v3"
      ],
      "centrality": 0.0001,
      "burt_constraint": 0.5033,
      "effective_size": 2.0,
      "structural_hole": false
    }
  ],
  "edges": [
    {
      "source": "Data Source Change",
      "target": "Concept Drift",
      "relation": "causes",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Changes in data sources are inevitable and pose significant risks; technical effects include concept drift (abstract, section on repercussions).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Data Source Change",
      "target": "Data Quality",
      "relation": "threatens",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The quality and integrity of data-science-driven statistics rely on accuracy and reliability of data sources; changing sources pose risks (abstract).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Data Source Change",
      "target": "Bias",
      "relation": "produces",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Technical repercussions of changing data sources include bias (abstract, section on repercussions).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Machine Learning",
      "target": "Official Statistics",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Data science has become increasingly essential for production of official statistics; ML enables automated collection, processing, and analysis of large amounts of data (abstract).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Data Source Change",
      "target": "Data Availability",
      "relation": "affects",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Technical effects of changing data sources include availability (abstract, section on repercussions).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Data Quality",
      "target": "Machine Learning",
      "relation": "constrains",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The accuracy and reliability of data sources support machine learning techniques in official statistics; quality is crucial (abstract).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Robustness",
      "target": "Data Source Change",
      "relation": "mitigates",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Precautionary measures include enhancing robustness in both data sourcing and statistical techniques to maintain integrity under change (abstract, conclusion).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Monitoring",
      "target": "Concept Drift",
      "relation": "detects",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Thorough monitoring is offered as a precautionary measure to address repercussions of changing data sources (abstract, conclusion).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Data Source Change",
      "target": "Statistical Validity",
      "relation": "threatens",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Technical repercussions of changing data sources include validity (abstract, section on repercussions).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Ethical Governance",
      "target": "Data Source Change",
      "relation": "regulates",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Origins and causes of changing data sources span ownership, ethics, regulation, and public perception (abstract).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Data Source Change",
      "target": "Public Perception",
      "relation": "affects",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Repercussions of changing data sources include effects on neutrality and potential discontinuation of statistical offering, impacting public discourse (abstract).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "Official Statistics",
      "target": "Public Perception",
      "relation": "influences",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Official statistics are essential for policy-making, decision-making, and public discourse (abstract).",
      "source_papers": [
        "2306.04338v1"
      ]
    },
    {
      "source": "DOME framework",
      "target": "supervised machine learning",
      "relation": "characterises_by",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "DOME provides a structured methods description for machine learning based on data, optimization, model, evaluation components.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "supervised machine learning",
      "target": "machine learning validation",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Modern biology relies on ML to provide predictions, necessitating scrutiny on ML performance and possible limitations via validation.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "machine learning validation",
      "target": "model performance",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Validation assesses the performance and limitations of a method or outcome through systematic evaluation protocols.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "Data Quality",
      "target": "model generalization",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Proper data collection, cleaning, and preprocessing establish the foundation for models that generalize beyond training data.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "evaluation metrics",
      "target": "machine learning validation",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Standardized metrics quantify and document model performance, integral to validation protocols recommended in DOME.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "hyperparameter optimization",
      "target": "model performance",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Systematic tuning of algorithm parameters maximizes model performance on validation data.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "model limitations",
      "target": "machine learning validation",
      "relation": "characterises_by",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Assessment of limitations helps reviewers and readers understand possible failure modes and restricted applicability of ML outcomes.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "DOME framework",
      "target": "reproducibility",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Structured documentation of data, optimization, model, and evaluation components facilitates independent verification and replication.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "community standards",
      "target": "machine learning validation",
      "relation": "characterises_by",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Community-wide recommendations aiming to establish standards of supervised ML validation in biology through DOME and related protocols.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "biological application",
      "target": "supervised machine learning",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Modern biology frequently relies on ML to provide predictions and improve decision processes across diverse applications.",
      "source_papers": [
        "2006.16189v4"
      ]
    },
    {
      "source": "transfer learning",
      "target": "LSTM recurrent neural network",
      "relation": "initializes",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "LSTM recurrent neural network",
      "target": "air pollution prediction",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS).",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "meteorological data",
      "target": "air pollution prediction",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Meteorological data and data on the concentration of APS have been utilized for prediction.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "air pollutant concentration",
      "target": "air pollution prediction",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Data on the concentration of APS have been utilized for air pollution prediction.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "air quality monitoring station",
      "target": "air pollutant concentration",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Air quality monitoring stations (AQMSs) have collected observed data of air pollutant concentrations.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "data sparsity",
      "target": "transfer learning",
      "relation": "motivates",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Some air quality monitoring stations (AQMSs) have less observed data in quantity; therefore, transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "pre-trained neural network",
      "target": "LSTM recurrent neural network",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "knowledge transfer",
      "target": "model performance",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.88,
      "evidence": "LSTM RNNs initialized with transfer learning methods have higher prediction accuracy than randomly initialized recurrent neural networks.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "temporal dependency",
      "target": "LSTM recurrent neural network",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "LSTM architectures are designed to capture temporal dependencies in sequential time-series pollutant concentration data.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "meteorological data",
      "target": "temporal dependency",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Meteorological variables contribute to temporal patterns in air pollutant concentrations.",
      "source_papers": [
        "2502.01654v1"
      ]
    },
    {
      "source": "Active learning",
      "target": "Query strategy",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Active learning is a paradigm that aims to select the most informative data points; selection strategies are core to this approach.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Online active learning",
      "target": "Data stream",
      "relation": "operates_on",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Online active learning involves continuously selecting and labeling observations as they arrive in a stream.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Online active learning",
      "target": "Active learning",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Online active learning is a variant of active learning focused on streaming data scenarios.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Pool-based active learning",
      "target": "Active learning",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Pool-based active learning is one of two broad categories within active learning approaches.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Active learning",
      "target": "Labeling cost",
      "relation": "minimizes",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in active learning.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Query strategy",
      "target": "Informativeness",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Query strategies aim to select the most informative observations for labeling; informativeness is the selection criterion.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Active learning",
      "target": "model performance",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Active learning aims to improve the performance of machine learning models by selecting informative observations.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Online active learning",
      "target": "Real-world applications",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Online active learning addresses the growing availability of data streams in real-world applications where labeling is costly.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Unlabeled data",
      "target": "Active learning",
      "relation": "is_input_to",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Active learning operates on unlabeled data, selecting which observations to label from those only available in an unlabeled form.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Online active learning",
      "target": "Labeling cost",
      "relation": "addresses",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Online active learning is proposed to overcome the issue of high labeling costs by selecting only the most informative observations.",
      "source_papers": [
        "2302.08893v4"
      ]
    },
    {
      "source": "Learning curves",
      "target": "Algorithm selection",
      "relation": "informs",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Learning curves provide insights into algorithm suitability at early stages, expediting the algorithm selection process.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Learning curves",
      "target": "Early stopping",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Learning curves have important applications in early stopping of model training.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Learning curves",
      "target": "Data acquisition",
      "relation": "informs",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Learning curves have important applications in data acquisition decisions.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Training set size",
      "target": "model performance",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Learning curves assess algorithm performance with respect to training set size as a key resource.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Learning curve models",
      "target": "Decision making",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Various learning curve models have been proposed to use learning curves for decision making.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "hyperparameter optimization",
      "target": "model performance",
      "relation": "regulates",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Learning curves model the performance of the combination of an algorithm and its hyperparameter configuration.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Learning curves",
      "target": "Reference performance",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Some learning curve models answer whether an algorithm will outperform a reference performance at a given budget.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "supervised machine learning",
      "target": "Learning curves",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Learning curves are a concept adopted in the context of machine learning to assess algorithm performance.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Learning curve models",
      "target": "Algorithm selection",
      "relation": "informs",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Learning curve models provide decision support for selecting suitable algorithms and configurations.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Resource allocation",
      "target": "Learning curves",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Learning curves assess algorithm performance with respect to resources like training examples and iterations.",
      "source_papers": [
        "2201.12150v2"
      ]
    },
    {
      "source": "Computational emotion models",
      "target": "Decision-making architecture",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Computational emotion models are grounded in the agent's decision making architecture, of which RL is an important subclass.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Computational emotion models",
      "target": "Reinforcement learning",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Emotion models may improve learning efficiency for ML researchers; emotions influence learning in RL-based agents.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Homeostasis",
      "target": "Computational emotion models",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Underlying dimensions like homeostasis are foundational from which emotions can be derived and modelled in RL-agents.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Appraisal theory",
      "target": "Computational emotion models",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Appraisal is identified as an underlying dimension from which emotions can be derived in RL-agents.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Computational emotion models",
      "target": "Action selection",
      "relation": "regulates",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Emotions influence motivation and action selection in agents; computational emotions are functional in decision-making.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Computational emotion models",
      "target": "Motivation",
      "relation": "influences",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Emotions are recognized as functional in decision-making by influencing motivation.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Intrinsic motivation",
      "target": "Reinforcement learning",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Survey draws connections to important RL sub-domains like intrinsic motivation.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Model-based reinforcement learning",
      "target": "Reinforcement learning",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Survey draws connections to important RL sub-domains like model-based RL.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Computational emotion models",
      "target": "Learning efficiency",
      "relation": "predicts",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Emotion models may improve learning efficiency; emotions influence the learning efficiency of the agent.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Computational emotion models",
      "target": "Social signals",
      "relation": "produces",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Emotions may be useful as social signals in human-robot interaction; emotions can communicate state.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Social signals",
      "target": "Human-robot interaction",
      "relation": "enhances",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Emotions can communicate state and enhance user investment in HRI; emotional expressions improve human-robot interaction.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "Affective modelling",
      "target": "Computational emotion models",
      "relation": "validates",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Affective modelling researchers can investigate their emotion theories in successful AI agent classes like RL.",
      "source_papers": [
        "1705.05172v1"
      ]
    },
    {
      "source": "TapNet",
      "target": "Few-shot learning",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "TapNet is explicitly proposed as 'neural networks augmented with task-adaptive projection for improved few-shot learning.'",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Meta-learning",
      "target": "Episode-based training",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Meta-learning employs 'episode-based training' where a network and reference vectors are learned across widely varying tasks.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Episode-based training",
      "target": "Support set",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Episode-based training simulates test conditions by providing labeled support sets and unlabeled query sets within each episode.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Episode-based training",
      "target": "Query set",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Episodes consist of support and query sets; query sets evaluate classification after task-specific adaptation.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Task-adaptive projection",
      "target": "Embedding space",
      "relation": "transforms",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Features in the embedding space are linearly projected into a new space as a form of quick task-specific conditioning for each episode.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Task-adaptive projection",
      "target": "Few-shot learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Task-adaptive projection improves generalization by conditioning features on task-specific structure, enabling superior few-shot performance.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Reference vectors",
      "target": "Distance metric",
      "relation": "compared_by",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Training loss is obtained based on distance metric between query and reference vectors in the projection space.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Query set",
      "target": "Distance metric",
      "relation": "measured_by",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Classification of query examples against reference vectors uses a learned distance metric in the task-projected embedding space.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "TapNet",
      "target": "Generalization",
      "relation": "produces",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Excellent generalization results are achieved; TapNet obtains state-of-the-art classification accuracies on Omniglot, miniImageNet, and tieredImageNet.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "TapNet",
      "target": "Omniglot dataset",
      "relation": "validated_on",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "TapNet is tested on Omniglot and obtains state-of-the-art classification accuracies under various few-shot scenarios.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "TapNet",
      "target": "miniImageNet dataset",
      "relation": "validated_on",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "TapNet is tested on miniImageNet and achieves state-of-the-art results.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "TapNet",
      "target": "tieredImageNet dataset",
      "relation": "validated_on",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "TapNet is tested on tieredImageNet and achieves state-of-the-art classification accuracies.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Meta-learning",
      "target": "Reference vectors",
      "relation": "learns",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "A network and a set of per-class reference vectors are learned across widely varying tasks via meta-learning.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "Embedding space",
      "target": "Task-adaptive projection",
      "relation": "transformed_by",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Features in the embedding space are linearly projected into a new space for each episode.",
      "source_papers": [
        "1905.06549v2"
      ]
    },
    {
      "source": "dynamic routing",
      "target": "computational unit",
      "relation": "selects",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "dynamically selects Computational Units (CU) for each input object from a learned set of transformations",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "ResNet architecture",
      "target": "image classification",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "ResNet architecture and its modifications produce state-of-the-art results in image classification problems",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "dynamic routing",
      "target": "image classification",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "achieve better results than the original ResNet on CIFAR-10.1 test set",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "input-dependent path",
      "target": "class-specific learning",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "network learned different routes for images from different classes and similar routes for similar images",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "learned transformations",
      "target": "computational unit",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "learned set of transformations that constitute the computational units",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "dynamic routing",
      "target": "learned transformations",
      "relation": "selects_from",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "dynamically selects Computational Units for each input object from a learned set of transformations",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "skip connections",
      "target": "network depth",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Skip connections enabling training of very deep networks",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "ResNet architecture",
      "target": "skip connections",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "ResNet architecture with skip connections",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "image classification",
      "target": "CIFAR-10 dataset",
      "relation": "evaluated_on",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "achieve better results than the original ResNet on CIFAR-10.1 test set",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "dynamic routing",
      "target": "model performance",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "achieve better results than the original ResNet on CIFAR-10.1 test set with dynamic routing mechanism",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "input-dependent path",
      "target": "computational unit",
      "relation": "consists_of",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "sequence of useful transformations and apply only required units",
      "source_papers": [
        "1811.04380v1"
      ]
    },
    {
      "source": "Energy landscape methods",
      "target": "Loss landscape",
      "relation": "analogous_to",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Methods from energy landscapes field are adapted to machine learning loss landscapes to identify conserved invariants.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Conserved weights",
      "target": "Feature relevance",
      "relation": "predicts",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Identifying conserved weights within groups of minima enables identification of drivers of model decision making, thus revealing feature importance.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Loss minima",
      "target": "Conserved weights",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Conserved weights are identified within groups of loss minima; weight invariants across minima indicate critical features.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Energy landscape methods",
      "target": "Machine learning interpretability",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "The paper proposes applying energy landscape techniques to make ML models more interpretable by identifying feature drivers.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Order parameters",
      "target": "Feature relevance",
      "relation": "analogous_to",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Order parameters from molecular sciences identify critical features of molecules; the paper proposes analogous invariants for ML loss landscapes.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Physics-inspired approach",
      "target": "Loss landscape",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Physics-inspired methods are applied to analyze and interpret the structure of machine learning loss landscapes.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Model decision making",
      "target": "Input data features",
      "relation": "depends_on",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "The paper investigates which features of input data prompt model decision making decisions.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Loss landscape",
      "target": "Model decision making",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "The loss landscape structure encodes the relationship between parameters and model predictions underlying decision making.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Conserved weights",
      "target": "Model decision drivers",
      "relation": "identifies",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Conserved weights are used to identify the drivers of model decision making.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Energy landscape methods",
      "target": "Conserved weights",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Energy landscape techniques are used to identify conserved weights within groups of loss minima.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Physics-inspired approach",
      "target": "Machine learning interpretability",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The paper demonstrates applicability of physics-inspired energy landscape methods to improve ML model interpretability.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Synthetic examples",
      "target": "Physics-inspired approach",
      "relation": "validates",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "Paper provides synthetic examples to demonstrate the applicability of physics-inspired interpretability methods.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Real-world application",
      "target": "Physics-inspired approach",
      "relation": "validates",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Real-world examples are given to show how physics-inspired methods help make models more interpretable in practical domains.",
      "source_papers": [
        "2304.02381v2"
      ]
    },
    {
      "source": "Random Neural Networks",
      "target": "supervised machine learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "RNNs have been successfully used as learning tools in supervised learning problems; tutorial focuses on their learning capabilities for this paradigm.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Random Neural Networks",
      "target": "Queuing network",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "RNNs can be seen as a specific type of queuing network; terminology from queuing theory and neural networks are used indistinctly.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Random Neural Networks",
      "target": "Neural Networks",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "RNNs are explicitly described as a class of Neural Networks with specific stochastic properties.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Random Neural Networks",
      "target": "Combinatorial optimization",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "RNNs have been used in combinatorial optimization where they are seen as neural systems for solving discrete optimization problems.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Learning procedures",
      "target": "Random Neural Networks",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Standard learning procedures used by RNNs are presented, covering first-order and second-order derivative techniques.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "First-order learning algorithms",
      "target": "Parameter optimization",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "First-order derivatives (gradients) are used as optimization technique for updating model parameters in RNNs.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Second-order learning algorithms",
      "target": "Parameter optimization",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Second-order derivatives provide an alternative optimization approach for updating RNN parameters with improved convergence properties.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Second-order learning algorithms",
      "target": "First-order learning algorithms",
      "relation": "outperforms",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "Second-order methods are presented as improvements over first-order approaches, using Hessian information for faster convergence.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Queuing network",
      "target": "Resource sharing",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Queuing networks are used to analyze the performance of resource sharing in engineering applications.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Random Neural Networks",
      "target": "Activation dynamics",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "RNNs have stochastic activation dynamics that distinguish them from standard neural networks and connect them to queuing models.",
      "source_papers": [
        "1609.04846v1"
      ]
    },
    {
      "source": "Quantized Neural Network",
      "target": "Network Compression",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "QNN is an efficient approach for network compression and can be widely used in the implementation of FPGAs.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "n-bit Quantization",
      "target": "Quantized Neural Network",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The paper proposes a learning framework for n-bit QNNs, whose weights are constrained to the power of two.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Gradient Vanishing",
      "target": "Backpropagation Algorithm",
      "relation": "inhibits",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "To solve the gradient vanishing problem, we propose a reconstructed gradient function for QNNs in back-propagation algorithm.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Reconstructed Gradient Function",
      "target": "Gradient Vanishing",
      "relation": "prevents",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The reconstructed gradient function for QNNs can directly get the real gradient rather than estimating an approximate gradient.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Reconstructed Gradient Function",
      "target": "Backpropagation Algorithm",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Reconstructed gradient function is proposed for QNNs in back-propagation algorithm that can directly get the real gradient.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "n-BQ-NN",
      "target": "FPGA Implementation",
      "relation": "optimizes",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "n-BQ-NN uses shift operation to replace the multiply operation and is more suitable for the inference on FPGAs.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Shift Vector Processing Element",
      "target": "Shift Operation",
      "relation": "employs",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "SVPE array is designed to replace all 16-bit multiplications with SHIFT operations in convolution operation on FPGAs.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Shift Operation",
      "target": "Digital Signal Processing Resource",
      "relation": "avoids_consuming",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "The SHIFT operation in SVPE array will not consume Digital Signal Processings (DSPs) resources on FPGAs.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Shift Vector Processing Element",
      "target": "Energy Consumption",
      "relation": "reduces",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Use of SVPE array reduces average energy consumption to 68.7% of the VPE array with 16-bit.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "n-BQ-NN",
      "target": "model performance",
      "relation": "preserves",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Quantized models through our learning framework can achieve almost the same accuracies with the original full-precision models.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Quantized Neural Network",
      "target": "model performance",
      "relation": "maintains",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Experimental results show that quantized models of ResNet, DenseNet and AlexNet can achieve almost the same accuracies.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "Reconstructed Gradient Function",
      "target": "model performance",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The reconstructed gradient function enables the learning framework to achieve state-of-the-art results.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "n-BQ-NN",
      "target": "FPGA Implementation",
      "relation": "accelerates",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "n-BQ-NN with SVPE can execute 2.9 times faster than with the vector processing element (VPE) in inference.",
      "source_papers": [
        "2004.02396v1"
      ]
    },
    {
      "source": "BERT",
      "target": "fake news detection",
      "relation": "outperforms",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "BERT and similar pre-trained models perform the best for fake news detection, especially with very small dataset.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "pre-trained language models",
      "target": "fake news detection",
      "relation": "transfers_to",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Advanced pre-trained language models explored for fake news detection along with traditional and deep learning approaches.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "traditional machine learning",
      "target": "fake news detection",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "Traditional machine learning approaches compared with deep learning and pre-trained models for fake news detection.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "deep learning",
      "target": "fake news detection",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Deep learning models explored and compared with traditional and pre-trained approaches for fake news detection.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "pre-trained language models",
      "target": "low-resource languages",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "These models are significantly better option for languages with limited electronic contents, i.e., training data.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "dataset bias",
      "target": "fake news detection",
      "relation": "inhibits",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Most prior work focused on specific news types (political), leading to dataset-bias of the models; benchmark study accumulates largest and most diversified dataset to address this.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "news datasets",
      "target": "fake news detection",
      "relation": "trained_on",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Benchmark study conducted on three different datasets to assess machine learning approaches for fake news detection.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "social media propagation",
      "target": "fake news detection",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "Proliferation of fake news and its propagation on social media is the motivation for developing detection models.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "article topic",
      "target": "model performance",
      "relation": "predicts",
      "edge_type": "empirical",
      "weight": 0.65,
      "evidence": "Analysis of models' performance based on article's topic carried out to understand detection effectiveness across domains.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "article length",
      "target": "model performance",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.6,
      "evidence": "Several analyses based on article's length were discussed to understand factors affecting model performance.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "benchmark study",
      "target": "fake news detection",
      "relation": "validates",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Benchmark study assesses performance of different applicable machine learning approaches by comparing their performances from different aspects.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "BERT",
      "target": "pre-trained language models",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "BERT is a specific instance of pre-trained language models, representing state-of-the-art approach in this class.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "text classification",
      "target": "fake news detection",
      "relation": "generalises_to",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Fake news detection is a specialized application of text classification to binary or multi-class news authenticity prediction.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "fake news detection",
      "target": "model performance",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Comparisons of model performances from different aspects based on detection task outcomes across multiple dimensions.",
      "source_papers": [
        "1905.04749v2"
      ]
    },
    {
      "source": "Privacy-Preserving Machine Learning",
      "target": "Healthcare ML",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "PPML techniques enable secure ML development in healthcare applications while protecting sensitive patient information throughout the pipeline.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Data Privacy",
      "target": "Privacy-Preserving Machine Learning",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "PPML is fundamentally defined by the requirement to protect data privacy along the entire ML pipeline.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Medical Data",
      "target": "Data Privacy",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Due to the sensitive nature of medical data, privacy must be considered throughout the entire ML pipeline.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Model Training",
      "target": "Privacy-Preserving Machine Learning",
      "relation": "part_of",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The review primarily focuses on privacy-preserving training as a key component of PPML.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Model Inference",
      "target": "Privacy-Preserving Machine Learning",
      "relation": "part_of",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The review focuses on privacy-preserving inference-as-a-service as a key component of PPML.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Inference-as-a-Service",
      "target": "Model Inference",
      "relation": "is_a",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Inference-as-a-service is a service delivery model for making predictions while preserving privacy.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "ML Pipeline",
      "target": "Data Privacy",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Privacy must be considered along the entire ML pipeline, from model training to inference.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Healthcare ML",
      "target": "Disease Diagnosis",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "ML has shown success in modeling healthcare prediction tasks including disease diagnosis.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Healthcare ML",
      "target": "Disease Prognosis",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "ML has shown success in modeling healthcare prediction tasks including disease prognosis.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Healthcare ML",
      "target": "Patient Treatment",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "ML has shown success in modeling healthcare prediction tasks including patient treatment.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Privacy-Preserving Machine Learning",
      "target": "Real-world application",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "The review aims to guide development of private and efficient ML models in healthcare with prospects of translating research into real-world settings.",
      "source_papers": [
        "2303.15563v1"
      ]
    },
    {
      "source": "Pipeline synthesis",
      "target": "AutoML",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Pipeline synthesis is the core component of AutoML systems for automatically designing machine learning pipelines.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Reinforcement learning",
      "target": "Pipeline synthesis",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "AlphaD3M reached state-of-the-art results using reinforcement learning with self-play for pipeline synthesis.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Model-based reinforcement learning",
      "target": "Reinforcement learning",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "This work extends AutoML by using model-based reinforcement learning with a pipeline grammar.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Pipeline grammar",
      "target": "Pipeline synthesis",
      "relation": "constrains",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The work uses a pipeline grammar to structure and constrain the pipeline synthesis search space.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Self-play",
      "target": "Reinforcement learning",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "AlphaD3M achieved state-of-the-art results using reinforcement learning with self-play for improved generalization.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "pre-trained neural network",
      "target": "Pipeline synthesis",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "The approach extends AlphaD3M by using a pre-trained model that generalizes from many different datasets and similar tasks.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "pre-trained neural network",
      "target": "AutoML",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Results demonstrate improved performance compared with existing methods on AutoML benchmark datasets using pre-trained models.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Bayesian optimization",
      "target": "AutoML",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "The strongest AutoML systems are based on Bayesian optimization among other techniques.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Evolutionary algorithms",
      "target": "AutoML",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "The strongest AutoML systems are based on evolutionary algorithms among other techniques.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "Neural Networks",
      "target": "AutoML",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "The strongest AutoML systems are based on neural networks among other techniques.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "AutoML",
      "target": "Classification task",
      "relation": "generalizes_to",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Results demonstrate improved performance on AutoML benchmark datasets for classification and regression tasks.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "AutoML",
      "target": "Regression task",
      "relation": "generalizes_to",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Results demonstrate improved performance on AutoML benchmark datasets for classification and regression tasks.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "AutoML",
      "target": "Benchmark dataset",
      "relation": "trained_on",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "AutoML systems are evaluated and compared on benchmark datasets for classification and regression.",
      "source_papers": [
        "1905.10345v1"
      ]
    },
    {
      "source": "meta-reinforcement learning",
      "target": "deep reinforcement learning",
      "relation": "extends",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Meta-RL casts the development of better RL algorithms as a machine learning problem itself, addressing limitations of deep RL.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "meta-reinforcement learning",
      "target": "policy adaptation",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Meta-RL learns a policy capable of adapting to any new task from the task distribution with minimal data.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "meta-reinforcement learning",
      "target": "task distribution",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Meta-RL is studied in a problem setting where given a distribution of tasks, the goal is to learn a policy adaptable to any new task from the task distribution.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "meta-reinforcement learning",
      "target": "data efficiency",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Meta-RL alleviates the poor data efficiency limitation of deep RL by enabling rapid adaptation to new tasks.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "meta-reinforcement learning",
      "target": "Generalization",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Meta-RL addresses the limited generality of policies produced by standard deep RL through adaptation capabilities.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "policy adaptation",
      "target": "learning budget",
      "relation": "constrained_by",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "The amount of data available for each individual task constrains how quickly a policy can adapt to new tasks.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "task distribution",
      "target": "policy adaptation",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Learning from a distribution of tasks enables policies to develop adaptation mechanisms for handling task variation.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "meta-reinforcement learning",
      "target": "RL algorithm",
      "relation": "optimizes",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Meta-RL treats the development of better RL algorithms as the primary learning objective.",
      "source_papers": [
        "2301.08028v4"
      ]
    },
    {
      "source": "Shannon entropy",
      "target": "Uncertainty quantification",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Shannon entropy is extended through integration with rough set theory to offer uncertainty quantification of data's intrinsic structure.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Rough set theory",
      "target": "Granularity analysis",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Methodology synergizes the granularity of rough set theory with uncertainty quantification for deeper insight.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Shannon entropy",
      "target": "Information structure",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Entropy quantifies information content within data's intrinsic structure.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Rough set theory",
      "target": "Information structure",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Rough set theory partitions data into equivalence classes, revealing underlying organizational patterns.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Data complexity",
      "target": "Machine learning evaluation",
      "relation": "constrains",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Assessment must illuminate underlying data complexity alongside predictive performance.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Machine learning evaluation",
      "target": "model performance",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Evaluation approach assesses predictive performance as core metric.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Machine learning evaluation",
      "target": "Robustness",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Methodology capable of illuminating model robustness alongside accuracy.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Shannon entropy",
      "target": "Machine learning evaluation",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Conventional application of entropy is extended through integration to generalize evaluation approach in machine learning.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Rough set theory",
      "target": "Machine learning evaluation",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Rough set theory combined with Shannon entropy to offer deeper insight into data structure and model evaluation.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Machine learning interpretability",
      "target": "Machine learning evaluation",
      "relation": "characterises",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Integrated approach illuminates interpretability of machine learning models.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Data complexity",
      "target": "Robustness",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Understanding data complexity and attributes informs model robustness assessment.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "Machine learning evaluation",
      "target": "Algorithm selection",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Evaluation methodology facilitates more informed decision-making in model selection and application.",
      "source_papers": [
        "2404.12511v1"
      ]
    },
    {
      "source": "single linkage clustering",
      "target": "dendrogram",
      "relation": "produces",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Single linkage criterion naturally generates a dendrogram by successively merging closest clusters.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "single linkage clustering",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Minimax distance measures correspond to building a dendrogram with single linkage criterion, with specific level and distance functions.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "dendrogram",
      "target": "level function",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Different level functions parameterize the hierarchical structure of dendrograms to define distances.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "dendrogram",
      "target": "distance function",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Distance functions are defined over dendrogram nodes to quantify dissimilarity between clusters.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "level function",
      "target": "Minimax distance",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Level functions specify the hierarchical parameterization that produces Minimax distance metrics.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "distance function",
      "target": "Minimax distance",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Distance functions combined with level functions define the Minimax distance measure.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "dendrogram",
      "target": "Embedding space",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Via appropriate embedding, dendrograms are transformed into vector-based representations to enable numerical machine learning algorithms.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "Embedding space",
      "target": "unsupervised representation learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Vector embeddings derived from dendrograms provide the feature representations for unsupervised learning tasks.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "unsupervised representation learning",
      "target": "Minimax distance",
      "relation": "generalises",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The framework extends Minimax distance methods from single linkage to arbitrary dendrograms and level/distance functions.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "ensemble clustering",
      "target": "graph-based aggregation",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Multiple dendrogram-based distances are aggregated by constructing a weighted graph encoding label consistency across solutions.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "graph-based aggregation",
      "target": "correlation clustering",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Correlation clustering is applied to the aggregated graph to produce final consensus clusters from ensemble predictions.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "ensemble clustering",
      "target": "Algorithm selection",
      "relation": "addresses",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Aggregation of different dendrogram-based distances helps resolve the model selection problem by combining multiple solutions.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "deep representations",
      "target": "unsupervised representation learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Multi-layered sequential composition of distances and features, analogous to deep learning, refines the final representations.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "dendrogram",
      "target": "deep representations",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Different dendrograms are combined sequentially in the spirit of multi-layered architectures to obtain final features.",
      "source_papers": [
        "1812.09225v4"
      ]
    },
    {
      "source": "Stochastic gradient descent",
      "target": "Network embedding",
      "relation": "trains",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Query strategy",
      "target": "Stochastic gradient descent",
      "relation": "configures",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The sub-sampling scheme can be freely chosen in stochastic gradient methods for learning embeddings.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "node2vec",
      "target": "Query strategy",
      "relation": "exemplifies",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Representation methods using a subsampling approach, such as node2vec, are encapsulated in a single unifying framework.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Exchangeable graph",
      "target": "Asymptotic distribution",
      "relation": "enables_analysis_of",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Under the assumption that the graph is exchangeable, the distribution of the learned embedding vectors asymptotically decouples.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Network embedding",
      "target": "Asymptotic distribution",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "We characterize the asymptotic distribution of the learned embedding vectors.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Loss function",
      "target": "Asymptotic distribution",
      "relation": "determines",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "The asymptotic distribution is provided in terms of the latent parameters, which includes the choice of loss function.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Embedding dimension",
      "target": "Asymptotic distribution",
      "relation": "affects",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The asymptotic distribution is characterized in terms of the embedding dimension.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Convergence rate",
      "target": "Asymptotic distribution",
      "relation": "measures",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "We provided rates of convergence in terms of the latent parameters, which characterize the asymptotic distribution.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Loss function",
      "target": "Fisher consistency",
      "relation": "may_violate",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Typically used loss functions may lead to shortcomings, such as a lack of Fisher consistency.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Network embedding",
      "target": "Node classification",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Tasks of interest including node classification are performed on learned embeddings.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Network embedding",
      "target": "Link prediction",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Tasks of interest including link prediction are performed on learned embeddings.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Network embedding",
      "target": "Node clustering",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Tasks of interest including node clustering are performed on learned embeddings.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Latent parameters",
      "target": "Asymptotic distribution",
      "relation": "parameterise",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "The asymptotic distribution is characterized in terms of the latent parameters.",
      "source_papers": [
        "2107.02363v4"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "Minimum spanning tree",
      "relation": "reduces_to",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "The equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that allows efficient computation.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "Pairwise distance embedding",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Pairwise Minimax distances are embedded into a new vector space such that squared Euclidean distances equal the pairwise Minimax distances.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Pairwise distance embedding",
      "target": "Eigenvalue decomposition",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "An eigenvalue decomposition is performed on centered matrices to obtain relevant features from multiple pairwise Minimax matrices.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "Feature extraction",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Minimax distances are used to extract features that capture unknown underlying patterns and structures in the data in a nonparametric way.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "K-nearest neighbor search",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.75,
      "evidence": "Computing Minimax distances from a fixed test object can be used in K-nearest neighbor search applications.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "Outlier detection",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The edges selected by Minimax distances are investigated to explore the ability to detect outlier objects.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Feature extraction",
      "target": "Nonparametric methods",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "The Minimax distance framework employs nonparametric methods to extract features without assuming specific distributional forms.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Minimax distance",
      "target": "Base distance measure",
      "relation": "generalises",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The efficient algorithm for computing Minimax distances is applicable with any arbitrary base distance measure.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Centered matrix",
      "target": "Eigenvalue decomposition",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Centered matrices are produced prior to performing eigenvalue decomposition for feature extraction from multiple distance matrices.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Graph representation",
      "target": "Minimax distance",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Minimax distances are computed over a graph structure, with equivalence to minimum spanning tree distances.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Numerical data",
      "target": "Minimax distance",
      "relation": "correlates_with",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The general-purpose framework employs Minimax distances with machine learning methods that perform on numerical data.",
      "source_papers": [
        "1904.13223v3"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Moment constraints",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "MEMe is capable of dealing with hundreds of moments and constructs distributions satisfying moment constraints.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Entropy maximization",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Maximum entropy method fundamentally operates by maximizing entropy subject to constraints.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Constrained Bayesian inference",
      "relation": "approximates",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Paper demonstrates equivalence of the proposed maximum entropy algorithm to constrained Bayesian variational inference.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Variational inference",
      "relation": "reduces_to",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Maximum entropy method is shown to be equivalent to constrained Bayesian variational inference framework.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Log determinant estimation",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Paper demonstrates application of MEMe to fast log determinant estimation as a key use case.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Bayesian optimization",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Maximum entropy method is applied to information-theoretic Bayesian optimization as a second application.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Large-scale machine learning",
      "relation": "generalises_to",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Proposed method is described as capable of dealing with hundreds of moments, enabling large-scale applications.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Moment matching",
      "target": "Moment constraints",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Moment matching operationalizes satisfaction of moment constraints through inference.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Efficient approximation",
      "target": "Large-scale machine learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Efficient approximation lies at the heart of large-scale machine learning problems, enabling computational tractability.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Maximum entropy method",
      "target": "Efficient approximation",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "MEMe is proposed as a novel algorithm allowing for computationally efficient approximations.",
      "source_papers": [
        "1906.01101v1"
      ]
    },
    {
      "source": "Universal graph embedding",
      "target": "Graph neural network",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "DUGNN model incorporates a novel graph neural network (as a universal graph encoder)",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Universal graph embedding",
      "target": "transfer learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "By learning task-independent graph embeddings across diverse datasets, DUGNN also reaps the benefits of transfer learning",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Universal graph embedding",
      "target": "Graph classification",
      "relation": "predicts",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "DUGNN model consistently outperforms existing models by 3%-8% on graph classification benchmark datasets",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Graph neural network",
      "target": "Feature aggregation",
      "relation": "characterised_by",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Graph neural networks learn representations by iteratively aggregating node features across neighborhoods",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Graph kernel",
      "target": "Universal graph embedding",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "DUGNN leverages rich Graph Kernels (as a multi-task graph decoder) for both unsupervised and supervised learning",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Unsupervised learning",
      "target": "Universal graph embedding",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Embeddings learned through unsupervised fashion to create universal task-independent representations",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "supervised machine learning",
      "target": "Universal graph embedding",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Adaptive supervised learning applied on top of task-independent embeddings for downstream tasks",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Multi-task learning",
      "target": "Universal graph embedding",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Learning task-independent embeddings across diverse datasets leverages multi-task learning benefits",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Graph encoder",
      "target": "Universal graph embedding",
      "relation": "produces",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Universal graph encoder transforms graph structure into fixed-dimensional embeddings",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Graph decoder",
      "target": "Graph classification",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Graph kernel decoder makes task-specific predictions for graph classification from universal embeddings",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Universal graph embedding",
      "target": "Task-independent representation",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "DUGNN designed to learn task-independent graph embeddings that generalize across datasets",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "Graph neural network",
      "target": "Graph kernel",
      "relation": "outperforms",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "DUGNN model consistently outperforms both existing state-of-the-art GNN models and Graph Kernels by 3%-8%",
      "source_papers": [
        "1909.10086v3"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "DRL algorithms",
      "relation": "implements",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "DRL algorithms",
      "target": "deep reinforcement learning",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "DRL algorithms are specific instantiations of deep reinforcement learning combining neural networks with RL.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "Chainer framework",
      "relation": "built_on",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "ChainerRL is built using the Chainer deep learning framework.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "Policy learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "ChainerRL implements algorithms that learn policies from rewards and observations.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "Neural network approximation",
      "target": "deep reinforcement learning",
      "relation": "part-of",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Deep RL combines deep neural networks with reinforcement learning as core components.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "reproducibility",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "ChainerRL provides scripts that closely replicate the original papers' experimental settings to foster reproducible research.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "benchmark study",
      "relation": "validates_with",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "ChainerRL reproduces published benchmark results for several algorithms.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "Agent visualization",
      "relation": "provides",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "ChainerRL library",
      "target": "Python implementation",
      "relation": "is-a",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "ChainerRL is an open-source library built using Python.",
      "source_papers": [
        "1912.03905v2"
      ]
    },
    {
      "source": "Alpha MAML",
      "target": "Model-Agnostic Meta-Learning",
      "relation": "extends",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Alpha MAML is an extension to MAML to incorporate an online hyperparameter adaptation scheme.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Model-Agnostic Meta-Learning",
      "target": "Few-shot learning",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "MAML primes the model for few-shot learning of new tasks.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Online hyperparameter adaptation",
      "target": "hyperparameter optimization",
      "relation": "replaces_computation_of",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Alpha MAML eliminates the need to tune meta-learning and learning rates through online adaptation.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Alpha MAML",
      "target": "Training stability",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Alpha MAML improves training stability with less sensitivity to hyperparameter choice.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Meta-learning rate",
      "target": "Model-Agnostic Meta-Learning",
      "relation": "is_feature_for",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Meta-learning rate is a hyperparameter requiring tuning in MAML training.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Online hyperparameter adaptation",
      "target": "Meta-learning rate",
      "relation": "regulates",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Alpha MAML's adaptation scheme automatically adjusts meta-learning rate.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Model-Agnostic Meta-Learning",
      "target": "Classification task",
      "relation": "generalizes_to",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The MAML algorithm performs well on few-shot learning problems in classification.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Model-Agnostic Meta-Learning",
      "target": "Regression task",
      "relation": "generalizes_to",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "The MAML algorithm performs well on few-shot learning problems in regression.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Model-Agnostic Meta-Learning",
      "target": "policy adaptation",
      "relation": "generalizes_to",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "MAML performs well on fine-tuning of policy gradients in reinforcement learning.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "Alpha MAML",
      "target": "Omniglot dataset",
      "relation": "trained_on",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Results with the Omniglot database demonstrate improvement to training stability.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "hyperparameter optimization",
      "target": "Model-Agnostic Meta-Learning",
      "relation": "constrains",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "MAML comes with the need for costly hyperparameter tuning for training stability.",
      "source_papers": [
        "1905.07435v1"
      ]
    },
    {
      "source": "class imbalance learning",
      "target": "minority class",
      "relation": "concerns",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Class imbalance learning addresses underrepresentation of minority classes in training.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "class imbalance learning",
      "target": "majority class",
      "relation": "concerns",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Models become biased toward majority class during training due to class imbalance.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "random vector functional link network",
      "target": "class imbalance learning",
      "relation": "requires",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "RVFL network is effective for classification but suffers when dealing with imbalanced datasets.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "random vector functional link network",
      "relation": "extends",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Proposed GE-IFRVFL-CIL integrates graph embedding and intuitionistic fuzzy theory into RVFL architecture.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "weighting mechanism",
      "relation": "incorporates",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Proposed model incorporates a weighting mechanism to handle imbalanced datasets.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "Network embedding",
      "relation": "incorporates",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Model leverages graph embedding to preserve inherent topological structure of datasets.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "intuitionistic fuzzy sets",
      "relation": "incorporates",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Model employs intuitionistic fuzzy theory to handle uncertainty and imprecision in data.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "Network embedding",
      "target": "topological structure preservation",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Graph embedding is leveraged to preserve the inherent topological structure of datasets.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "intuitionistic fuzzy sets",
      "target": "uncertainty handling",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Intuitionistic fuzzy theory handles uncertainty and imprecision in the data.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "weighting mechanism",
      "target": "class imbalance learning",
      "relation": "addresses",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Weighting scheme is used to counteract the negative effects of class imbalance.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "KEEL benchmark datasets",
      "relation": "trained_on",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Proposed model achieved superior performance on KEEL benchmark imbalanced datasets.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "ADNI dataset",
      "relation": "trained_on",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Model was implemented on ADNI dataset and achieved promising results in real-world applications.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "noise robustness",
      "relation": "exhibits",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Model demonstrated superior performance on datasets with and without Gaussian noise.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "GE-IFRVFL-CIL model",
      "target": "generalization performance",
      "relation": "improves",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Proposed model offers superior performance mitigating noise effects and preserving dataset structures for better generalization.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "Network embedding",
      "target": "noise robustness",
      "relation": "augments",
      "edge_type": "empirical",
      "weight": 0.7,
      "evidence": "Graph embedding combined with other techniques mitigates detrimental effects of noise and outliers.",
      "source_papers": [
        "2307.07881v2"
      ]
    },
    {
      "source": "Event-based sensors",
      "target": "Sparse spatiotemporal data",
      "relation": "produces",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Event-based sensors generate continuous ultra-sparse spatiotemporal data.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "ALERT module",
      "target": "PointNet embedding",
      "relation": "extends",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "ALERT module is an embedding based on PointNet models that continuously integrates new and dismisses old events.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Leakage mechanism",
      "target": "ALERT module",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Leakage mechanism allows ALERT to continuously dismiss old events thanks to a leakage mechanism.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Sparse spatiotemporal data",
      "target": "Patch-based sparsity",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize efficiency.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Patch-based sparsity",
      "target": "Vision Transformer",
      "relation": "inspired_by",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Patch-based approach is inspired by Vision Transformer to optimize efficiency of the method.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "ALERT module",
      "target": "Synchronous readout",
      "relation": "feeds",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Flexible readout of the embedded data from ALERT allows to feed downstream models with up-to-date features at any sampling rate.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Synchronous readout",
      "target": "Transformer model",
      "relation": "provides_input_to",
      "edge_type": "empirical",
      "weight": 0.95,
      "evidence": "Features extracted via synchronous readout are processed by a transformer model.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Transformer model",
      "target": "Object recognition",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Transformer model trained for object and gesture recognition tasks.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Transformer model",
      "target": "Gesture recognition",
      "relation": "enables",
      "edge_type": "empirical",
      "weight": 0.9,
      "evidence": "Transformer model trained for object and gesture recognition tasks.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Asynchronous processing",
      "target": "Synchronous readout",
      "relation": "bridges_to",
      "edge_type": "empirical",
      "weight": 1.0,
      "evidence": "Hybrid pipeline composed of asynchronous sensing and synchronous processing with flexible readout.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "ALERT module",
      "target": "Latency",
      "relation": "reduces",
      "edge_type": "empirical",
      "weight": 0.85,
      "evidence": "Achieving state-of-the-art performances with lower latency than competitors.",
      "source_papers": [
        "2402.01393v3"
      ]
    },
    {
      "source": "Asynchronous processing",
      "target": "Latency",
      "relation": "reduces",
      "edge_type": "empirical",
      "weight": 0.8,
      "evidence": "Asynchronous approach achieves lower latency than synchronous competitors.",
      "source_papers": [
        "2402.01393v3"
      ]
    }
  ]
}