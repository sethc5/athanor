{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0b9945",
   "metadata": {},
   "source": [
    "# Athanor — Stage 3: Hypothesis Generator\n",
    "\n",
    "**Input:** Stage 2 `gap_report.json`  \n",
    "**Process:** Human-in-loop approval → Claude generates falsifiable hypothesis + experiment design per gap  \n",
    "**Output:** `outputs/hypotheses/<domain>/hypothesis_report.json`  \n",
    "\n",
    "**This closes the core loop:**  \n",
    "`Ingest → Map → Gap-find → Hypothesize → Design experiment → Test computationally → Surface → Repeat`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "console = Console()\n",
    "print(\"✓ Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4c8e",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"domain\": \"information_theory\",\n",
    "    \"claude_model\": \"claude-opus-4-5\",\n",
    "\n",
    "    # Stage 2 input\n",
    "    \"gap_report_path\": Path(\"..\") / \"outputs\" / \"gaps\" / \"gap_report.json\",\n",
    "\n",
    "    # Stage 3 outputs\n",
    "    \"output_dir\": Path(\"..\") / \"outputs\" / \"hypotheses\" / \"information_theory\",\n",
    "\n",
    "    # Generate hypotheses for all gaps, or only approved ones?\n",
    "    # Set to True after running §3 (human review)\n",
    "    \"approved_only\": False,\n",
    "\n",
    "    # Cap: how many hypotheses to generate (controls cost)\n",
    "    \"max_hypotheses\": 10,\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    console.print(\"[bold red]ANTHROPIC_API_KEY not set![/]\")\n",
    "else:\n",
    "    console.print(f\"[bold green]✓ Config ready[/] — model: {CONFIG['claude_model']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d0e74",
   "metadata": {},
   "source": [
    "## 2. Load Stage 2 Gap Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dabfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.gaps.models import GapReport\n",
    "\n",
    "if not CONFIG[\"gap_report_path\"].exists():\n",
    "    console.print(f\"[bold red]Missing:[/] {CONFIG['gap_report_path']}\\nRun Stage 2 first.\")\n",
    "    raise FileNotFoundError(CONFIG[\"gap_report_path\"])\n",
    "\n",
    "gap_report = GapReport.model_validate_json(CONFIG[\"gap_report_path\"].read_text())\n",
    "ranked = gap_report.ranked[:CONFIG[\"max_hypotheses\"]]\n",
    "\n",
    "table = Table(title=f\"Gap Report — {gap_report.domain} ({len(gap_report.analyses)} gaps)\", show_lines=True)\n",
    "table.add_column(\"#\", style=\"dim\", width=3)\n",
    "table.add_column(\"Concept A\", style=\"cyan\")\n",
    "table.add_column(\"Concept B\", style=\"cyan\")\n",
    "table.add_column(\"Score\", style=\"bold green\")\n",
    "table.add_column(\"N\", style=\"magenta\")\n",
    "table.add_column(\"T\", style=\"blue\")\n",
    "table.add_column(\"I\", style=\"red\")\n",
    "table.add_column(\"✓?\", style=\"yellow\")\n",
    "table.add_column(\"Research Question\", style=\"white\")\n",
    "\n",
    "for i, a in enumerate(ranked, 1):\n",
    "    approved_str = {None: \"—\", True: \"✓\", False: \"✗\"}.get(getattr(a, \"approved\", None), \"—\")\n",
    "    table.add_row(\n",
    "        str(i), a.concept_a, a.concept_b,\n",
    "        f\"{a.composite_score:.2f}\",\n",
    "        str(a.novelty), str(a.tractability), str(a.impact),\n",
    "        approved_str,\n",
    "        a.research_question[:70] + (\"…\" if len(a.research_question) > 70 else \"\"),\n",
    "    )\n",
    "\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836e14a",
   "metadata": {},
   "source": [
    "## 3. Human-in-Loop Review\n",
    "\n",
    "Set `approved=True` for gaps you want hypotheses for.  \n",
    "Set `approved=False` to skip.  \n",
    "Leave `approved=None` to include all (when `approved_only=False` in CONFIG).\n",
    "\n",
    "Edit the dict below and re-run this cell before §4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14097b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Edit this dict to approve/reject gaps ─────────────────────────────────────\n",
    "# Key:   \"{concept_a} ↔ {concept_b}\"  (as shown in the table above)\n",
    "# Value: True = run hypothesis gen | False = skip | (omit = leave as None)\n",
    "APPROVALS: dict[str, bool] = {\n",
    "    # Example — replace with actual concept pairs from your table:\n",
    "    # \"entropy ↔ neural networks\": True,\n",
    "    # \"channel capacity ↔ algorithmic complexity\": True,\n",
    "}\n",
    "\n",
    "# Apply approvals\n",
    "for analysis in ranked:\n",
    "    key = f\"{analysis.concept_a} ↔ {analysis.concept_b}\"\n",
    "    key_rev = f\"{analysis.concept_b} ↔ {analysis.concept_a}\"\n",
    "    if key in APPROVALS:\n",
    "        analysis.approved = APPROVALS[key]\n",
    "    elif key_rev in APPROVALS:\n",
    "        analysis.approved = APPROVALS[key_rev]\n",
    "    # else: leave as None\n",
    "\n",
    "approved_count = sum(1 for a in ranked if getattr(a, \"approved\", None) is True)\n",
    "rejected_count = sum(1 for a in ranked if getattr(a, \"approved\", None) is False)\n",
    "unreviewed = sum(1 for a in ranked if getattr(a, \"approved\", None) is None)\n",
    "\n",
    "console.print(f\"[bold]Review status:[/] ✓ {approved_count} approved | ✗ {rejected_count} rejected | — {unreviewed} unreviewed\")\n",
    "if CONFIG[\"approved_only\"] and approved_count == 0:\n",
    "    console.print(\"[yellow]Warning: approved_only=True but no gaps approved. Set approvals above or set approved_only=False.[/]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c81fe",
   "metadata": {},
   "source": [
    "## 4. Generate Hypotheses\n",
    "\n",
    "Claude produces for each gap:\n",
    "- A falsifiable **hypothesis statement**\n",
    "- The proposed **mechanism**\n",
    "- A specific, measurable **prediction**\n",
    "- **Falsification criteria** (what would definitively refute it)\n",
    "- A full **experiment design** with steps, tools, effort estimate\n",
    "- Flag: **computational** vs. wet-lab primary\n",
    "\n",
    "Results are cached — re-run is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa52f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.hypotheses import HypothesisGenerator, HypothesisReport\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "cache_path = CONFIG[\"output_dir\"] / \"hypothesis_cache.json\"\n",
    "\n",
    "if cache_path.exists():\n",
    "    console.print(\"[yellow]Loading cached hypothesis report…[/]\")\n",
    "    hyp_report = HypothesisReport.model_validate_json(cache_path.read_text())\n",
    "else:\n",
    "    generator = HypothesisGenerator(\n",
    "        domain=CONFIG[\"domain\"],\n",
    "        model=CONFIG[\"claude_model\"],\n",
    "        api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
    "    )\n",
    "\n",
    "    targets = [\n",
    "        a for a in ranked\n",
    "        if not CONFIG[\"approved_only\"] or getattr(a, \"approved\", None) is True\n",
    "    ]\n",
    "\n",
    "    hyp_report = HypothesisReport(\n",
    "        domain=CONFIG[\"domain\"],\n",
    "        query=gap_report.query,\n",
    "        n_gaps_considered=len(targets),\n",
    "    )\n",
    "\n",
    "    for i, analysis in enumerate(tqdm(targets, desc=\"Generating hypotheses\")):\n",
    "        console.print(f\"  [{i+1}/{len(targets)}] [cyan]{analysis.concept_a}[/] ↔ [cyan]{analysis.concept_b}[/]\")\n",
    "        hyp = generator._generate_one(analysis)\n",
    "        if hyp:\n",
    "            hyp_report.hypotheses.append(hyp)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    cache_path.write_text(hyp_report.model_dump_json(indent=2))\n",
    "    console.print(f\"[green]✓ Cached → {cache_path}[/]\")\n",
    "\n",
    "console.print(f\"\\n[bold green]✓ {len(hyp_report.hypotheses)} hypotheses generated[/]\")\n",
    "console.print(f\"  Computational: {len(hyp_report.computational)} | Pending review: {len(hyp_report.pending_review)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
