{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0b9945",
   "metadata": {},
   "source": [
    "# Athanor — Stage 3: Hypothesis Generator\n",
    "\n",
    "**Input:** Stage 2 `gap_report.json`  \n",
    "**Process:** Human-in-loop approval → Claude generates falsifiable hypothesis + experiment design per gap  \n",
    "**Output:** `outputs/hypotheses/<domain>/hypothesis_report.json`  \n",
    "\n",
    "**This closes the core loop:**  \n",
    "`Ingest → Map → Gap-find → Hypothesize → Design experiment → Test computationally → Surface → Repeat`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "console = Console()\n",
    "print(\"✓ Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4c8e",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"domain\": \"information_theory\",\n",
    "    \"claude_model\": \"claude-opus-4-5\",\n",
    "\n",
    "    # Stage 2 input\n",
    "    \"gap_report_path\": Path(\"..\") / \"outputs\" / \"gaps\" / \"gap_report.json\",\n",
    "\n",
    "    # Stage 3 outputs\n",
    "    \"output_dir\": Path(\"..\") / \"outputs\" / \"hypotheses\" / \"information_theory\",\n",
    "\n",
    "    # Generate hypotheses for all gaps, or only approved ones?\n",
    "    # Set to True after running §3 (human review)\n",
    "    \"approved_only\": False,\n",
    "\n",
    "    # Cap: how many hypotheses to generate (controls cost)\n",
    "    \"max_hypotheses\": 10,\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    console.print(\"[bold red]ANTHROPIC_API_KEY not set![/]\")\n",
    "else:\n",
    "    console.print(f\"[bold green]✓ Config ready[/] — model: {CONFIG['claude_model']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d0e74",
   "metadata": {},
   "source": [
    "## 2. Load Stage 2 Gap Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dabfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.gaps.models import GapReport\n",
    "\n",
    "if not CONFIG[\"gap_report_path\"].exists():\n",
    "    console.print(f\"[bold red]Missing:[/] {CONFIG['gap_report_path']}\\nRun Stage 2 first.\")\n",
    "    raise FileNotFoundError(CONFIG[\"gap_report_path\"])\n",
    "\n",
    "gap_report = GapReport.model_validate_json(CONFIG[\"gap_report_path\"].read_text())\n",
    "ranked = gap_report.ranked[:CONFIG[\"max_hypotheses\"]]\n",
    "\n",
    "table = Table(title=f\"Gap Report — {gap_report.domain} ({len(gap_report.analyses)} gaps)\", show_lines=True)\n",
    "table.add_column(\"#\", style=\"dim\", width=3)\n",
    "table.add_column(\"Concept A\", style=\"cyan\")\n",
    "table.add_column(\"Concept B\", style=\"cyan\")\n",
    "table.add_column(\"Score\", style=\"bold green\")\n",
    "table.add_column(\"N\", style=\"magenta\")\n",
    "table.add_column(\"T\", style=\"blue\")\n",
    "table.add_column(\"I\", style=\"red\")\n",
    "table.add_column(\"✓?\", style=\"yellow\")\n",
    "table.add_column(\"Research Question\", style=\"white\")\n",
    "\n",
    "for i, a in enumerate(ranked, 1):\n",
    "    approved_str = {None: \"—\", True: \"✓\", False: \"✗\"}.get(getattr(a, \"approved\", None), \"—\")\n",
    "    table.add_row(\n",
    "        str(i), a.concept_a, a.concept_b,\n",
    "        f\"{a.composite_score:.2f}\",\n",
    "        str(a.novelty), str(a.tractability), str(a.impact),\n",
    "        approved_str,\n",
    "        a.research_question[:70] + (\"…\" if len(a.research_question) > 70 else \"\"),\n",
    "    )\n",
    "\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836e14a",
   "metadata": {},
   "source": [
    "## 3. Human-in-Loop Review\n",
    "\n",
    "Set `approved=True` for gaps you want hypotheses for.  \n",
    "Set `approved=False` to skip.  \n",
    "Leave `approved=None` to include all (when `approved_only=False` in CONFIG).\n",
    "\n",
    "Edit the dict below and re-run this cell before §4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14097b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Edit this dict to approve/reject gaps ─────────────────────────────────────\n",
    "# Key:   \"{concept_a} ↔ {concept_b}\"  (as shown in the table above)\n",
    "# Value: True = run hypothesis gen | False = skip | (omit = leave as None)\n",
    "APPROVALS: dict[str, bool] = {\n",
    "    # Example — replace with actual concept pairs from your table:\n",
    "    # \"entropy ↔ neural networks\": True,\n",
    "    # \"channel capacity ↔ algorithmic complexity\": True,\n",
    "}\n",
    "\n",
    "# Apply approvals\n",
    "for analysis in ranked:\n",
    "    key = f\"{analysis.concept_a} ↔ {analysis.concept_b}\"\n",
    "    key_rev = f\"{analysis.concept_b} ↔ {analysis.concept_a}\"\n",
    "    if key in APPROVALS:\n",
    "        analysis.approved = APPROVALS[key]\n",
    "    elif key_rev in APPROVALS:\n",
    "        analysis.approved = APPROVALS[key_rev]\n",
    "    # else: leave as None\n",
    "\n",
    "approved_count = sum(1 for a in ranked if getattr(a, \"approved\", None) is True)\n",
    "rejected_count = sum(1 for a in ranked if getattr(a, \"approved\", None) is False)\n",
    "unreviewed = sum(1 for a in ranked if getattr(a, \"approved\", None) is None)\n",
    "\n",
    "console.print(f\"[bold]Review status:[/] ✓ {approved_count} approved | ✗ {rejected_count} rejected | — {unreviewed} unreviewed\")\n",
    "if CONFIG[\"approved_only\"] and approved_count == 0:\n",
    "    console.print(\"[yellow]Warning: approved_only=True but no gaps approved. Set approvals above or set approved_only=False.[/]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c81fe",
   "metadata": {},
   "source": [
    "## 4. Generate Hypotheses\n",
    "\n",
    "Claude produces for each gap:\n",
    "- A falsifiable **hypothesis statement**\n",
    "- The proposed **mechanism**\n",
    "- A specific, measurable **prediction**\n",
    "- **Falsification criteria** (what would definitively refute it)\n",
    "- A full **experiment design** with steps, tools, effort estimate\n",
    "- Flag: **computational** vs. wet-lab primary\n",
    "\n",
    "Results are cached — re-run is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa52f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.hypotheses import HypothesisGenerator, HypothesisReport\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "cache_path = CONFIG[\"output_dir\"] / \"hypothesis_cache.json\"\n",
    "\n",
    "if cache_path.exists():\n",
    "    console.print(\"[yellow]Loading cached hypothesis report…[/]\")\n",
    "    hyp_report = HypothesisReport.model_validate_json(cache_path.read_text())\n",
    "else:\n",
    "    generator = HypothesisGenerator(\n",
    "        domain=CONFIG[\"domain\"],\n",
    "        model=CONFIG[\"claude_model\"],\n",
    "        api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
    "    )\n",
    "\n",
    "    targets = [\n",
    "        a for a in ranked\n",
    "        if not CONFIG[\"approved_only\"] or getattr(a, \"approved\", None) is True\n",
    "    ]\n",
    "\n",
    "    hyp_report = HypothesisReport(\n",
    "        domain=CONFIG[\"domain\"],\n",
    "        query=gap_report.query,\n",
    "        n_gaps_considered=len(targets),\n",
    "    )\n",
    "\n",
    "    for i, analysis in enumerate(tqdm(targets, desc=\"Generating hypotheses\")):\n",
    "        console.print(f\"  [{i+1}/{len(targets)}] [cyan]{analysis.concept_a}[/] ↔ [cyan]{analysis.concept_b}[/]\")\n",
    "        hyp = generator._generate_one(analysis)\n",
    "        if hyp:\n",
    "            hyp_report.hypotheses.append(hyp)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    cache_path.write_text(hyp_report.model_dump_json(indent=2))\n",
    "    console.print(f\"[green]✓ Cached → {cache_path}[/]\")\n",
    "\n",
    "console.print(f\"\\n[bold green]✓ {len(hyp_report.hypotheses)} hypotheses generated[/]\")\n",
    "console.print(f\"  Computational: {len(hyp_report.computational)} | Pending review: {len(hyp_report.pending_review)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcc083",
   "metadata": {},
   "source": [
    "## 5. Top Hypotheses — Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.panel import Panel\n",
    "from rich.columns import Columns\n",
    "from rich.text import Text\n",
    "\n",
    "top = hyp_report.top(CONFIG[\"max_hypotheses\"])\n",
    "console.print(f\"\\n[bold]Top {len(top)} Hypotheses[/bold]\\n\")\n",
    "\n",
    "for i, hyp in enumerate(top, 1):\n",
    "    content = (\n",
    "        f\"[bold cyan]Statement:[/bold cyan] {hyp.statement}\\n\\n\"\n",
    "        f\"[bold cyan]Mechanism:[/bold cyan] {hyp.mechanism}\\n\\n\"\n",
    "        f\"[bold cyan]Prediction:[/bold cyan] {hyp.prediction}\\n\\n\"\n",
    "        f\"[bold cyan]Falsifiable:[/bold cyan] {'✓' if hyp.falsifiable else '✗'} | \"\n",
    "        f\"[bold cyan]Novelty:[/bold cyan] {hyp.novelty_score:.2f} | \"\n",
    "        f\"[bold cyan]Rigor:[/bold cyan] {hyp.rigor_score:.2f} | \"\n",
    "        f\"[bold cyan]Impact:[/bold cyan] {hyp.impact_score:.2f} | \"\n",
    "        f\"[bold yellow]Composite:[/bold yellow] {hyp.composite_score:.2f}\"\n",
    "    )\n",
    "    console.print(Panel(content, title=f\"#{i} {hyp.source_gap}\", border_style=\"blue\"))\n",
    "    console.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60a0da",
   "metadata": {},
   "source": [
    "## 6. Experiment Design Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd209c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table\n",
    "\n",
    "for i, hyp in enumerate(top, 1):\n",
    "    exp = hyp.experiment\n",
    "    table = Table(title=f\"Experiment #{i}: {hyp.source_gap}\", show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"Field\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Details\")\n",
    "\n",
    "    table.add_row(\"Approach\", exp.approach)\n",
    "    table.add_row(\"Steps\", \"\\n\".join(f\"{j+1}. {s}\" for j, s in enumerate(exp.steps)))\n",
    "    table.add_row(\"Tools\", \", \".join(exp.tools))\n",
    "    table.add_row(\"Computational\", \"✓\" if exp.computational else \"✗\")\n",
    "    table.add_row(\"Effort\", exp.estimated_effort)\n",
    "    table.add_row(\"Data Req.\", exp.data_requirements)\n",
    "    table.add_row(\"Expected (+)\", exp.expected_positive)\n",
    "    table.add_row(\"Expected (−)\", exp.expected_negative)\n",
    "    table.add_row(\"Null Hyp.\", exp.null_hypothesis)\n",
    "    table.add_row(\"Limitations\", exp.limitations)\n",
    "    console.print(table)\n",
    "    console.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6c75b",
   "metadata": {},
   "source": [
    "## 7. Visualization — Hypothesis Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "hyps = hyp_report.hypotheses\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"source_gap\": h.source_gap,\n",
    "        \"novelty\": h.novelty_score,\n",
    "        \"rigor\": h.rigor_score,\n",
    "        \"impact\": h.impact_score,\n",
    "        \"composite\": h.composite_score,\n",
    "        \"falsifiable\": h.falsifiable,\n",
    "        \"statement\": h.statement[:80] + \"…\" if len(h.statement) > 80 else h.statement,\n",
    "    }\n",
    "    for h in hyps\n",
    "])\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"rigor\",\n",
    "    y=\"novelty\",\n",
    "    size=\"impact\",\n",
    "    color=\"composite\",\n",
    "    hover_data=[\"source_gap\", \"statement\", \"falsifiable\"],\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    title=f\"Hypothesis Landscape — {CONFIG['domain'].replace('_', ' ').title()}\",\n",
    "    labels={\"rigor\": \"Rigor Score\", \"novelty\": \"Novelty Score\", \"composite\": \"Composite\"},\n",
    "    size_max=30,\n",
    ")\n",
    "fig.update_layout(height=550)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5328a",
   "metadata": {},
   "source": [
    "## 8. Save HypothesisReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb715257",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = CONFIG[\"output_dir\"] / \"hypothesis_report.json\"\n",
    "out_path.write_text(hyp_report.model_dump_json(indent=2))\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    f\"[bold green]HypothesisReport saved → {out_path}[/bold green]\\n\\n\"\n",
    "    f\"  Total hypotheses   : {len(hyp_report.hypotheses)}\\n\"\n",
    "    f\"  Computational      : {len(hyp_report.computational)}\\n\"\n",
    "    f\"  Pending review     : {len(hyp_report.pending_review)}\\n\"\n",
    "    f\"  Top composite score: {hyp_report.top(1)[0].composite_score:.3f}\",\n",
    "    title=\"[bold]Stage 3 Output[/bold]\",\n",
    "    border_style=\"green\",\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235cdd5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stage 3 Complete ✓\n",
    "\n",
    "The Athanor pipeline has now completed all three stages:\n",
    "\n",
    "| Stage | Notebook | Output |\n",
    "|-------|----------|--------|\n",
    "| 1 — Literature Mapper | `stage1_literature_mapper.ipynb` | `concept_graph.json`, `papers.json` |\n",
    "| 2 — Gap Finder | `stage2_gap_finder.ipynb` | `gap_report.json` |\n",
    "| 3 — Hypothesis Generator | `stage3_hypothesis_generator.ipynb` | `hypothesis_report.json` |\n",
    "\n",
    "### Next Steps\n",
    "- **Human review**: set `APPROVALS` dict and re-run Stage 3 with `approved_only=True`\n",
    "- **CLI**: `athanor run --domain information_theory --stages 1,2,3`\n",
    "- **New domain**: copy `domains/information_theory.yaml`, edit query + seed concepts, run pipeline\n",
    "- **Stage 4**: experiment prioritization, resource estimation, collaboration matching\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
