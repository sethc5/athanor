{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d80021b",
   "metadata": {},
   "source": [
    "# Athanor — Stage 1: Literature Mapper\n",
    "\n",
    "**Domain-agnostic automated science infrastructure**\n",
    "\n",
    "**Pipeline:**\n",
    "`Ingest (arXiv) → Parse → Embed → Claude concept extraction → Merge graph → Analyze → Visualize`\n",
    "\n",
    "**Test domain:** Information theory (swap the query in §2 to change domains)\n",
    "\n",
    "**Outputs:**\n",
    "- `data/raw/` — cached arXiv JSON\n",
    "- `outputs/graphs/` — interactive HTML graph + JSON graph dump\n",
    "- In-notebook plotly figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02eb4a",
   "metadata": {},
   "source": [
    "## 1. Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771be893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "def pip(*pkgs):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs], check=True)\n",
    "\n",
    "pip(\n",
    "    \"arxiv>=2.1.0\",\n",
    "    \"anthropic>=0.40.0\",\n",
    "    \"sentence-transformers>=3.0.0\",\n",
    "    \"networkx>=3.3\",\n",
    "    \"pyvis>=0.3.2\",\n",
    "    \"plotly>=5.23.0\",\n",
    "    \"pydantic>=2.0.0\",\n",
    "    \"scipy>=1.13.0\",\n",
    "    \"python-dotenv>=1.0.0\",\n",
    "    \"rich>=13.7.0\",\n",
    "    \"tqdm>=4.66.0\",\n",
    "    \"scikit-learn>=1.5.0\",\n",
    "    \"ipywidgets>=8.1.0\",\n",
    ")\n",
    "\n",
    "print(\"✓ All packages installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697de8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, re, logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Make the athanor package importable from notebooks/\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "console = Console()\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(name)s — %(message)s\")\n",
    "\n",
    "print(\"✓ Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7acee",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Edit this cell to change domain.** Everything downstream reads from `CONFIG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae968c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "# ── SWAP THIS BLOCK TO CHANGE DOMAIN ─────────────────────────────────────────\n",
    "CONFIG = {\n",
    "    # Domain identity\n",
    "    \"domain\": \"information theory\",\n",
    "\n",
    "    # arXiv search query — drives everything\n",
    "    \"arxiv_query\": \"information theory entropy channel capacity mutual information\",\n",
    "\n",
    "    # How many papers to fetch (keep ≤ 20 for first runs to control API cost)\n",
    "    \"max_papers\": 15,\n",
    "\n",
    "    # Sentence-transformer model for embeddings (local, no API cost)\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "\n",
    "    # Claude model for concept extraction\n",
    "    # claude-opus-4-5 = highest quality | claude-haiku-4-5 = fastest/cheapest\n",
    "    \"claude_model\": \"claude-opus-4-5\",\n",
    "\n",
    "    # Output paths\n",
    "    \"data_dir\": Path(\"..\") / \"data\" / \"raw\",\n",
    "    \"output_dir\": Path(\"..\") / \"outputs\" / \"graphs\",\n",
    "\n",
    "    # Sparse-connection detection threshold\n",
    "    # Pairs with cosine similarity > this but graph distance > 2 are flagged\n",
    "    \"sparse_sim_threshold\": 0.45,\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for d in (CONFIG[\"data_dir\"], CONFIG[\"output_dir\"]):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Validate API key\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    console.print(\"[bold red]ANTHROPIC_API_KEY not set![/] Copy .env.example → .env and add your key.\")\n",
    "else:\n",
    "    console.print(f\"[bold green]✓ Config ready[/] — domain: [cyan]{CONFIG['domain']}[/], papers: {CONFIG['max_papers']}, model: {CONFIG['claude_model']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974a95a",
   "metadata": {},
   "source": [
    "## 3. Fetch Corpus from arXiv\n",
    "\n",
    "Results are cached locally — re-running this cell is free after the first fetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ef9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.ingest import ArxivClient, parse_papers\n",
    "\n",
    "client = ArxivClient(cache_dir=CONFIG[\"data_dir\"])\n",
    "\n",
    "papers_raw = client.fetch(\n",
    "    query=CONFIG[\"arxiv_query\"],\n",
    "    max_results=CONFIG[\"max_papers\"],\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# ── summary table ─────────────────────────────────────────────────────────────\n",
    "table = Table(title=f\"Fetched {len(papers_raw)} papers — {CONFIG['domain']}\", show_lines=True)\n",
    "table.add_column(\"ID\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Title\", style=\"white\")\n",
    "table.add_column(\"Published\", style=\"magenta\")\n",
    "table.add_column(\"Authors\", style=\"green\")\n",
    "\n",
    "for p in papers_raw:\n",
    "    table.add_row(\n",
    "        p.arxiv_id,\n",
    "        p.title[:70] + (\"…\" if len(p.title) > 70 else \"\"),\n",
    "        p.published,\n",
    "        \", \".join(p.authors[:2]) + (f\" +{len(p.authors)-2}\" if len(p.authors) > 2 else \"\"),\n",
    "    )\n",
    "\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30522072",
   "metadata": {},
   "source": [
    "## 4. Parse & Prepare Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parse_papers(papers_raw)\n",
    "\n",
    "console.print(f\"[bold green]✓ Parsed {len(parsed)} papers[/]\")\n",
    "console.print(\"\\n[bold]Sample — first paper text digest:[/]\")\n",
    "console.print(parsed[0][\"text\"][:600] + \"…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f435ba",
   "metadata": {},
   "source": [
    "## 5. Embed Papers\n",
    "\n",
    "Generates dense vector representations using a local sentence-transformer model.  \n",
    "No API calls — runs fully offline after first model download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.embed import Embedder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedder = Embedder(model_name=CONFIG[\"embedding_model\"])\n",
    "\n",
    "texts = [p[\"text\"] for p in parsed]\n",
    "embeddings = embedder.embed(texts)  # shape (N, D)\n",
    "\n",
    "# Pairwise cosine similarity — used downstream for sparse-connection detection\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Attach embeddings to parsed dicts for provenance\n",
    "for i, p in enumerate(parsed):\n",
    "    p[\"embedding\"] = embeddings[i]\n",
    "\n",
    "console.print(f\"[bold green]✓ Embeddings computed[/] — shape: {embeddings.shape}\")\n",
    "console.print(f\"   Similarity matrix: {sim_matrix.shape}, range [{sim_matrix.min():.3f}, {sim_matrix.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d08bf3",
   "metadata": {},
   "source": [
    "## 6. Concept Extraction via Claude\n",
    "\n",
    "For each paper, Claude extracts:\n",
    "- **concepts** — canonical scientific terms with descriptions  \n",
    "- **edges** — typed relationships between concepts with evidence  \n",
    "\n",
    "This is the AI backbone. The prompt is domain-agnostic — works identically for longevity biology or quantum gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.graph import ConceptExtractor\n",
    "\n",
    "extractor = ConceptExtractor(\n",
    "    model=CONFIG[\"claude_model\"],\n",
    "    api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
    ")\n",
    "\n",
    "# ── extract from every paper ──────────────────────────────────────────────────\n",
    "paper_extractions = []  # list of (arxiv_id, concepts, edges)\n",
    "\n",
    "cache_path = CONFIG[\"data_dir\"] / \"extractions_cache.json\"\n",
    "\n",
    "if cache_path.exists():\n",
    "    console.print(\"[yellow]Loading cached extractions…[/]\")\n",
    "    raw_cache = json.loads(cache_path.read_text())\n",
    "    # Rebuild objects from cache dicts\n",
    "    from athanor.graph.models import Concept, Edge\n",
    "    for item in raw_cache:\n",
    "        concepts = [Concept(**c) for c in item[\"concepts\"]]\n",
    "        edges = [Edge(**e) for e in item[\"edges\"]]\n",
    "        paper_extractions.append((item[\"arxiv_id\"], concepts, edges))\n",
    "else:\n",
    "    for paper in tqdm(parsed, desc=\"Extracting concepts\"):\n",
    "        concepts, edges = extractor.extract(\n",
    "            text=paper[\"text\"],\n",
    "            arxiv_id=paper[\"arxiv_id\"],\n",
    "        )\n",
    "        paper_extractions.append((paper[\"arxiv_id\"], concepts, edges))\n",
    "        time.sleep(0.5)  # gentle rate limiting\n",
    "\n",
    "    # Cache to disk\n",
    "    serialisable = [\n",
    "        {\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"concepts\": [c.model_dump() for c in concepts],\n",
    "            \"edges\": [e.model_dump() for e in edges],\n",
    "        }\n",
    "        for arxiv_id, concepts, edges in paper_extractions\n",
    "    ]\n",
    "    cache_path.write_text(json.dumps(serialisable, indent=2))\n",
    "    console.print(f\"[green]✓ Extractions cached to {cache_path}[/]\")\n",
    "\n",
    "total_concepts = sum(len(c) for _, c, _ in paper_extractions)\n",
    "total_edges = sum(len(e) for _, _, e in paper_extractions)\n",
    "console.print(f\"[bold green]✓ Extraction complete[/] — {total_concepts} concepts, {total_edges} edges across {len(paper_extractions)} papers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1ee4e",
   "metadata": {},
   "source": [
    "## 7. Build the Concept Graph\n",
    "\n",
    "Merge per-paper extractions into a unified graph:\n",
    "- Concepts with the same label (case-insensitive) are deduplicated\n",
    "- Edges accumulate weight across papers\n",
    "- Provenance (which papers contributed each node/edge) is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c49f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.graph import GraphBuilder\n",
    "\n",
    "# GraphBuilder.build() accepts the same parsed_papers format,\n",
    "# but here we feed the pre-computed extractions to avoid re-calling Claude.\n",
    "# We build the graph manually from cached extractions.\n",
    "\n",
    "from athanor.graph.models import ConceptGraph, Concept, Edge\n",
    "\n",
    "# Collect all concepts and edges\n",
    "all_concepts: List = []\n",
    "all_edges: List = []\n",
    "for arxiv_id, concepts, edges in paper_extractions:\n",
    "    all_concepts.extend(concepts)\n",
    "    all_edges.extend(edges)\n",
    "\n",
    "# Build via GraphBuilder internal merge (reuse the logic)\n",
    "builder = GraphBuilder(extractor=extractor)\n",
    "merged_concepts, merged_edges, _ = builder._merge(all_concepts, all_edges)\n",
    "\n",
    "concept_graph = ConceptGraph(\n",
    "    domain=CONFIG[\"domain\"],\n",
    "    query=CONFIG[\"arxiv_query\"],\n",
    "    concepts=merged_concepts,\n",
    "    edges=merged_edges,\n",
    ")\n",
    "builder._compute_centrality(concept_graph)\n",
    "\n",
    "# Persist graph JSON\n",
    "graph_json_path = CONFIG[\"output_dir\"] / \"concept_graph.json\"\n",
    "graph_json_path.write_text(concept_graph.model_dump_json(indent=2))\n",
    "\n",
    "G = concept_graph.to_networkx()\n",
    "console.print(f\"[bold green]✓ Concept graph built[/]\")\n",
    "console.print(f\"   Nodes: {G.number_of_nodes()}\")\n",
    "console.print(f\"   Edges: {G.number_of_edges()}\")\n",
    "console.print(f\"   Graph density: {nx.density(G):.4f}\")\n",
    "console.print(f\"   Connected components: {nx.number_connected_components(G)}\")\n",
    "console.print(f\"   Saved JSON: {graph_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e80b8",
   "metadata": {},
   "source": [
    "## 8. Graph Analysis & Sparse Connection Detection\n",
    "\n",
    "Two signals for Stage 2 gap-finding:\n",
    "\n",
    "1. **Bridge edges** (low Jaccard overlap between endpoints' neighborhoods) — concepts that connect otherwise distant clusters\n",
    "2. **Embedding-near / graph-far pairs** — concepts semantically similar but structurally unconnected — these are the *implicit gaps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Graph metrics ─────────────────────────────────────────────────────────────\n",
    "betweenness = nx.betweenness_centrality(G, weight=\"weight\", normalized=True)\n",
    "degree_cent  = nx.degree_centrality(G)\n",
    "clustering   = nx.clustering(G, weight=\"weight\")\n",
    "\n",
    "# Top concepts by betweenness (hubs that bridge clusters)\n",
    "top_hubs = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "hub_table = Table(title=\"Top 10 Hub Concepts (betweenness centrality)\", show_lines=True)\n",
    "hub_table.add_column(\"Concept\", style=\"cyan\")\n",
    "hub_table.add_column(\"Betweenness\", style=\"magenta\")\n",
    "hub_table.add_column(\"Degree\", style=\"green\")\n",
    "hub_table.add_column(\"Clustering\", style=\"yellow\")\n",
    "for concept, score in top_hubs:\n",
    "    hub_table.add_row(\n",
    "        concept,\n",
    "        f\"{score:.4f}\",\n",
    "        f\"{degree_cent.get(concept, 0):.4f}\",\n",
    "        f\"{clustering.get(concept, 0):.4f}\",\n",
    "    )\n",
    "console.print(hub_table)\n",
    "\n",
    "# ── Bridge edges (sparse connections in graph structure) ───────────────────────\n",
    "bridge_edges = concept_graph.sparse_connections(top_k=10)\n",
    "console.print(f\"\\n[bold]Bridge edges (structurally sparse connections):[/]\")\n",
    "for e in bridge_edges[:5]:\n",
    "    console.print(f\"  [cyan]{e.source}[/] ──[{e.relation}]→ [cyan]{e.target}[/]  (w={e.weight:.2f})\")\n",
    "    if e.evidence:\n",
    "        console.print(f\"    [dim]Evidence: {e.evidence[:120]}[/]\")\n",
    "\n",
    "# ── Embedding-near / graph-far pairs (implicit gaps) ──────────────────────────\n",
    "console.print(\"\\n[bold]Embedding-near / graph-far pairs (candidate research gaps):[/]\")\n",
    "\n",
    "# Build a label→embedding lookup using paper embeddings for concept names\n",
    "labels = [c.label for c in concept_graph.concepts]\n",
    "concept_texts = [c.label + \". \" + c.description for c in concept_graph.concepts]\n",
    "concept_embeddings = embedder.embed(concept_texts)\n",
    "concept_sim = cosine_similarity(concept_embeddings)\n",
    "\n",
    "threshold = CONFIG[\"sparse_sim_threshold\"]\n",
    "candidate_gaps = []\n",
    "for i in range(len(labels)):\n",
    "    for j in range(i + 1, len(labels)):\n",
    "        if concept_sim[i, j] < threshold:\n",
    "            continue\n",
    "        # Are they graph-far?\n",
    "        try:\n",
    "            dist = nx.shortest_path_length(G, labels[i], labels[j])\n",
    "        except nx.NetworkXNoPath:\n",
    "            dist = 999\n",
    "        if dist > 2:\n",
    "            candidate_gaps.append((concept_sim[i, j], dist, labels[i], labels[j]))\n",
    "\n",
    "candidate_gaps.sort(reverse=True)\n",
    "\n",
    "gap_table = Table(title=\"Top Candidate Gaps (semantically similar, structurally distant)\", show_lines=True)\n",
    "gap_table.add_column(\"Concept A\", style=\"cyan\")\n",
    "gap_table.add_column(\"Concept B\", style=\"cyan\")\n",
    "gap_table.add_column(\"Similarity\", style=\"green\")\n",
    "gap_table.add_column(\"Graph Distance\", style=\"red\")\n",
    "for sim, dist, a, b in candidate_gaps[:10]:\n",
    "    gap_table.add_row(a, b, f\"{sim:.3f}\", str(dist) if dist < 999 else \"∞\")\n",
    "console.print(gap_table)\n",
    "\n",
    "# Store for Stage 2\n",
    "candidate_gaps_export = [\n",
    "    {\"concept_a\": a, \"concept_b\": b, \"similarity\": float(sim), \"graph_distance\": dist}\n",
    "    for sim, dist, a, b in candidate_gaps\n",
    "]\n",
    "gaps_path = CONFIG[\"output_dir\"] / \"candidate_gaps.json\"\n",
    "gaps_path.write_text(json.dumps(candidate_gaps_export, indent=2))\n",
    "console.print(f\"\\n[green]✓ {len(candidate_gaps)} candidate gaps saved → {gaps_path}[/]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d5b3f",
   "metadata": {},
   "source": [
    "## 9. Visualization\n",
    "\n",
    "Two renders:\n",
    "- **pyvis** — force-directed interactive HTML (best for exploration). Red dashed edges = bridge connections.\n",
    "- **plotly** — in-notebook static/interactive. Node size = betweenness centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athanor.viz import GraphVisualizer\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "viz = GraphVisualizer(output_dir=CONFIG[\"output_dir\"])\n",
    "\n",
    "# ── pyvis interactive HTML ────────────────────────────────────────────────────\n",
    "html_path = viz.pyvis_html(\n",
    "    concept_graph,\n",
    "    filename=\"concept_graph.html\",\n",
    "    highlight_sparse=True,\n",
    ")\n",
    "console.print(f\"[bold green]✓ Interactive graph saved → {html_path}[/]\")\n",
    "\n",
    "# Render inline in notebook (works in classic Jupyter; in VS Code open the file)\n",
    "display(IFrame(str(html_path), width=\"100%\", height=\"800px\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plotly in-notebook figure ─────────────────────────────────────────────────\n",
    "fig = viz.plotly_figure(concept_graph)\n",
    "fig.show()\n",
    "\n",
    "# Also export as static HTML for sharing\n",
    "plotly_path = CONFIG[\"output_dir\"] / \"concept_graph_plotly.html\"\n",
    "fig.write_html(str(plotly_path))\n",
    "console.print(f\"[green]✓ Plotly graph saved → {plotly_path}[/]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba61e2d",
   "metadata": {},
   "source": [
    "## Stage 1 Complete ✓\n",
    "\n",
    "**Outputs produced:**\n",
    "| File | Contents |\n",
    "|------|----------|\n",
    "| `data/raw/arxiv_*.json` | Cached paper metadata |\n",
    "| `data/raw/extractions_cache.json` | Claude concept extractions per paper |\n",
    "| `outputs/graphs/concept_graph.json` | Full merged concept graph (Pydantic model) |\n",
    "| `outputs/graphs/candidate_gaps.json` | Embedding-near / graph-far pairs → Stage 2 input |\n",
    "| `outputs/graphs/concept_graph.html` | Interactive pyvis visualization |\n",
    "| `outputs/graphs/concept_graph_plotly.html` | Plotly visualization |\n",
    "\n",
    "**To change domain:** edit the `CONFIG` block in §2, delete the cache files, re-run.\n",
    "\n",
    "---\n",
    "\n",
    "**Stage 2** will read `candidate_gaps.json` and ask Claude: *\"What research question does this gap imply, and why hasn't it been answered?\"*\n",
    "\n",
    "**Stage 3** will take the best questions and propose tractable experiment designs — computational first, flagging what needs wet lab.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
